{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 890,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0056179775280898875,
      "grad_norm": 0.527427077293396,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 1.888,
      "step": 1
    },
    {
      "epoch": 0.011235955056179775,
      "grad_norm": 0.52858567237854,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 1.8092,
      "step": 2
    },
    {
      "epoch": 0.016853932584269662,
      "grad_norm": 0.5637223124504089,
      "learning_rate": 5.555555555555556e-06,
      "loss": 2.0185,
      "step": 3
    },
    {
      "epoch": 0.02247191011235955,
      "grad_norm": 0.7685739994049072,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 2.7098,
      "step": 4
    },
    {
      "epoch": 0.028089887640449437,
      "grad_norm": 0.5997740626335144,
      "learning_rate": 9.259259259259259e-06,
      "loss": 2.0895,
      "step": 5
    },
    {
      "epoch": 0.033707865168539325,
      "grad_norm": 0.7233952879905701,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 1.7747,
      "step": 6
    },
    {
      "epoch": 0.03932584269662921,
      "grad_norm": 0.7186373472213745,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 2.0928,
      "step": 7
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 0.6931007504463196,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 2.1501,
      "step": 8
    },
    {
      "epoch": 0.05056179775280899,
      "grad_norm": 0.7053710222244263,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.8391,
      "step": 9
    },
    {
      "epoch": 0.056179775280898875,
      "grad_norm": 0.5708274841308594,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 1.8581,
      "step": 10
    },
    {
      "epoch": 0.06179775280898876,
      "grad_norm": 0.5943424105644226,
      "learning_rate": 2.037037037037037e-05,
      "loss": 1.8987,
      "step": 11
    },
    {
      "epoch": 0.06741573033707865,
      "grad_norm": 0.6329241394996643,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 1.8207,
      "step": 12
    },
    {
      "epoch": 0.07303370786516854,
      "grad_norm": 0.8006532192230225,
      "learning_rate": 2.4074074074074074e-05,
      "loss": 1.8362,
      "step": 13
    },
    {
      "epoch": 0.07865168539325842,
      "grad_norm": 0.6257917284965515,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 1.8199,
      "step": 14
    },
    {
      "epoch": 0.08426966292134831,
      "grad_norm": 0.655730664730072,
      "learning_rate": 2.777777777777778e-05,
      "loss": 1.8744,
      "step": 15
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 0.7233570218086243,
      "learning_rate": 2.962962962962963e-05,
      "loss": 1.8852,
      "step": 16
    },
    {
      "epoch": 0.09550561797752809,
      "grad_norm": 0.831516683101654,
      "learning_rate": 3.148148148148148e-05,
      "loss": 1.7595,
      "step": 17
    },
    {
      "epoch": 0.10112359550561797,
      "grad_norm": 0.96403568983078,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.7493,
      "step": 18
    },
    {
      "epoch": 0.10674157303370786,
      "grad_norm": 0.9333449602127075,
      "learning_rate": 3.518518518518519e-05,
      "loss": 2.105,
      "step": 19
    },
    {
      "epoch": 0.11235955056179775,
      "grad_norm": 0.8581129908561707,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 1.9178,
      "step": 20
    },
    {
      "epoch": 0.11797752808988764,
      "grad_norm": 0.8525934815406799,
      "learning_rate": 3.888888888888889e-05,
      "loss": 1.7042,
      "step": 21
    },
    {
      "epoch": 0.12359550561797752,
      "grad_norm": 0.7624507546424866,
      "learning_rate": 4.074074074074074e-05,
      "loss": 1.6487,
      "step": 22
    },
    {
      "epoch": 0.12921348314606743,
      "grad_norm": 1.2301692962646484,
      "learning_rate": 4.259259259259259e-05,
      "loss": 2.4569,
      "step": 23
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 1.0243375301361084,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 1.6087,
      "step": 24
    },
    {
      "epoch": 0.1404494382022472,
      "grad_norm": 1.1087650060653687,
      "learning_rate": 4.62962962962963e-05,
      "loss": 1.4236,
      "step": 25
    },
    {
      "epoch": 0.14606741573033707,
      "grad_norm": 1.2983596324920654,
      "learning_rate": 4.814814814814815e-05,
      "loss": 1.6969,
      "step": 26
    },
    {
      "epoch": 0.15168539325842698,
      "grad_norm": 1.1555482149124146,
      "learning_rate": 5e-05,
      "loss": 1.6728,
      "step": 27
    },
    {
      "epoch": 0.15730337078651685,
      "grad_norm": 1.213228702545166,
      "learning_rate": 4.999983435143142e-05,
      "loss": 1.2629,
      "step": 28
    },
    {
      "epoch": 0.16292134831460675,
      "grad_norm": 1.286223292350769,
      "learning_rate": 4.999933740792084e-05,
      "loss": 1.6974,
      "step": 29
    },
    {
      "epoch": 0.16853932584269662,
      "grad_norm": 0.9371748566627502,
      "learning_rate": 4.999850917605369e-05,
      "loss": 1.2822,
      "step": 30
    },
    {
      "epoch": 0.17415730337078653,
      "grad_norm": 0.8617205619812012,
      "learning_rate": 4.9997349666805604e-05,
      "loss": 1.0581,
      "step": 31
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.9773658514022827,
      "learning_rate": 4.999585889554227e-05,
      "loss": 0.9669,
      "step": 32
    },
    {
      "epoch": 0.1853932584269663,
      "grad_norm": 0.8140249848365784,
      "learning_rate": 4.999403688201921e-05,
      "loss": 0.9496,
      "step": 33
    },
    {
      "epoch": 0.19101123595505617,
      "grad_norm": 0.8090336322784424,
      "learning_rate": 4.999188365038156e-05,
      "loss": 0.9133,
      "step": 34
    },
    {
      "epoch": 0.19662921348314608,
      "grad_norm": 0.8504349589347839,
      "learning_rate": 4.998939922916368e-05,
      "loss": 1.0609,
      "step": 35
    },
    {
      "epoch": 0.20224719101123595,
      "grad_norm": 0.8108276724815369,
      "learning_rate": 4.9986583651288844e-05,
      "loss": 0.7416,
      "step": 36
    },
    {
      "epoch": 0.20786516853932585,
      "grad_norm": 0.8898236751556396,
      "learning_rate": 4.998343695406875e-05,
      "loss": 0.7463,
      "step": 37
    },
    {
      "epoch": 0.21348314606741572,
      "grad_norm": 0.8459187150001526,
      "learning_rate": 4.9979959179203095e-05,
      "loss": 0.7107,
      "step": 38
    },
    {
      "epoch": 0.21910112359550563,
      "grad_norm": 0.9136841893196106,
      "learning_rate": 4.9976150372778944e-05,
      "loss": 0.6604,
      "step": 39
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 1.220565915107727,
      "learning_rate": 4.997201058527016e-05,
      "loss": 1.0535,
      "step": 40
    },
    {
      "epoch": 0.2303370786516854,
      "grad_norm": 0.8104730248451233,
      "learning_rate": 4.996753987153673e-05,
      "loss": 0.5359,
      "step": 41
    },
    {
      "epoch": 0.23595505617977527,
      "grad_norm": 1.182726263999939,
      "learning_rate": 4.996273829082404e-05,
      "loss": 0.8268,
      "step": 42
    },
    {
      "epoch": 0.24157303370786518,
      "grad_norm": 1.2317339181900024,
      "learning_rate": 4.99576059067621e-05,
      "loss": 0.7767,
      "step": 43
    },
    {
      "epoch": 0.24719101123595505,
      "grad_norm": 0.899352490901947,
      "learning_rate": 4.995214278736467e-05,
      "loss": 0.3905,
      "step": 44
    },
    {
      "epoch": 0.25280898876404495,
      "grad_norm": 0.8267571926116943,
      "learning_rate": 4.994634900502837e-05,
      "loss": 0.3364,
      "step": 45
    },
    {
      "epoch": 0.25842696629213485,
      "grad_norm": 1.4162652492523193,
      "learning_rate": 4.9940224636531754e-05,
      "loss": 0.7401,
      "step": 46
    },
    {
      "epoch": 0.2640449438202247,
      "grad_norm": 1.6667143106460571,
      "learning_rate": 4.993376976303426e-05,
      "loss": 0.482,
      "step": 47
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.9494081139564514,
      "learning_rate": 4.9926984470075104e-05,
      "loss": 0.3164,
      "step": 48
    },
    {
      "epoch": 0.2752808988764045,
      "grad_norm": 0.6241448521614075,
      "learning_rate": 4.991986884757224e-05,
      "loss": 0.1822,
      "step": 49
    },
    {
      "epoch": 0.2808988764044944,
      "grad_norm": 1.0684617757797241,
      "learning_rate": 4.991242298982107e-05,
      "loss": 0.3944,
      "step": 50
    },
    {
      "epoch": 0.28651685393258425,
      "grad_norm": 2.6259708404541016,
      "learning_rate": 4.990464699549325e-05,
      "loss": 1.1285,
      "step": 51
    },
    {
      "epoch": 0.29213483146067415,
      "grad_norm": 1.3109830617904663,
      "learning_rate": 4.9896540967635364e-05,
      "loss": 0.4484,
      "step": 52
    },
    {
      "epoch": 0.29775280898876405,
      "grad_norm": 1.4038176536560059,
      "learning_rate": 4.988810501366756e-05,
      "loss": 0.547,
      "step": 53
    },
    {
      "epoch": 0.30337078651685395,
      "grad_norm": 1.1049035787582397,
      "learning_rate": 4.9879339245382154e-05,
      "loss": 0.1819,
      "step": 54
    },
    {
      "epoch": 0.3089887640449438,
      "grad_norm": 0.5004037022590637,
      "learning_rate": 4.987024377894208e-05,
      "loss": 0.1196,
      "step": 55
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.5879114270210266,
      "learning_rate": 4.9860818734879434e-05,
      "loss": 0.079,
      "step": 56
    },
    {
      "epoch": 0.3202247191011236,
      "grad_norm": 1.176592230796814,
      "learning_rate": 4.985106423809381e-05,
      "loss": 0.3938,
      "step": 57
    },
    {
      "epoch": 0.3258426966292135,
      "grad_norm": 1.7997270822525024,
      "learning_rate": 4.984098041785069e-05,
      "loss": 0.2785,
      "step": 58
    },
    {
      "epoch": 0.33146067415730335,
      "grad_norm": 1.2951916456222534,
      "learning_rate": 4.98305674077797e-05,
      "loss": 0.1801,
      "step": 59
    },
    {
      "epoch": 0.33707865168539325,
      "grad_norm": 0.39626237750053406,
      "learning_rate": 4.9819825345872855e-05,
      "loss": 0.0807,
      "step": 60
    },
    {
      "epoch": 0.34269662921348315,
      "grad_norm": 0.7961227297782898,
      "learning_rate": 4.980875437448274e-05,
      "loss": 0.0951,
      "step": 61
    },
    {
      "epoch": 0.34831460674157305,
      "grad_norm": 0.780543863773346,
      "learning_rate": 4.9797354640320594e-05,
      "loss": 0.068,
      "step": 62
    },
    {
      "epoch": 0.3539325842696629,
      "grad_norm": 0.4911896586418152,
      "learning_rate": 4.978562629445438e-05,
      "loss": 0.0852,
      "step": 63
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 1.4432885646820068,
      "learning_rate": 4.977356949230681e-05,
      "loss": 0.4335,
      "step": 64
    },
    {
      "epoch": 0.3651685393258427,
      "grad_norm": 0.40936481952667236,
      "learning_rate": 4.976118439365324e-05,
      "loss": 0.0698,
      "step": 65
    },
    {
      "epoch": 0.3707865168539326,
      "grad_norm": 0.4622005820274353,
      "learning_rate": 4.974847116261957e-05,
      "loss": 0.0633,
      "step": 66
    },
    {
      "epoch": 0.37640449438202245,
      "grad_norm": 0.5160611271858215,
      "learning_rate": 4.973542996768009e-05,
      "loss": 0.0624,
      "step": 67
    },
    {
      "epoch": 0.38202247191011235,
      "grad_norm": 0.4863431751728058,
      "learning_rate": 4.972206098165522e-05,
      "loss": 0.0485,
      "step": 68
    },
    {
      "epoch": 0.38764044943820225,
      "grad_norm": 1.5559687614440918,
      "learning_rate": 4.970836438170924e-05,
      "loss": 0.3044,
      "step": 69
    },
    {
      "epoch": 0.39325842696629215,
      "grad_norm": 0.29808929562568665,
      "learning_rate": 4.969434034934791e-05,
      "loss": 0.041,
      "step": 70
    },
    {
      "epoch": 0.398876404494382,
      "grad_norm": 0.3668711185455322,
      "learning_rate": 4.967998907041611e-05,
      "loss": 0.0386,
      "step": 71
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 2.0176069736480713,
      "learning_rate": 4.9665310735095335e-05,
      "loss": 0.3229,
      "step": 72
    },
    {
      "epoch": 0.4101123595505618,
      "grad_norm": 0.5659025311470032,
      "learning_rate": 4.965030553790122e-05,
      "loss": 0.0218,
      "step": 73
    },
    {
      "epoch": 0.4157303370786517,
      "grad_norm": 0.5099859237670898,
      "learning_rate": 4.963497367768091e-05,
      "loss": 0.0221,
      "step": 74
    },
    {
      "epoch": 0.42134831460674155,
      "grad_norm": 0.8931615948677063,
      "learning_rate": 4.9619315357610455e-05,
      "loss": 0.046,
      "step": 75
    },
    {
      "epoch": 0.42696629213483145,
      "grad_norm": 0.25133857131004333,
      "learning_rate": 4.960333078519214e-05,
      "loss": 0.0354,
      "step": 76
    },
    {
      "epoch": 0.43258426966292135,
      "grad_norm": 0.4336722791194916,
      "learning_rate": 4.9587020172251664e-05,
      "loss": 0.0401,
      "step": 77
    },
    {
      "epoch": 0.43820224719101125,
      "grad_norm": 0.6689789295196533,
      "learning_rate": 4.957038373493541e-05,
      "loss": 0.0519,
      "step": 78
    },
    {
      "epoch": 0.4438202247191011,
      "grad_norm": 0.7283533811569214,
      "learning_rate": 4.955342169370755e-05,
      "loss": 0.0327,
      "step": 79
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.5691818594932556,
      "learning_rate": 4.953613427334711e-05,
      "loss": 0.0296,
      "step": 80
    },
    {
      "epoch": 0.4550561797752809,
      "grad_norm": 0.3096812069416046,
      "learning_rate": 4.9518521702944994e-05,
      "loss": 0.0162,
      "step": 81
    },
    {
      "epoch": 0.4606741573033708,
      "grad_norm": 2.0144975185394287,
      "learning_rate": 4.950058421590097e-05,
      "loss": 0.3161,
      "step": 82
    },
    {
      "epoch": 0.46629213483146065,
      "grad_norm": 1.7177361249923706,
      "learning_rate": 4.948232204992057e-05,
      "loss": 0.2469,
      "step": 83
    },
    {
      "epoch": 0.47191011235955055,
      "grad_norm": 0.274530291557312,
      "learning_rate": 4.946373544701193e-05,
      "loss": 0.0148,
      "step": 84
    },
    {
      "epoch": 0.47752808988764045,
      "grad_norm": 1.1925939321517944,
      "learning_rate": 4.944482465348256e-05,
      "loss": 0.0602,
      "step": 85
    },
    {
      "epoch": 0.48314606741573035,
      "grad_norm": 1.3090379238128662,
      "learning_rate": 4.942558991993616e-05,
      "loss": 0.188,
      "step": 86
    },
    {
      "epoch": 0.4887640449438202,
      "grad_norm": 2.334019422531128,
      "learning_rate": 4.940603150126919e-05,
      "loss": 0.3228,
      "step": 87
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.4307161867618561,
      "learning_rate": 4.9386149656667604e-05,
      "loss": 0.031,
      "step": 88
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.3786855936050415,
      "learning_rate": 4.93659446496033e-05,
      "loss": 0.1656,
      "step": 89
    },
    {
      "epoch": 0.5056179775280899,
      "grad_norm": 1.0042790174484253,
      "learning_rate": 4.934541674783073e-05,
      "loss": 0.1549,
      "step": 90
    },
    {
      "epoch": 0.5112359550561798,
      "grad_norm": 1.938494324684143,
      "learning_rate": 4.932456622338331e-05,
      "loss": 0.2677,
      "step": 91
    },
    {
      "epoch": 0.5168539325842697,
      "grad_norm": 0.8633168935775757,
      "learning_rate": 4.9303393352569774e-05,
      "loss": 0.0502,
      "step": 92
    },
    {
      "epoch": 0.5224719101123596,
      "grad_norm": 0.894551157951355,
      "learning_rate": 4.9281898415970604e-05,
      "loss": 0.0709,
      "step": 93
    },
    {
      "epoch": 0.5280898876404494,
      "grad_norm": 0.5354738235473633,
      "learning_rate": 4.926008169843424e-05,
      "loss": 0.0342,
      "step": 94
    },
    {
      "epoch": 0.5337078651685393,
      "grad_norm": 0.4348489046096802,
      "learning_rate": 4.923794348907331e-05,
      "loss": 0.0366,
      "step": 95
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.792256772518158,
      "learning_rate": 4.9215484081260844e-05,
      "loss": 0.0504,
      "step": 96
    },
    {
      "epoch": 0.5449438202247191,
      "grad_norm": 1.0271447896957397,
      "learning_rate": 4.919270377262632e-05,
      "loss": 0.1231,
      "step": 97
    },
    {
      "epoch": 0.550561797752809,
      "grad_norm": 0.5800986886024475,
      "learning_rate": 4.9169602865051815e-05,
      "loss": 0.0523,
      "step": 98
    },
    {
      "epoch": 0.5561797752808989,
      "grad_norm": 0.8707902431488037,
      "learning_rate": 4.914618166466787e-05,
      "loss": 0.0558,
      "step": 99
    },
    {
      "epoch": 0.5617977528089888,
      "grad_norm": 0.5182386040687561,
      "learning_rate": 4.912244048184958e-05,
      "loss": 0.0331,
      "step": 100
    },
    {
      "epoch": 0.5674157303370787,
      "grad_norm": 0.9763083457946777,
      "learning_rate": 4.909837963121237e-05,
      "loss": 0.1267,
      "step": 101
    },
    {
      "epoch": 0.5730337078651685,
      "grad_norm": 0.39026814699172974,
      "learning_rate": 4.9073999431607876e-05,
      "loss": 0.0344,
      "step": 102
    },
    {
      "epoch": 0.5786516853932584,
      "grad_norm": 0.3198765516281128,
      "learning_rate": 4.904930020611972e-05,
      "loss": 0.0181,
      "step": 103
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.7130171060562134,
      "learning_rate": 4.9024282282059195e-05,
      "loss": 0.1094,
      "step": 104
    },
    {
      "epoch": 0.5898876404494382,
      "grad_norm": 0.7375363707542419,
      "learning_rate": 4.899894599096098e-05,
      "loss": 0.0436,
      "step": 105
    },
    {
      "epoch": 0.5955056179775281,
      "grad_norm": 0.2343243658542633,
      "learning_rate": 4.89732916685787e-05,
      "loss": 0.0235,
      "step": 106
    },
    {
      "epoch": 0.601123595505618,
      "grad_norm": 0.596444308757782,
      "learning_rate": 4.894731965488049e-05,
      "loss": 0.0551,
      "step": 107
    },
    {
      "epoch": 0.6067415730337079,
      "grad_norm": 0.701154351234436,
      "learning_rate": 4.892103029404451e-05,
      "loss": 0.0801,
      "step": 108
    },
    {
      "epoch": 0.6123595505617978,
      "grad_norm": 1.4984610080718994,
      "learning_rate": 4.889442393445435e-05,
      "loss": 0.2846,
      "step": 109
    },
    {
      "epoch": 0.6179775280898876,
      "grad_norm": 0.4141717255115509,
      "learning_rate": 4.886750092869445e-05,
      "loss": 0.0348,
      "step": 110
    },
    {
      "epoch": 0.6235955056179775,
      "grad_norm": 0.8782057166099548,
      "learning_rate": 4.88402616335454e-05,
      "loss": 0.0238,
      "step": 111
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.3805752098560333,
      "learning_rate": 4.881270640997921e-05,
      "loss": 0.0308,
      "step": 112
    },
    {
      "epoch": 0.6348314606741573,
      "grad_norm": 0.9014304876327515,
      "learning_rate": 4.878483562315456e-05,
      "loss": 0.0988,
      "step": 113
    },
    {
      "epoch": 0.6404494382022472,
      "grad_norm": 0.5890285968780518,
      "learning_rate": 4.875664964241191e-05,
      "loss": 0.047,
      "step": 114
    },
    {
      "epoch": 0.6460674157303371,
      "grad_norm": 0.6742485165596008,
      "learning_rate": 4.872814884126867e-05,
      "loss": 0.0625,
      "step": 115
    },
    {
      "epoch": 0.651685393258427,
      "grad_norm": 0.9065677523612976,
      "learning_rate": 4.869933359741417e-05,
      "loss": 0.0804,
      "step": 116
    },
    {
      "epoch": 0.6573033707865169,
      "grad_norm": 0.37143492698669434,
      "learning_rate": 4.8670204292704735e-05,
      "loss": 0.0235,
      "step": 117
    },
    {
      "epoch": 0.6629213483146067,
      "grad_norm": 0.5008161067962646,
      "learning_rate": 4.864076131315857e-05,
      "loss": 0.0311,
      "step": 118
    },
    {
      "epoch": 0.6685393258426966,
      "grad_norm": 0.3973131477832794,
      "learning_rate": 4.861100504895068e-05,
      "loss": 0.0171,
      "step": 119
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 1.6313985586166382,
      "learning_rate": 4.8580935894407646e-05,
      "loss": 0.1244,
      "step": 120
    },
    {
      "epoch": 0.6797752808988764,
      "grad_norm": 0.5760793089866638,
      "learning_rate": 4.855055424800249e-05,
      "loss": 0.0552,
      "step": 121
    },
    {
      "epoch": 0.6853932584269663,
      "grad_norm": 0.6409871578216553,
      "learning_rate": 4.851986051234929e-05,
      "loss": 0.0436,
      "step": 122
    },
    {
      "epoch": 0.6910112359550562,
      "grad_norm": 0.49342280626296997,
      "learning_rate": 4.8488855094197924e-05,
      "loss": 0.0343,
      "step": 123
    },
    {
      "epoch": 0.6966292134831461,
      "grad_norm": 0.3971874713897705,
      "learning_rate": 4.8457538404428646e-05,
      "loss": 0.0272,
      "step": 124
    },
    {
      "epoch": 0.702247191011236,
      "grad_norm": 0.9204808473587036,
      "learning_rate": 4.842591085804663e-05,
      "loss": 0.0528,
      "step": 125
    },
    {
      "epoch": 0.7078651685393258,
      "grad_norm": 0.5061252117156982,
      "learning_rate": 4.839397287417652e-05,
      "loss": 0.0434,
      "step": 126
    },
    {
      "epoch": 0.7134831460674157,
      "grad_norm": 0.7180249691009521,
      "learning_rate": 4.83617248760568e-05,
      "loss": 0.0672,
      "step": 127
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.8795035481452942,
      "learning_rate": 4.832916729103427e-05,
      "loss": 0.0808,
      "step": 128
    },
    {
      "epoch": 0.7247191011235955,
      "grad_norm": 0.2898567318916321,
      "learning_rate": 4.829630055055829e-05,
      "loss": 0.0198,
      "step": 129
    },
    {
      "epoch": 0.7303370786516854,
      "grad_norm": 1.7427699565887451,
      "learning_rate": 4.8263125090175165e-05,
      "loss": 0.1651,
      "step": 130
    },
    {
      "epoch": 0.7359550561797753,
      "grad_norm": 0.6721501350402832,
      "learning_rate": 4.822964134952229e-05,
      "loss": 0.0537,
      "step": 131
    },
    {
      "epoch": 0.7415730337078652,
      "grad_norm": 0.540553629398346,
      "learning_rate": 4.8195849772322355e-05,
      "loss": 0.0665,
      "step": 132
    },
    {
      "epoch": 0.7471910112359551,
      "grad_norm": 1.7081308364868164,
      "learning_rate": 4.816175080637748e-05,
      "loss": 0.031,
      "step": 133
    },
    {
      "epoch": 0.7528089887640449,
      "grad_norm": 1.1550614833831787,
      "learning_rate": 4.812734490356325e-05,
      "loss": 0.09,
      "step": 134
    },
    {
      "epoch": 0.7584269662921348,
      "grad_norm": 2.8958280086517334,
      "learning_rate": 4.809263251982275e-05,
      "loss": 0.0798,
      "step": 135
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 1.088889479637146,
      "learning_rate": 4.805761411516053e-05,
      "loss": 0.0764,
      "step": 136
    },
    {
      "epoch": 0.7696629213483146,
      "grad_norm": 0.198432058095932,
      "learning_rate": 4.802229015363646e-05,
      "loss": 0.0143,
      "step": 137
    },
    {
      "epoch": 0.7752808988764045,
      "grad_norm": 0.5208020806312561,
      "learning_rate": 4.798666110335963e-05,
      "loss": 0.0394,
      "step": 138
    },
    {
      "epoch": 0.7808988764044944,
      "grad_norm": 0.3570554852485657,
      "learning_rate": 4.795072743648215e-05,
      "loss": 0.0575,
      "step": 139
    },
    {
      "epoch": 0.7865168539325843,
      "grad_norm": 0.7743752002716064,
      "learning_rate": 4.791448962919285e-05,
      "loss": 0.0313,
      "step": 140
    },
    {
      "epoch": 0.7921348314606742,
      "grad_norm": 0.9323815703392029,
      "learning_rate": 4.7877948161711006e-05,
      "loss": 0.0442,
      "step": 141
    },
    {
      "epoch": 0.797752808988764,
      "grad_norm": 1.0537489652633667,
      "learning_rate": 4.784110351827996e-05,
      "loss": 0.0936,
      "step": 142
    },
    {
      "epoch": 0.8033707865168539,
      "grad_norm": 0.9777823686599731,
      "learning_rate": 4.78039561871607e-05,
      "loss": 0.0493,
      "step": 143
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.20168936252593994,
      "learning_rate": 4.776650666062542e-05,
      "loss": 0.0286,
      "step": 144
    },
    {
      "epoch": 0.8146067415730337,
      "grad_norm": 0.8110122084617615,
      "learning_rate": 4.772875543495093e-05,
      "loss": 0.1479,
      "step": 145
    },
    {
      "epoch": 0.8202247191011236,
      "grad_norm": 0.26129940152168274,
      "learning_rate": 4.7690703010412185e-05,
      "loss": 0.0192,
      "step": 146
    },
    {
      "epoch": 0.8258426966292135,
      "grad_norm": 0.35092663764953613,
      "learning_rate": 4.765234989127553e-05,
      "loss": 0.0224,
      "step": 147
    },
    {
      "epoch": 0.8314606741573034,
      "grad_norm": 0.9850471615791321,
      "learning_rate": 4.761369658579212e-05,
      "loss": 0.1195,
      "step": 148
    },
    {
      "epoch": 0.8370786516853933,
      "grad_norm": 1.1359078884124756,
      "learning_rate": 4.757474360619113e-05,
      "loss": 0.0458,
      "step": 149
    },
    {
      "epoch": 0.8426966292134831,
      "grad_norm": 0.5832824110984802,
      "learning_rate": 4.753549146867299e-05,
      "loss": 0.0919,
      "step": 150
    },
    {
      "epoch": 0.848314606741573,
      "grad_norm": 0.3390987813472748,
      "learning_rate": 4.749594069340252e-05,
      "loss": 0.05,
      "step": 151
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.8122544884681702,
      "learning_rate": 4.7456091804502064e-05,
      "loss": 0.03,
      "step": 152
    },
    {
      "epoch": 0.8595505617977528,
      "grad_norm": 0.5129034519195557,
      "learning_rate": 4.7415945330044556e-05,
      "loss": 0.0814,
      "step": 153
    },
    {
      "epoch": 0.8651685393258427,
      "grad_norm": 0.25234270095825195,
      "learning_rate": 4.737550180204646e-05,
      "loss": 0.0237,
      "step": 154
    },
    {
      "epoch": 0.8707865168539326,
      "grad_norm": 0.25768542289733887,
      "learning_rate": 4.7334761756460784e-05,
      "loss": 0.0366,
      "step": 155
    },
    {
      "epoch": 0.8764044943820225,
      "grad_norm": 0.323933869600296,
      "learning_rate": 4.729372573316993e-05,
      "loss": 0.0365,
      "step": 156
    },
    {
      "epoch": 0.8820224719101124,
      "grad_norm": 0.2379804402589798,
      "learning_rate": 4.725239427597862e-05,
      "loss": 0.0317,
      "step": 157
    },
    {
      "epoch": 0.8876404494382022,
      "grad_norm": 0.31233593821525574,
      "learning_rate": 4.7210767932606546e-05,
      "loss": 0.0511,
      "step": 158
    },
    {
      "epoch": 0.8932584269662921,
      "grad_norm": 0.9454509019851685,
      "learning_rate": 4.716884725468127e-05,
      "loss": 0.1362,
      "step": 159
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.38527336716651917,
      "learning_rate": 4.712663279773081e-05,
      "loss": 0.0318,
      "step": 160
    },
    {
      "epoch": 0.9044943820224719,
      "grad_norm": 0.22842001914978027,
      "learning_rate": 4.708412512117631e-05,
      "loss": 0.0252,
      "step": 161
    },
    {
      "epoch": 0.9101123595505618,
      "grad_norm": 0.40106630325317383,
      "learning_rate": 4.704132478832464e-05,
      "loss": 0.0574,
      "step": 162
    },
    {
      "epoch": 0.9157303370786517,
      "grad_norm": 0.2728044092655182,
      "learning_rate": 4.699823236636091e-05,
      "loss": 0.0348,
      "step": 163
    },
    {
      "epoch": 0.9213483146067416,
      "grad_norm": 0.2112097293138504,
      "learning_rate": 4.695484842634094e-05,
      "loss": 0.0271,
      "step": 164
    },
    {
      "epoch": 0.9269662921348315,
      "grad_norm": 0.5256819725036621,
      "learning_rate": 4.691117354318377e-05,
      "loss": 0.0386,
      "step": 165
    },
    {
      "epoch": 0.9325842696629213,
      "grad_norm": 0.2981692850589752,
      "learning_rate": 4.6867208295663924e-05,
      "loss": 0.0274,
      "step": 166
    },
    {
      "epoch": 0.9382022471910112,
      "grad_norm": 0.5042353868484497,
      "learning_rate": 4.6822953266403836e-05,
      "loss": 0.0544,
      "step": 167
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.6675478219985962,
      "learning_rate": 4.677840904186609e-05,
      "loss": 0.0729,
      "step": 168
    },
    {
      "epoch": 0.949438202247191,
      "grad_norm": 2.249640703201294,
      "learning_rate": 4.673357621234564e-05,
      "loss": 0.118,
      "step": 169
    },
    {
      "epoch": 0.9550561797752809,
      "grad_norm": 0.45289111137390137,
      "learning_rate": 4.668845537196201e-05,
      "loss": 0.056,
      "step": 170
    },
    {
      "epoch": 0.9606741573033708,
      "grad_norm": 0.37050119042396545,
      "learning_rate": 4.664304711865143e-05,
      "loss": 0.0285,
      "step": 171
    },
    {
      "epoch": 0.9662921348314607,
      "grad_norm": 9.227156639099121,
      "learning_rate": 4.659735205415884e-05,
      "loss": 0.3314,
      "step": 172
    },
    {
      "epoch": 0.9719101123595506,
      "grad_norm": 0.23098067939281464,
      "learning_rate": 4.655137078403002e-05,
      "loss": 0.0296,
      "step": 173
    },
    {
      "epoch": 0.9775280898876404,
      "grad_norm": 0.5160046815872192,
      "learning_rate": 4.65051039176035e-05,
      "loss": 0.0531,
      "step": 174
    },
    {
      "epoch": 0.9831460674157303,
      "grad_norm": 0.63209468126297,
      "learning_rate": 4.645855206800249e-05,
      "loss": 0.0306,
      "step": 175
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.5950393676757812,
      "learning_rate": 4.641171585212677e-05,
      "loss": 0.0542,
      "step": 176
    },
    {
      "epoch": 0.9943820224719101,
      "grad_norm": 0.36585909128189087,
      "learning_rate": 4.6364595890644505e-05,
      "loss": 0.0299,
      "step": 177
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5497788190841675,
      "learning_rate": 4.631719280798403e-05,
      "loss": 0.0329,
      "step": 178
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.05561840534210205,
      "eval_runtime": 103.3692,
      "eval_samples_per_second": 0.9,
      "eval_steps_per_second": 0.455,
      "step": 178
    },
    {
      "epoch": 1.0056179775280898,
      "grad_norm": 0.2808246910572052,
      "learning_rate": 4.626950723232558e-05,
      "loss": 0.0268,
      "step": 179
    },
    {
      "epoch": 1.0112359550561798,
      "grad_norm": 0.5470560193061829,
      "learning_rate": 4.622153979559294e-05,
      "loss": 0.0347,
      "step": 180
    },
    {
      "epoch": 1.0168539325842696,
      "grad_norm": 0.4230470061302185,
      "learning_rate": 4.6173291133445076e-05,
      "loss": 0.0275,
      "step": 181
    },
    {
      "epoch": 1.0224719101123596,
      "grad_norm": 0.23552711308002472,
      "learning_rate": 4.612476188526773e-05,
      "loss": 0.023,
      "step": 182
    },
    {
      "epoch": 1.0280898876404494,
      "grad_norm": 0.7489832639694214,
      "learning_rate": 4.607595269416497e-05,
      "loss": 0.0335,
      "step": 183
    },
    {
      "epoch": 1.0337078651685394,
      "grad_norm": 0.3632313013076782,
      "learning_rate": 4.602686420695058e-05,
      "loss": 0.0204,
      "step": 184
    },
    {
      "epoch": 1.0393258426966292,
      "grad_norm": 0.7956604361534119,
      "learning_rate": 4.597749707413958e-05,
      "loss": 0.0832,
      "step": 185
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.43568918108940125,
      "learning_rate": 4.592785194993956e-05,
      "loss": 0.0471,
      "step": 186
    },
    {
      "epoch": 1.050561797752809,
      "grad_norm": 0.7191663980484009,
      "learning_rate": 4.587792949224203e-05,
      "loss": 0.1277,
      "step": 187
    },
    {
      "epoch": 1.0561797752808988,
      "grad_norm": 0.3718462288379669,
      "learning_rate": 4.582773036261368e-05,
      "loss": 0.04,
      "step": 188
    },
    {
      "epoch": 1.0617977528089888,
      "grad_norm": 0.9538937211036682,
      "learning_rate": 4.5777255226287606e-05,
      "loss": 0.1497,
      "step": 189
    },
    {
      "epoch": 1.0674157303370786,
      "grad_norm": 0.4178251028060913,
      "learning_rate": 4.572650475215456e-05,
      "loss": 0.0692,
      "step": 190
    },
    {
      "epoch": 1.0730337078651686,
      "grad_norm": 0.4339239001274109,
      "learning_rate": 4.5675479612754004e-05,
      "loss": 0.0705,
      "step": 191
    },
    {
      "epoch": 1.0786516853932584,
      "grad_norm": 0.34431013464927673,
      "learning_rate": 4.562418048426524e-05,
      "loss": 0.0234,
      "step": 192
    },
    {
      "epoch": 1.0842696629213484,
      "grad_norm": 0.3082885146141052,
      "learning_rate": 4.557260804649845e-05,
      "loss": 0.0219,
      "step": 193
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.5496937036514282,
      "learning_rate": 4.552076298288567e-05,
      "loss": 0.0572,
      "step": 194
    },
    {
      "epoch": 1.095505617977528,
      "grad_norm": 0.17891891300678253,
      "learning_rate": 4.546864598047175e-05,
      "loss": 0.0196,
      "step": 195
    },
    {
      "epoch": 1.101123595505618,
      "grad_norm": 0.2173370122909546,
      "learning_rate": 4.541625772990523e-05,
      "loss": 0.0173,
      "step": 196
    },
    {
      "epoch": 1.1067415730337078,
      "grad_norm": 0.334079384803772,
      "learning_rate": 4.536359892542921e-05,
      "loss": 0.0356,
      "step": 197
    },
    {
      "epoch": 1.1123595505617978,
      "grad_norm": 0.17630337178707123,
      "learning_rate": 4.531067026487214e-05,
      "loss": 0.0149,
      "step": 198
    },
    {
      "epoch": 1.1179775280898876,
      "grad_norm": 0.23701462149620056,
      "learning_rate": 4.525747244963857e-05,
      "loss": 0.0289,
      "step": 199
    },
    {
      "epoch": 1.1235955056179776,
      "grad_norm": 0.518445611000061,
      "learning_rate": 4.520400618469985e-05,
      "loss": 0.0759,
      "step": 200
    },
    {
      "epoch": 1.1292134831460674,
      "grad_norm": 0.6409814357757568,
      "learning_rate": 4.5150272178584805e-05,
      "loss": 0.0991,
      "step": 201
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.12929822504520416,
      "learning_rate": 4.5096271143370326e-05,
      "loss": 0.0111,
      "step": 202
    },
    {
      "epoch": 1.1404494382022472,
      "grad_norm": 0.4262920320034027,
      "learning_rate": 4.5042003794671954e-05,
      "loss": 0.0394,
      "step": 203
    },
    {
      "epoch": 1.146067415730337,
      "grad_norm": 0.5994799137115479,
      "learning_rate": 4.4987470851634376e-05,
      "loss": 0.0792,
      "step": 204
    },
    {
      "epoch": 1.151685393258427,
      "grad_norm": 0.3921738862991333,
      "learning_rate": 4.493267303692191e-05,
      "loss": 0.0448,
      "step": 205
    },
    {
      "epoch": 1.1573033707865168,
      "grad_norm": 0.2578849494457245,
      "learning_rate": 4.487761107670891e-05,
      "loss": 0.03,
      "step": 206
    },
    {
      "epoch": 1.1629213483146068,
      "grad_norm": 0.2654309570789337,
      "learning_rate": 4.48222857006702e-05,
      "loss": 0.0475,
      "step": 207
    },
    {
      "epoch": 1.1685393258426966,
      "grad_norm": 0.6429484486579895,
      "learning_rate": 4.476669764197129e-05,
      "loss": 0.0484,
      "step": 208
    },
    {
      "epoch": 1.1741573033707866,
      "grad_norm": 0.6409008502960205,
      "learning_rate": 4.471084763725878e-05,
      "loss": 0.061,
      "step": 209
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.7671300172805786,
      "learning_rate": 4.4654736426650546e-05,
      "loss": 0.102,
      "step": 210
    },
    {
      "epoch": 1.1853932584269664,
      "grad_norm": 0.3204667270183563,
      "learning_rate": 4.459836475372592e-05,
      "loss": 0.0241,
      "step": 211
    },
    {
      "epoch": 1.1910112359550562,
      "grad_norm": 0.40738534927368164,
      "learning_rate": 4.4541733365515856e-05,
      "loss": 0.0641,
      "step": 212
    },
    {
      "epoch": 1.196629213483146,
      "grad_norm": 0.39278021454811096,
      "learning_rate": 4.448484301249303e-05,
      "loss": 0.0532,
      "step": 213
    },
    {
      "epoch": 1.202247191011236,
      "grad_norm": 0.32997360825538635,
      "learning_rate": 4.4427694448561874e-05,
      "loss": 0.0354,
      "step": 214
    },
    {
      "epoch": 1.2078651685393258,
      "grad_norm": 0.8033084273338318,
      "learning_rate": 4.437028843104863e-05,
      "loss": 0.0596,
      "step": 215
    },
    {
      "epoch": 1.2134831460674158,
      "grad_norm": 0.442534863948822,
      "learning_rate": 4.431262572069125e-05,
      "loss": 0.0455,
      "step": 216
    },
    {
      "epoch": 1.2191011235955056,
      "grad_norm": 0.5122983455657959,
      "learning_rate": 4.425470708162938e-05,
      "loss": 0.0778,
      "step": 217
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.2509024143218994,
      "learning_rate": 4.4196533281394184e-05,
      "loss": 0.0375,
      "step": 218
    },
    {
      "epoch": 1.2303370786516854,
      "grad_norm": 0.4790158271789551,
      "learning_rate": 4.413810509089822e-05,
      "loss": 0.0796,
      "step": 219
    },
    {
      "epoch": 1.2359550561797752,
      "grad_norm": 0.5011918544769287,
      "learning_rate": 4.407942328442515e-05,
      "loss": 0.0507,
      "step": 220
    },
    {
      "epoch": 1.2415730337078652,
      "grad_norm": 0.8239577412605286,
      "learning_rate": 4.402048863961957e-05,
      "loss": 0.0761,
      "step": 221
    },
    {
      "epoch": 1.247191011235955,
      "grad_norm": 0.5281482934951782,
      "learning_rate": 4.396130193747665e-05,
      "loss": 0.0485,
      "step": 222
    },
    {
      "epoch": 1.252808988764045,
      "grad_norm": 0.3557882606983185,
      "learning_rate": 4.390186396233178e-05,
      "loss": 0.0277,
      "step": 223
    },
    {
      "epoch": 1.2584269662921348,
      "grad_norm": 0.4202120900154114,
      "learning_rate": 4.384217550185019e-05,
      "loss": 0.0587,
      "step": 224
    },
    {
      "epoch": 1.2640449438202248,
      "grad_norm": 0.5591136813163757,
      "learning_rate": 4.378223734701655e-05,
      "loss": 0.0485,
      "step": 225
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.4178372323513031,
      "learning_rate": 4.3722050292124393e-05,
      "loss": 0.046,
      "step": 226
    },
    {
      "epoch": 1.2752808988764044,
      "grad_norm": 0.499774307012558,
      "learning_rate": 4.36616151347657e-05,
      "loss": 0.0463,
      "step": 227
    },
    {
      "epoch": 1.2808988764044944,
      "grad_norm": 0.33003881573677063,
      "learning_rate": 4.360093267582025e-05,
      "loss": 0.047,
      "step": 228
    },
    {
      "epoch": 1.2865168539325842,
      "grad_norm": 0.266189843416214,
      "learning_rate": 4.354000371944505e-05,
      "loss": 0.0308,
      "step": 229
    },
    {
      "epoch": 1.2921348314606742,
      "grad_norm": 3.51526141166687,
      "learning_rate": 4.347882907306362e-05,
      "loss": 0.1082,
      "step": 230
    },
    {
      "epoch": 1.297752808988764,
      "grad_norm": 0.3949393928050995,
      "learning_rate": 4.34174095473554e-05,
      "loss": 0.0277,
      "step": 231
    },
    {
      "epoch": 1.303370786516854,
      "grad_norm": 0.32806286215782166,
      "learning_rate": 4.335574595624491e-05,
      "loss": 0.024,
      "step": 232
    },
    {
      "epoch": 1.3089887640449438,
      "grad_norm": 0.5557664036750793,
      "learning_rate": 4.329383911689098e-05,
      "loss": 0.0342,
      "step": 233
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.300744891166687,
      "learning_rate": 4.323168984967597e-05,
      "loss": 0.0347,
      "step": 234
    },
    {
      "epoch": 1.3202247191011236,
      "grad_norm": 0.3212307095527649,
      "learning_rate": 4.316929897819485e-05,
      "loss": 0.0387,
      "step": 235
    },
    {
      "epoch": 1.3258426966292136,
      "grad_norm": 0.1302984654903412,
      "learning_rate": 4.31066673292443e-05,
      "loss": 0.0151,
      "step": 236
    },
    {
      "epoch": 1.3314606741573034,
      "grad_norm": 0.1968306601047516,
      "learning_rate": 4.3043795732811756e-05,
      "loss": 0.0143,
      "step": 237
    },
    {
      "epoch": 1.3370786516853932,
      "grad_norm": 0.401540070772171,
      "learning_rate": 4.298068502206443e-05,
      "loss": 0.0586,
      "step": 238
    },
    {
      "epoch": 1.3426966292134832,
      "grad_norm": 0.15350283682346344,
      "learning_rate": 4.2917336033338216e-05,
      "loss": 0.0097,
      "step": 239
    },
    {
      "epoch": 1.348314606741573,
      "grad_norm": 0.3398582637310028,
      "learning_rate": 4.285374960612667e-05,
      "loss": 0.0356,
      "step": 240
    },
    {
      "epoch": 1.3539325842696628,
      "grad_norm": 0.3363010287284851,
      "learning_rate": 4.278992658306984e-05,
      "loss": 0.0324,
      "step": 241
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.28575387597084045,
      "learning_rate": 4.272586780994312e-05,
      "loss": 0.0306,
      "step": 242
    },
    {
      "epoch": 1.3651685393258428,
      "grad_norm": 0.5423350930213928,
      "learning_rate": 4.266157413564603e-05,
      "loss": 0.0648,
      "step": 243
    },
    {
      "epoch": 1.3707865168539326,
      "grad_norm": 1.039638638496399,
      "learning_rate": 4.259704641219099e-05,
      "loss": 0.1369,
      "step": 244
    },
    {
      "epoch": 1.3764044943820224,
      "grad_norm": 0.4026361107826233,
      "learning_rate": 4.2532285494692e-05,
      "loss": 0.0471,
      "step": 245
    },
    {
      "epoch": 1.3820224719101124,
      "grad_norm": 0.14110831916332245,
      "learning_rate": 4.246729224135331e-05,
      "loss": 0.0144,
      "step": 246
    },
    {
      "epoch": 1.3876404494382022,
      "grad_norm": 0.16889607906341553,
      "learning_rate": 4.2402067513458086e-05,
      "loss": 0.0273,
      "step": 247
    },
    {
      "epoch": 1.3932584269662922,
      "grad_norm": 0.4563111960887909,
      "learning_rate": 4.233661217535694e-05,
      "loss": 0.0302,
      "step": 248
    },
    {
      "epoch": 1.398876404494382,
      "grad_norm": 0.248752623796463,
      "learning_rate": 4.227092709445652e-05,
      "loss": 0.0263,
      "step": 249
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.48819175362586975,
      "learning_rate": 4.220501314120802e-05,
      "loss": 0.0758,
      "step": 250
    },
    {
      "epoch": 1.4101123595505618,
      "grad_norm": 0.6088745594024658,
      "learning_rate": 4.213887118909556e-05,
      "loss": 0.0747,
      "step": 251
    },
    {
      "epoch": 1.4157303370786516,
      "grad_norm": 0.5962110161781311,
      "learning_rate": 4.207250211462475e-05,
      "loss": 0.0708,
      "step": 252
    },
    {
      "epoch": 1.4213483146067416,
      "grad_norm": 0.46965405344963074,
      "learning_rate": 4.2005906797310934e-05,
      "loss": 0.0801,
      "step": 253
    },
    {
      "epoch": 1.4269662921348314,
      "grad_norm": 0.47563856840133667,
      "learning_rate": 4.193908611966766e-05,
      "loss": 0.0703,
      "step": 254
    },
    {
      "epoch": 1.4325842696629214,
      "grad_norm": 0.5125031471252441,
      "learning_rate": 4.187204096719488e-05,
      "loss": 0.034,
      "step": 255
    },
    {
      "epoch": 1.4382022471910112,
      "grad_norm": 0.33822277188301086,
      "learning_rate": 4.180477222836727e-05,
      "loss": 0.02,
      "step": 256
    },
    {
      "epoch": 1.4438202247191012,
      "grad_norm": 0.45690417289733887,
      "learning_rate": 4.173728079462248e-05,
      "loss": 0.0292,
      "step": 257
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.2078075110912323,
      "learning_rate": 4.166956756034924e-05,
      "loss": 0.0254,
      "step": 258
    },
    {
      "epoch": 1.4550561797752808,
      "grad_norm": 0.3212522566318512,
      "learning_rate": 4.160163342287558e-05,
      "loss": 0.0219,
      "step": 259
    },
    {
      "epoch": 1.4606741573033708,
      "grad_norm": 0.249989852309227,
      "learning_rate": 4.153347928245691e-05,
      "loss": 0.0225,
      "step": 260
    },
    {
      "epoch": 1.4662921348314606,
      "grad_norm": 0.3221660852432251,
      "learning_rate": 4.14651060422641e-05,
      "loss": 0.0383,
      "step": 261
    },
    {
      "epoch": 1.4719101123595506,
      "grad_norm": 0.5261919498443604,
      "learning_rate": 4.13965146083715e-05,
      "loss": 0.0348,
      "step": 262
    },
    {
      "epoch": 1.4775280898876404,
      "grad_norm": 0.20916993916034698,
      "learning_rate": 4.132770588974493e-05,
      "loss": 0.0166,
      "step": 263
    },
    {
      "epoch": 1.4831460674157304,
      "grad_norm": 0.506365180015564,
      "learning_rate": 4.125868079822966e-05,
      "loss": 0.0515,
      "step": 264
    },
    {
      "epoch": 1.4887640449438202,
      "grad_norm": 0.2886394262313843,
      "learning_rate": 4.1189440248538276e-05,
      "loss": 0.0217,
      "step": 265
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.7425514459609985,
      "learning_rate": 4.111998515823864e-05,
      "loss": 0.0845,
      "step": 266
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.35573235154151917,
      "learning_rate": 4.1050316447741636e-05,
      "loss": 0.0302,
      "step": 267
    },
    {
      "epoch": 1.50561797752809,
      "grad_norm": 0.20357748866081238,
      "learning_rate": 4.0980435040289055e-05,
      "loss": 0.0174,
      "step": 268
    },
    {
      "epoch": 1.5112359550561798,
      "grad_norm": 0.28822940587997437,
      "learning_rate": 4.0910341861941295e-05,
      "loss": 0.0227,
      "step": 269
    },
    {
      "epoch": 1.5168539325842696,
      "grad_norm": 0.807671844959259,
      "learning_rate": 4.084003784156513e-05,
      "loss": 0.0529,
      "step": 270
    },
    {
      "epoch": 1.5224719101123596,
      "grad_norm": 0.2197992354631424,
      "learning_rate": 4.076952391082138e-05,
      "loss": 0.0171,
      "step": 271
    },
    {
      "epoch": 1.5280898876404494,
      "grad_norm": 0.5840113759040833,
      "learning_rate": 4.069880100415259e-05,
      "loss": 0.0452,
      "step": 272
    },
    {
      "epoch": 1.5337078651685392,
      "grad_norm": 0.12770934402942657,
      "learning_rate": 4.0627870058770624e-05,
      "loss": 0.0167,
      "step": 273
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.22525456547737122,
      "learning_rate": 4.055673201464424e-05,
      "loss": 0.0145,
      "step": 274
    },
    {
      "epoch": 1.5449438202247192,
      "grad_norm": 0.5041562914848328,
      "learning_rate": 4.0485387814486656e-05,
      "loss": 0.0614,
      "step": 275
    },
    {
      "epoch": 1.550561797752809,
      "grad_norm": 0.19805487990379333,
      "learning_rate": 4.0413838403743045e-05,
      "loss": 0.0192,
      "step": 276
    },
    {
      "epoch": 1.5561797752808988,
      "grad_norm": 0.7608011960983276,
      "learning_rate": 4.0342084730577996e-05,
      "loss": 0.0838,
      "step": 277
    },
    {
      "epoch": 1.5617977528089888,
      "grad_norm": 0.28078192472457886,
      "learning_rate": 4.027012774586297e-05,
      "loss": 0.0236,
      "step": 278
    },
    {
      "epoch": 1.5674157303370788,
      "grad_norm": 0.660754382610321,
      "learning_rate": 4.01979684031637e-05,
      "loss": 0.0863,
      "step": 279
    },
    {
      "epoch": 1.5730337078651684,
      "grad_norm": 0.2808799147605896,
      "learning_rate": 4.012560765872753e-05,
      "loss": 0.0225,
      "step": 280
    },
    {
      "epoch": 1.5786516853932584,
      "grad_norm": 0.2777266800403595,
      "learning_rate": 4.0053046471470746e-05,
      "loss": 0.0152,
      "step": 281
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 0.24774739146232605,
      "learning_rate": 3.99802858029659e-05,
      "loss": 0.0244,
      "step": 282
    },
    {
      "epoch": 1.5898876404494382,
      "grad_norm": 0.6554973721504211,
      "learning_rate": 3.990732661742904e-05,
      "loss": 0.0743,
      "step": 283
    },
    {
      "epoch": 1.595505617977528,
      "grad_norm": 0.4039965867996216,
      "learning_rate": 3.983416988170694e-05,
      "loss": 0.0603,
      "step": 284
    },
    {
      "epoch": 1.601123595505618,
      "grad_norm": 0.5284679532051086,
      "learning_rate": 3.976081656526428e-05,
      "loss": 0.0669,
      "step": 285
    },
    {
      "epoch": 1.606741573033708,
      "grad_norm": 0.5645961761474609,
      "learning_rate": 3.96872676401708e-05,
      "loss": 0.041,
      "step": 286
    },
    {
      "epoch": 1.6123595505617978,
      "grad_norm": 0.7564839124679565,
      "learning_rate": 3.961352408108846e-05,
      "loss": 0.1134,
      "step": 287
    },
    {
      "epoch": 1.6179775280898876,
      "grad_norm": 0.5132441520690918,
      "learning_rate": 3.953958686525844e-05,
      "loss": 0.0347,
      "step": 288
    },
    {
      "epoch": 1.6235955056179776,
      "grad_norm": 0.4154549241065979,
      "learning_rate": 3.9465456972488254e-05,
      "loss": 0.0304,
      "step": 289
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 0.47438856959342957,
      "learning_rate": 3.9391135385138765e-05,
      "loss": 0.0608,
      "step": 290
    },
    {
      "epoch": 1.6348314606741572,
      "grad_norm": 0.16102160513401031,
      "learning_rate": 3.931662308811114e-05,
      "loss": 0.0192,
      "step": 291
    },
    {
      "epoch": 1.6404494382022472,
      "grad_norm": 0.3559429943561554,
      "learning_rate": 3.9241921068833785e-05,
      "loss": 0.0376,
      "step": 292
    },
    {
      "epoch": 1.6460674157303372,
      "grad_norm": 0.3562780022621155,
      "learning_rate": 3.9167030317249334e-05,
      "loss": 0.0182,
      "step": 293
    },
    {
      "epoch": 1.651685393258427,
      "grad_norm": 0.5047184824943542,
      "learning_rate": 3.909195182580142e-05,
      "loss": 0.0624,
      "step": 294
    },
    {
      "epoch": 1.6573033707865168,
      "grad_norm": 0.34567469358444214,
      "learning_rate": 3.9016686589421636e-05,
      "loss": 0.0513,
      "step": 295
    },
    {
      "epoch": 1.6629213483146068,
      "grad_norm": 0.18133847415447235,
      "learning_rate": 3.894123560551627e-05,
      "loss": 0.022,
      "step": 296
    },
    {
      "epoch": 1.6685393258426966,
      "grad_norm": 0.2926389276981354,
      "learning_rate": 3.886559987395313e-05,
      "loss": 0.0179,
      "step": 297
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 0.18593452870845795,
      "learning_rate": 3.878978039704825e-05,
      "loss": 0.0221,
      "step": 298
    },
    {
      "epoch": 1.6797752808988764,
      "grad_norm": 0.5562571883201599,
      "learning_rate": 3.8713778179552664e-05,
      "loss": 0.0332,
      "step": 299
    },
    {
      "epoch": 1.6853932584269664,
      "grad_norm": 0.34042325615882874,
      "learning_rate": 3.863759422863906e-05,
      "loss": 0.0197,
      "step": 300
    },
    {
      "epoch": 1.6910112359550562,
      "grad_norm": 0.2651381492614746,
      "learning_rate": 3.856122955388842e-05,
      "loss": 0.0223,
      "step": 301
    },
    {
      "epoch": 1.696629213483146,
      "grad_norm": 0.3503357768058777,
      "learning_rate": 3.848468516727669e-05,
      "loss": 0.0193,
      "step": 302
    },
    {
      "epoch": 1.702247191011236,
      "grad_norm": 0.3834904432296753,
      "learning_rate": 3.84079620831613e-05,
      "loss": 0.0254,
      "step": 303
    },
    {
      "epoch": 1.7078651685393258,
      "grad_norm": 0.4781907796859741,
      "learning_rate": 3.833106131826777e-05,
      "loss": 0.027,
      "step": 304
    },
    {
      "epoch": 1.7134831460674156,
      "grad_norm": 0.38403838872909546,
      "learning_rate": 3.8253983891676236e-05,
      "loss": 0.0334,
      "step": 305
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 0.4704713821411133,
      "learning_rate": 3.817673082480794e-05,
      "loss": 0.0511,
      "step": 306
    },
    {
      "epoch": 1.7247191011235956,
      "grad_norm": 0.42957043647766113,
      "learning_rate": 3.809930314141166e-05,
      "loss": 0.0391,
      "step": 307
    },
    {
      "epoch": 1.7303370786516854,
      "grad_norm": 1.5398266315460205,
      "learning_rate": 3.802170186755021e-05,
      "loss": 0.1789,
      "step": 308
    },
    {
      "epoch": 1.7359550561797752,
      "grad_norm": 0.4734722673892975,
      "learning_rate": 3.7943928031586764e-05,
      "loss": 0.0512,
      "step": 309
    },
    {
      "epoch": 1.7415730337078652,
      "grad_norm": 0.47735679149627686,
      "learning_rate": 3.78659826641713e-05,
      "loss": 0.0599,
      "step": 310
    },
    {
      "epoch": 1.7471910112359552,
      "grad_norm": 0.6093922853469849,
      "learning_rate": 3.77878667982269e-05,
      "loss": 0.0729,
      "step": 311
    },
    {
      "epoch": 1.7528089887640448,
      "grad_norm": 0.4764096438884735,
      "learning_rate": 3.770958146893607e-05,
      "loss": 0.0617,
      "step": 312
    },
    {
      "epoch": 1.7584269662921348,
      "grad_norm": 0.49107518792152405,
      "learning_rate": 3.763112771372704e-05,
      "loss": 0.0281,
      "step": 313
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 0.2601630985736847,
      "learning_rate": 3.7552506572259985e-05,
      "loss": 0.0151,
      "step": 314
    },
    {
      "epoch": 1.7696629213483146,
      "grad_norm": 0.3130832016468048,
      "learning_rate": 3.7473719086413256e-05,
      "loss": 0.0297,
      "step": 315
    },
    {
      "epoch": 1.7752808988764044,
      "grad_norm": 0.7046939730644226,
      "learning_rate": 3.739476630026961e-05,
      "loss": 0.0366,
      "step": 316
    },
    {
      "epoch": 1.7808988764044944,
      "grad_norm": 0.379118412733078,
      "learning_rate": 3.7315649260102323e-05,
      "loss": 0.0265,
      "step": 317
    },
    {
      "epoch": 1.7865168539325844,
      "grad_norm": 0.37926414608955383,
      "learning_rate": 3.723636901436135e-05,
      "loss": 0.0266,
      "step": 318
    },
    {
      "epoch": 1.7921348314606742,
      "grad_norm": 0.21586526930332184,
      "learning_rate": 3.715692661365943e-05,
      "loss": 0.039,
      "step": 319
    },
    {
      "epoch": 1.797752808988764,
      "grad_norm": 0.20323330163955688,
      "learning_rate": 3.707732311075816e-05,
      "loss": 0.0231,
      "step": 320
    },
    {
      "epoch": 1.803370786516854,
      "grad_norm": 0.2680882215499878,
      "learning_rate": 3.699755956055403e-05,
      "loss": 0.0236,
      "step": 321
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 0.19846676290035248,
      "learning_rate": 3.69176370200645e-05,
      "loss": 0.0203,
      "step": 322
    },
    {
      "epoch": 1.8146067415730336,
      "grad_norm": 0.51865553855896,
      "learning_rate": 3.683755654841391e-05,
      "loss": 0.0656,
      "step": 323
    },
    {
      "epoch": 1.8202247191011236,
      "grad_norm": 0.41585421562194824,
      "learning_rate": 3.67573192068195e-05,
      "loss": 0.0483,
      "step": 324
    },
    {
      "epoch": 1.8258426966292136,
      "grad_norm": 0.5326073169708252,
      "learning_rate": 3.6676926058577335e-05,
      "loss": 0.0369,
      "step": 325
    },
    {
      "epoch": 1.8314606741573034,
      "grad_norm": 0.7043209671974182,
      "learning_rate": 3.65963781690482e-05,
      "loss": 0.0761,
      "step": 326
    },
    {
      "epoch": 1.8370786516853932,
      "grad_norm": 0.24671199917793274,
      "learning_rate": 3.651567660564352e-05,
      "loss": 0.019,
      "step": 327
    },
    {
      "epoch": 1.8426966292134832,
      "grad_norm": 0.48376041650772095,
      "learning_rate": 3.643482243781116e-05,
      "loss": 0.0516,
      "step": 328
    },
    {
      "epoch": 1.848314606741573,
      "grad_norm": 0.42266568541526794,
      "learning_rate": 3.6353816737021305e-05,
      "loss": 0.0348,
      "step": 329
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 0.32763898372650146,
      "learning_rate": 3.62726605767522e-05,
      "loss": 0.0203,
      "step": 330
    },
    {
      "epoch": 1.8595505617977528,
      "grad_norm": 0.32239413261413574,
      "learning_rate": 3.6191355032476027e-05,
      "loss": 0.0285,
      "step": 331
    },
    {
      "epoch": 1.8651685393258428,
      "grad_norm": 0.3852097988128662,
      "learning_rate": 3.610990118164451e-05,
      "loss": 0.0268,
      "step": 332
    },
    {
      "epoch": 1.8707865168539326,
      "grad_norm": 0.3297015130519867,
      "learning_rate": 3.6028300103674775e-05,
      "loss": 0.0288,
      "step": 333
    },
    {
      "epoch": 1.8764044943820224,
      "grad_norm": 0.14118613302707672,
      "learning_rate": 3.594655287993495e-05,
      "loss": 0.0169,
      "step": 334
    },
    {
      "epoch": 1.8820224719101124,
      "grad_norm": 0.2367703914642334,
      "learning_rate": 3.5864660593729885e-05,
      "loss": 0.0172,
      "step": 335
    },
    {
      "epoch": 1.8876404494382022,
      "grad_norm": 0.36694878339767456,
      "learning_rate": 3.578262433028679e-05,
      "loss": 0.0327,
      "step": 336
    },
    {
      "epoch": 1.893258426966292,
      "grad_norm": 0.3126879036426544,
      "learning_rate": 3.570044517674083e-05,
      "loss": 0.0372,
      "step": 337
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 0.757376492023468,
      "learning_rate": 3.561812422212073e-05,
      "loss": 0.0463,
      "step": 338
    },
    {
      "epoch": 1.904494382022472,
      "grad_norm": 0.643761932849884,
      "learning_rate": 3.553566255733436e-05,
      "loss": 0.0523,
      "step": 339
    },
    {
      "epoch": 1.9101123595505618,
      "grad_norm": 0.22228731215000153,
      "learning_rate": 3.5453061275154256e-05,
      "loss": 0.0137,
      "step": 340
    },
    {
      "epoch": 1.9157303370786516,
      "grad_norm": 0.6729681491851807,
      "learning_rate": 3.537032147020314e-05,
      "loss": 0.0497,
      "step": 341
    },
    {
      "epoch": 1.9213483146067416,
      "grad_norm": 0.34131503105163574,
      "learning_rate": 3.528744423893946e-05,
      "loss": 0.047,
      "step": 342
    },
    {
      "epoch": 1.9269662921348316,
      "grad_norm": 0.39510706067085266,
      "learning_rate": 3.520443067964277e-05,
      "loss": 0.0566,
      "step": 343
    },
    {
      "epoch": 1.9325842696629212,
      "grad_norm": 0.23481391370296478,
      "learning_rate": 3.512128189239926e-05,
      "loss": 0.0374,
      "step": 344
    },
    {
      "epoch": 1.9382022471910112,
      "grad_norm": 0.26413944363594055,
      "learning_rate": 3.5037998979087125e-05,
      "loss": 0.0158,
      "step": 345
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 0.39763137698173523,
      "learning_rate": 3.495458304336203e-05,
      "loss": 0.0206,
      "step": 346
    },
    {
      "epoch": 1.949438202247191,
      "grad_norm": 0.24423715472221375,
      "learning_rate": 3.487103519064236e-05,
      "loss": 0.0201,
      "step": 347
    },
    {
      "epoch": 1.9550561797752808,
      "grad_norm": 0.32616570591926575,
      "learning_rate": 3.4787356528094724e-05,
      "loss": 0.0233,
      "step": 348
    },
    {
      "epoch": 1.9606741573033708,
      "grad_norm": 0.18867908418178558,
      "learning_rate": 3.470354816461915e-05,
      "loss": 0.0163,
      "step": 349
    },
    {
      "epoch": 1.9662921348314608,
      "grad_norm": 0.4708600342273712,
      "learning_rate": 3.4619611210834496e-05,
      "loss": 0.0209,
      "step": 350
    },
    {
      "epoch": 1.9719101123595506,
      "grad_norm": 0.48194265365600586,
      "learning_rate": 3.453554677906364e-05,
      "loss": 0.0402,
      "step": 351
    },
    {
      "epoch": 1.9775280898876404,
      "grad_norm": 0.479215145111084,
      "learning_rate": 3.445135598331883e-05,
      "loss": 0.0504,
      "step": 352
    },
    {
      "epoch": 1.9831460674157304,
      "grad_norm": 0.39985767006874084,
      "learning_rate": 3.4367039939286834e-05,
      "loss": 0.0449,
      "step": 353
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 0.6207613945007324,
      "learning_rate": 3.4282599764314224e-05,
      "loss": 0.0537,
      "step": 354
    },
    {
      "epoch": 1.99438202247191,
      "grad_norm": 0.5489397048950195,
      "learning_rate": 3.41980365773925e-05,
      "loss": 0.0634,
      "step": 355
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.549493134021759,
      "learning_rate": 3.4113351499143374e-05,
      "loss": 0.0275,
      "step": 356
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.04685436189174652,
      "eval_runtime": 107.3282,
      "eval_samples_per_second": 0.867,
      "eval_steps_per_second": 0.438,
      "step": 356
    },
    {
      "epoch": 2.00561797752809,
      "grad_norm": 0.6035869717597961,
      "learning_rate": 3.402854565180377e-05,
      "loss": 0.0273,
      "step": 357
    },
    {
      "epoch": 2.0112359550561796,
      "grad_norm": 0.3804537355899811,
      "learning_rate": 3.3943620159211084e-05,
      "loss": 0.0234,
      "step": 358
    },
    {
      "epoch": 2.0168539325842696,
      "grad_norm": 0.2684202194213867,
      "learning_rate": 3.385857614678821e-05,
      "loss": 0.0344,
      "step": 359
    },
    {
      "epoch": 2.0224719101123596,
      "grad_norm": 0.19483351707458496,
      "learning_rate": 3.3773414741528674e-05,
      "loss": 0.0115,
      "step": 360
    },
    {
      "epoch": 2.0280898876404496,
      "grad_norm": 0.1734911948442459,
      "learning_rate": 3.3688137071981654e-05,
      "loss": 0.0073,
      "step": 361
    },
    {
      "epoch": 2.033707865168539,
      "grad_norm": 0.14035043120384216,
      "learning_rate": 3.360274426823707e-05,
      "loss": 0.0124,
      "step": 362
    },
    {
      "epoch": 2.039325842696629,
      "grad_norm": 0.3437412977218628,
      "learning_rate": 3.3517237461910566e-05,
      "loss": 0.0183,
      "step": 363
    },
    {
      "epoch": 2.044943820224719,
      "grad_norm": 0.3674852252006531,
      "learning_rate": 3.3431617786128554e-05,
      "loss": 0.0415,
      "step": 364
    },
    {
      "epoch": 2.050561797752809,
      "grad_norm": 0.29533177614212036,
      "learning_rate": 3.334588637551318e-05,
      "loss": 0.0145,
      "step": 365
    },
    {
      "epoch": 2.056179775280899,
      "grad_norm": 0.4841676950454712,
      "learning_rate": 3.326004436616728e-05,
      "loss": 0.0587,
      "step": 366
    },
    {
      "epoch": 2.061797752808989,
      "grad_norm": 0.19446691870689392,
      "learning_rate": 3.317409289565932e-05,
      "loss": 0.0099,
      "step": 367
    },
    {
      "epoch": 2.067415730337079,
      "grad_norm": 0.1736321747303009,
      "learning_rate": 3.308803310300836e-05,
      "loss": 0.0095,
      "step": 368
    },
    {
      "epoch": 2.0730337078651684,
      "grad_norm": 0.379098117351532,
      "learning_rate": 3.30018661286689e-05,
      "loss": 0.045,
      "step": 369
    },
    {
      "epoch": 2.0786516853932584,
      "grad_norm": 0.3788038194179535,
      "learning_rate": 3.2915593114515834e-05,
      "loss": 0.0356,
      "step": 370
    },
    {
      "epoch": 2.0842696629213484,
      "grad_norm": 0.23692691326141357,
      "learning_rate": 3.2829215203829244e-05,
      "loss": 0.0108,
      "step": 371
    },
    {
      "epoch": 2.0898876404494384,
      "grad_norm": 0.5975455641746521,
      "learning_rate": 3.274273354127933e-05,
      "loss": 0.0666,
      "step": 372
    },
    {
      "epoch": 2.095505617977528,
      "grad_norm": 0.1453527808189392,
      "learning_rate": 3.265614927291117e-05,
      "loss": 0.0078,
      "step": 373
    },
    {
      "epoch": 2.101123595505618,
      "grad_norm": 0.4773653447628021,
      "learning_rate": 3.2569463546129595e-05,
      "loss": 0.0342,
      "step": 374
    },
    {
      "epoch": 2.106741573033708,
      "grad_norm": 0.3214028477668762,
      "learning_rate": 3.24826775096839e-05,
      "loss": 0.0402,
      "step": 375
    },
    {
      "epoch": 2.1123595505617976,
      "grad_norm": 0.8964955806732178,
      "learning_rate": 3.239579231365271e-05,
      "loss": 0.0915,
      "step": 376
    },
    {
      "epoch": 2.1179775280898876,
      "grad_norm": 0.26987701654434204,
      "learning_rate": 3.230880910942871e-05,
      "loss": 0.0109,
      "step": 377
    },
    {
      "epoch": 2.1235955056179776,
      "grad_norm": 0.3543303906917572,
      "learning_rate": 3.222172904970334e-05,
      "loss": 0.0207,
      "step": 378
    },
    {
      "epoch": 2.1292134831460676,
      "grad_norm": 0.5643791556358337,
      "learning_rate": 3.21345532884516e-05,
      "loss": 0.0642,
      "step": 379
    },
    {
      "epoch": 2.134831460674157,
      "grad_norm": 0.5213272571563721,
      "learning_rate": 3.2047282980916674e-05,
      "loss": 0.0686,
      "step": 380
    },
    {
      "epoch": 2.140449438202247,
      "grad_norm": 0.9149295687675476,
      "learning_rate": 3.1959919283594705e-05,
      "loss": 0.0745,
      "step": 381
    },
    {
      "epoch": 2.146067415730337,
      "grad_norm": 0.6409038305282593,
      "learning_rate": 3.187246335421938e-05,
      "loss": 0.0436,
      "step": 382
    },
    {
      "epoch": 2.151685393258427,
      "grad_norm": 0.24463313817977905,
      "learning_rate": 3.17849163517467e-05,
      "loss": 0.0308,
      "step": 383
    },
    {
      "epoch": 2.157303370786517,
      "grad_norm": 0.324336439371109,
      "learning_rate": 3.169727943633947e-05,
      "loss": 0.0205,
      "step": 384
    },
    {
      "epoch": 2.162921348314607,
      "grad_norm": 0.6124590039253235,
      "learning_rate": 3.160955376935208e-05,
      "loss": 0.0597,
      "step": 385
    },
    {
      "epoch": 2.168539325842697,
      "grad_norm": 0.22520744800567627,
      "learning_rate": 3.152174051331503e-05,
      "loss": 0.0206,
      "step": 386
    },
    {
      "epoch": 2.1741573033707864,
      "grad_norm": 0.3568345010280609,
      "learning_rate": 3.143384083191952e-05,
      "loss": 0.0219,
      "step": 387
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 0.3640081286430359,
      "learning_rate": 3.1345855890002066e-05,
      "loss": 0.0252,
      "step": 388
    },
    {
      "epoch": 2.1853932584269664,
      "grad_norm": 0.4077148139476776,
      "learning_rate": 3.1257786853529045e-05,
      "loss": 0.0404,
      "step": 389
    },
    {
      "epoch": 2.191011235955056,
      "grad_norm": 0.622888445854187,
      "learning_rate": 3.116963488958125e-05,
      "loss": 0.0626,
      "step": 390
    },
    {
      "epoch": 2.196629213483146,
      "grad_norm": 0.25128936767578125,
      "learning_rate": 3.1081401166338405e-05,
      "loss": 0.0175,
      "step": 391
    },
    {
      "epoch": 2.202247191011236,
      "grad_norm": 0.2709979712963104,
      "learning_rate": 3.09930868530637e-05,
      "loss": 0.0196,
      "step": 392
    },
    {
      "epoch": 2.207865168539326,
      "grad_norm": 0.3886699974536896,
      "learning_rate": 3.090469312008831e-05,
      "loss": 0.0213,
      "step": 393
    },
    {
      "epoch": 2.2134831460674156,
      "grad_norm": 0.7035768032073975,
      "learning_rate": 3.0816221138795865e-05,
      "loss": 0.0319,
      "step": 394
    },
    {
      "epoch": 2.2191011235955056,
      "grad_norm": 0.22654655575752258,
      "learning_rate": 3.072767208160692e-05,
      "loss": 0.0168,
      "step": 395
    },
    {
      "epoch": 2.2247191011235956,
      "grad_norm": 0.23597337305545807,
      "learning_rate": 3.0639047121963445e-05,
      "loss": 0.0109,
      "step": 396
    },
    {
      "epoch": 2.2303370786516856,
      "grad_norm": 0.35943692922592163,
      "learning_rate": 3.055034743431326e-05,
      "loss": 0.0311,
      "step": 397
    },
    {
      "epoch": 2.235955056179775,
      "grad_norm": 0.16619955003261566,
      "learning_rate": 3.046157419409446e-05,
      "loss": 0.0114,
      "step": 398
    },
    {
      "epoch": 2.241573033707865,
      "grad_norm": 0.4937904477119446,
      "learning_rate": 3.0372728577719873e-05,
      "loss": 0.0564,
      "step": 399
    },
    {
      "epoch": 2.247191011235955,
      "grad_norm": 0.3284696638584137,
      "learning_rate": 3.0283811762561414e-05,
      "loss": 0.0284,
      "step": 400
    },
    {
      "epoch": 2.252808988764045,
      "grad_norm": 0.5976734161376953,
      "learning_rate": 3.0194824926934558e-05,
      "loss": 0.0585,
      "step": 401
    },
    {
      "epoch": 2.258426966292135,
      "grad_norm": 0.21039855480194092,
      "learning_rate": 3.010576925008264e-05,
      "loss": 0.0235,
      "step": 402
    },
    {
      "epoch": 2.264044943820225,
      "grad_norm": 0.5457379817962646,
      "learning_rate": 3.00166459121613e-05,
      "loss": 0.0454,
      "step": 403
    },
    {
      "epoch": 2.2696629213483144,
      "grad_norm": 0.4029808044433594,
      "learning_rate": 2.9927456094222804e-05,
      "loss": 0.0543,
      "step": 404
    },
    {
      "epoch": 2.2752808988764044,
      "grad_norm": 0.37788012623786926,
      "learning_rate": 2.9838200978200416e-05,
      "loss": 0.0245,
      "step": 405
    },
    {
      "epoch": 2.2808988764044944,
      "grad_norm": 0.30368319153785706,
      "learning_rate": 2.9748881746892694e-05,
      "loss": 0.0182,
      "step": 406
    },
    {
      "epoch": 2.2865168539325844,
      "grad_norm": 0.43456441164016724,
      "learning_rate": 2.9659499583947882e-05,
      "loss": 0.0173,
      "step": 407
    },
    {
      "epoch": 2.292134831460674,
      "grad_norm": 0.20616677403450012,
      "learning_rate": 2.957005567384815e-05,
      "loss": 0.0094,
      "step": 408
    },
    {
      "epoch": 2.297752808988764,
      "grad_norm": 0.1742309182882309,
      "learning_rate": 2.948055120189397e-05,
      "loss": 0.0137,
      "step": 409
    },
    {
      "epoch": 2.303370786516854,
      "grad_norm": 0.2785206437110901,
      "learning_rate": 2.939098735418835e-05,
      "loss": 0.0144,
      "step": 410
    },
    {
      "epoch": 2.308988764044944,
      "grad_norm": 0.6163432002067566,
      "learning_rate": 2.930136531762113e-05,
      "loss": 0.0768,
      "step": 411
    },
    {
      "epoch": 2.3146067415730336,
      "grad_norm": 0.3134572505950928,
      "learning_rate": 2.921168627985329e-05,
      "loss": 0.0261,
      "step": 412
    },
    {
      "epoch": 2.3202247191011236,
      "grad_norm": 0.5359384417533875,
      "learning_rate": 2.9121951429301162e-05,
      "loss": 0.0404,
      "step": 413
    },
    {
      "epoch": 2.3258426966292136,
      "grad_norm": 0.46650931239128113,
      "learning_rate": 2.9032161955120717e-05,
      "loss": 0.0505,
      "step": 414
    },
    {
      "epoch": 2.331460674157303,
      "grad_norm": 0.4540022611618042,
      "learning_rate": 2.894231904719178e-05,
      "loss": 0.0437,
      "step": 415
    },
    {
      "epoch": 2.337078651685393,
      "grad_norm": 0.2775721251964569,
      "learning_rate": 2.8852423896102275e-05,
      "loss": 0.017,
      "step": 416
    },
    {
      "epoch": 2.342696629213483,
      "grad_norm": 0.28857848048210144,
      "learning_rate": 2.876247769313246e-05,
      "loss": 0.019,
      "step": 417
    },
    {
      "epoch": 2.348314606741573,
      "grad_norm": 1.0475298166275024,
      "learning_rate": 2.8672481630239108e-05,
      "loss": 0.0668,
      "step": 418
    },
    {
      "epoch": 2.353932584269663,
      "grad_norm": 0.6004732847213745,
      "learning_rate": 2.8582436900039738e-05,
      "loss": 0.0374,
      "step": 419
    },
    {
      "epoch": 2.359550561797753,
      "grad_norm": 0.3326408863067627,
      "learning_rate": 2.849234469579681e-05,
      "loss": 0.0258,
      "step": 420
    },
    {
      "epoch": 2.365168539325843,
      "grad_norm": 0.3259666860103607,
      "learning_rate": 2.8402206211401887e-05,
      "loss": 0.0189,
      "step": 421
    },
    {
      "epoch": 2.370786516853933,
      "grad_norm": 0.5652219653129578,
      "learning_rate": 2.8312022641359852e-05,
      "loss": 0.0408,
      "step": 422
    },
    {
      "epoch": 2.3764044943820224,
      "grad_norm": 0.3287147283554077,
      "learning_rate": 2.8221795180773048e-05,
      "loss": 0.029,
      "step": 423
    },
    {
      "epoch": 2.3820224719101124,
      "grad_norm": 0.3799620270729065,
      "learning_rate": 2.813152502532545e-05,
      "loss": 0.0256,
      "step": 424
    },
    {
      "epoch": 2.3876404494382024,
      "grad_norm": 0.5896545052528381,
      "learning_rate": 2.804121337126681e-05,
      "loss": 0.036,
      "step": 425
    },
    {
      "epoch": 2.393258426966292,
      "grad_norm": 0.42926251888275146,
      "learning_rate": 2.7950861415396847e-05,
      "loss": 0.0144,
      "step": 426
    },
    {
      "epoch": 2.398876404494382,
      "grad_norm": 0.3144187927246094,
      "learning_rate": 2.78604703550493e-05,
      "loss": 0.0227,
      "step": 427
    },
    {
      "epoch": 2.404494382022472,
      "grad_norm": 0.6931906938552856,
      "learning_rate": 2.777004138807619e-05,
      "loss": 0.0753,
      "step": 428
    },
    {
      "epoch": 2.4101123595505616,
      "grad_norm": 0.5297741889953613,
      "learning_rate": 2.76795757128318e-05,
      "loss": 0.0386,
      "step": 429
    },
    {
      "epoch": 2.4157303370786516,
      "grad_norm": 0.36238086223602295,
      "learning_rate": 2.7589074528156927e-05,
      "loss": 0.0171,
      "step": 430
    },
    {
      "epoch": 2.4213483146067416,
      "grad_norm": 0.8145194053649902,
      "learning_rate": 2.7498539033362874e-05,
      "loss": 0.0628,
      "step": 431
    },
    {
      "epoch": 2.4269662921348316,
      "grad_norm": 0.1813487857580185,
      "learning_rate": 2.7407970428215684e-05,
      "loss": 0.0098,
      "step": 432
    },
    {
      "epoch": 2.432584269662921,
      "grad_norm": 0.22360023856163025,
      "learning_rate": 2.731736991292011e-05,
      "loss": 0.0084,
      "step": 433
    },
    {
      "epoch": 2.438202247191011,
      "grad_norm": 0.517037570476532,
      "learning_rate": 2.7226738688103832e-05,
      "loss": 0.0208,
      "step": 434
    },
    {
      "epoch": 2.443820224719101,
      "grad_norm": 0.7346238493919373,
      "learning_rate": 2.7136077954801437e-05,
      "loss": 0.0689,
      "step": 435
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 0.6103556752204895,
      "learning_rate": 2.7045388914438607e-05,
      "loss": 0.0223,
      "step": 436
    },
    {
      "epoch": 2.455056179775281,
      "grad_norm": 0.22554466128349304,
      "learning_rate": 2.6954672768816103e-05,
      "loss": 0.0132,
      "step": 437
    },
    {
      "epoch": 2.460674157303371,
      "grad_norm": 1.2257877588272095,
      "learning_rate": 2.68639307200939e-05,
      "loss": 0.1185,
      "step": 438
    },
    {
      "epoch": 2.466292134831461,
      "grad_norm": 0.1549096405506134,
      "learning_rate": 2.6773163970775244e-05,
      "loss": 0.018,
      "step": 439
    },
    {
      "epoch": 2.4719101123595504,
      "grad_norm": 1.0748399496078491,
      "learning_rate": 2.66823737236907e-05,
      "loss": 0.0925,
      "step": 440
    },
    {
      "epoch": 2.4775280898876404,
      "grad_norm": 0.7623831629753113,
      "learning_rate": 2.659156118198222e-05,
      "loss": 0.0338,
      "step": 441
    },
    {
      "epoch": 2.4831460674157304,
      "grad_norm": 0.801949679851532,
      "learning_rate": 2.6500727549087217e-05,
      "loss": 0.0328,
      "step": 442
    },
    {
      "epoch": 2.48876404494382,
      "grad_norm": 1.2928742170333862,
      "learning_rate": 2.640987402872258e-05,
      "loss": 0.1227,
      "step": 443
    },
    {
      "epoch": 2.49438202247191,
      "grad_norm": 0.5797972083091736,
      "learning_rate": 2.631900182486877e-05,
      "loss": 0.0479,
      "step": 444
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.47150206565856934,
      "learning_rate": 2.622811214175382e-05,
      "loss": 0.0225,
      "step": 445
    },
    {
      "epoch": 2.50561797752809,
      "grad_norm": 0.40045157074928284,
      "learning_rate": 2.6137206183837403e-05,
      "loss": 0.029,
      "step": 446
    },
    {
      "epoch": 2.51123595505618,
      "grad_norm": 0.5264219045639038,
      "learning_rate": 2.604628515579486e-05,
      "loss": 0.051,
      "step": 447
    },
    {
      "epoch": 2.5168539325842696,
      "grad_norm": 0.39181241393089294,
      "learning_rate": 2.595535026250125e-05,
      "loss": 0.0295,
      "step": 448
    },
    {
      "epoch": 2.5224719101123596,
      "grad_norm": 0.25217872858047485,
      "learning_rate": 2.586440270901536e-05,
      "loss": 0.0157,
      "step": 449
    },
    {
      "epoch": 2.5280898876404496,
      "grad_norm": 0.7917795777320862,
      "learning_rate": 2.577344370056375e-05,
      "loss": 0.0899,
      "step": 450
    },
    {
      "epoch": 2.533707865168539,
      "grad_norm": 0.4114810824394226,
      "learning_rate": 2.56824744425248e-05,
      "loss": 0.0499,
      "step": 451
    },
    {
      "epoch": 2.539325842696629,
      "grad_norm": 0.29708805680274963,
      "learning_rate": 2.5591496140412685e-05,
      "loss": 0.0242,
      "step": 452
    },
    {
      "epoch": 2.544943820224719,
      "grad_norm": 0.2634304165840149,
      "learning_rate": 2.5500509999861454e-05,
      "loss": 0.0202,
      "step": 453
    },
    {
      "epoch": 2.550561797752809,
      "grad_norm": 0.17841798067092896,
      "learning_rate": 2.5409517226609016e-05,
      "loss": 0.0105,
      "step": 454
    },
    {
      "epoch": 2.556179775280899,
      "grad_norm": 0.41888076066970825,
      "learning_rate": 2.5318519026481198e-05,
      "loss": 0.0513,
      "step": 455
    },
    {
      "epoch": 2.561797752808989,
      "grad_norm": 0.7305861711502075,
      "learning_rate": 2.52275166053757e-05,
      "loss": 0.0493,
      "step": 456
    },
    {
      "epoch": 2.567415730337079,
      "grad_norm": 0.24928870797157288,
      "learning_rate": 2.513651116924621e-05,
      "loss": 0.0197,
      "step": 457
    },
    {
      "epoch": 2.5730337078651684,
      "grad_norm": 0.6005033254623413,
      "learning_rate": 2.5045503924086333e-05,
      "loss": 0.0524,
      "step": 458
    },
    {
      "epoch": 2.5786516853932584,
      "grad_norm": 0.7274844646453857,
      "learning_rate": 2.4954496075913673e-05,
      "loss": 0.0524,
      "step": 459
    },
    {
      "epoch": 2.5842696629213484,
      "grad_norm": 0.27275940775871277,
      "learning_rate": 2.4863488830753798e-05,
      "loss": 0.0409,
      "step": 460
    },
    {
      "epoch": 2.5898876404494384,
      "grad_norm": 0.3832864463329315,
      "learning_rate": 2.4772483394624302e-05,
      "loss": 0.0358,
      "step": 461
    },
    {
      "epoch": 2.595505617977528,
      "grad_norm": 0.33115604519844055,
      "learning_rate": 2.468148097351881e-05,
      "loss": 0.0209,
      "step": 462
    },
    {
      "epoch": 2.601123595505618,
      "grad_norm": 0.17589695751667023,
      "learning_rate": 2.459048277339099e-05,
      "loss": 0.0124,
      "step": 463
    },
    {
      "epoch": 2.606741573033708,
      "grad_norm": 0.18808305263519287,
      "learning_rate": 2.4499490000138555e-05,
      "loss": 0.013,
      "step": 464
    },
    {
      "epoch": 2.6123595505617976,
      "grad_norm": 0.8324049115180969,
      "learning_rate": 2.4408503859587317e-05,
      "loss": 0.0751,
      "step": 465
    },
    {
      "epoch": 2.6179775280898876,
      "grad_norm": 0.48261308670043945,
      "learning_rate": 2.4317525557475206e-05,
      "loss": 0.0341,
      "step": 466
    },
    {
      "epoch": 2.6235955056179776,
      "grad_norm": 0.2649715840816498,
      "learning_rate": 2.4226556299436256e-05,
      "loss": 0.02,
      "step": 467
    },
    {
      "epoch": 2.629213483146067,
      "grad_norm": 0.5935967564582825,
      "learning_rate": 2.4135597290984652e-05,
      "loss": 0.0341,
      "step": 468
    },
    {
      "epoch": 2.634831460674157,
      "grad_norm": 0.6265687346458435,
      "learning_rate": 2.4044649737498757e-05,
      "loss": 0.074,
      "step": 469
    },
    {
      "epoch": 2.640449438202247,
      "grad_norm": 0.37573564052581787,
      "learning_rate": 2.3953714844205144e-05,
      "loss": 0.0219,
      "step": 470
    },
    {
      "epoch": 2.646067415730337,
      "grad_norm": 0.48095813393592834,
      "learning_rate": 2.3862793816162603e-05,
      "loss": 0.0415,
      "step": 471
    },
    {
      "epoch": 2.6516853932584272,
      "grad_norm": 0.22356294095516205,
      "learning_rate": 2.377188785824618e-05,
      "loss": 0.0204,
      "step": 472
    },
    {
      "epoch": 2.657303370786517,
      "grad_norm": 0.4313144087791443,
      "learning_rate": 2.3680998175131235e-05,
      "loss": 0.0369,
      "step": 473
    },
    {
      "epoch": 2.662921348314607,
      "grad_norm": 0.7182667255401611,
      "learning_rate": 2.3590125971277422e-05,
      "loss": 0.0547,
      "step": 474
    },
    {
      "epoch": 2.668539325842697,
      "grad_norm": 0.22594816982746124,
      "learning_rate": 2.349927245091279e-05,
      "loss": 0.0201,
      "step": 475
    },
    {
      "epoch": 2.6741573033707864,
      "grad_norm": 0.5268104076385498,
      "learning_rate": 2.3408438818017778e-05,
      "loss": 0.0393,
      "step": 476
    },
    {
      "epoch": 2.6797752808988764,
      "grad_norm": 0.3660241961479187,
      "learning_rate": 2.3317626276309306e-05,
      "loss": 0.0207,
      "step": 477
    },
    {
      "epoch": 2.6853932584269664,
      "grad_norm": 0.3511592745780945,
      "learning_rate": 2.3226836029224762e-05,
      "loss": 0.0338,
      "step": 478
    },
    {
      "epoch": 2.691011235955056,
      "grad_norm": 0.5912359952926636,
      "learning_rate": 2.3136069279906102e-05,
      "loss": 0.0443,
      "step": 479
    },
    {
      "epoch": 2.696629213483146,
      "grad_norm": 0.542970597743988,
      "learning_rate": 2.30453272311839e-05,
      "loss": 0.0357,
      "step": 480
    },
    {
      "epoch": 2.702247191011236,
      "grad_norm": 0.22134356200695038,
      "learning_rate": 2.2954611085561396e-05,
      "loss": 0.0152,
      "step": 481
    },
    {
      "epoch": 2.7078651685393256,
      "grad_norm": 0.5963819622993469,
      "learning_rate": 2.286392204519857e-05,
      "loss": 0.0391,
      "step": 482
    },
    {
      "epoch": 2.7134831460674156,
      "grad_norm": 0.4541224539279938,
      "learning_rate": 2.277326131189618e-05,
      "loss": 0.0328,
      "step": 483
    },
    {
      "epoch": 2.7191011235955056,
      "grad_norm": 0.3524722158908844,
      "learning_rate": 2.2682630087079894e-05,
      "loss": 0.0236,
      "step": 484
    },
    {
      "epoch": 2.7247191011235956,
      "grad_norm": 0.4940423369407654,
      "learning_rate": 2.259202957178432e-05,
      "loss": 0.0244,
      "step": 485
    },
    {
      "epoch": 2.7303370786516856,
      "grad_norm": 0.12685894966125488,
      "learning_rate": 2.2501460966637132e-05,
      "loss": 0.0071,
      "step": 486
    },
    {
      "epoch": 2.735955056179775,
      "grad_norm": 0.33782774209976196,
      "learning_rate": 2.2410925471843082e-05,
      "loss": 0.0148,
      "step": 487
    },
    {
      "epoch": 2.741573033707865,
      "grad_norm": 0.5189773440361023,
      "learning_rate": 2.2320424287168197e-05,
      "loss": 0.0258,
      "step": 488
    },
    {
      "epoch": 2.747191011235955,
      "grad_norm": 0.2190711498260498,
      "learning_rate": 2.2229958611923812e-05,
      "loss": 0.0084,
      "step": 489
    },
    {
      "epoch": 2.752808988764045,
      "grad_norm": 0.3156758248806,
      "learning_rate": 2.2139529644950703e-05,
      "loss": 0.0144,
      "step": 490
    },
    {
      "epoch": 2.758426966292135,
      "grad_norm": 0.7622292637825012,
      "learning_rate": 2.2049138584603156e-05,
      "loss": 0.0577,
      "step": 491
    },
    {
      "epoch": 2.764044943820225,
      "grad_norm": 0.5604062676429749,
      "learning_rate": 2.1958786628733192e-05,
      "loss": 0.0358,
      "step": 492
    },
    {
      "epoch": 2.7696629213483144,
      "grad_norm": 0.449767142534256,
      "learning_rate": 2.186847497467456e-05,
      "loss": 0.0199,
      "step": 493
    },
    {
      "epoch": 2.7752808988764044,
      "grad_norm": 0.23587378859519958,
      "learning_rate": 2.177820481922696e-05,
      "loss": 0.0107,
      "step": 494
    },
    {
      "epoch": 2.7808988764044944,
      "grad_norm": 0.2752093970775604,
      "learning_rate": 2.1687977358640147e-05,
      "loss": 0.0084,
      "step": 495
    },
    {
      "epoch": 2.7865168539325844,
      "grad_norm": 0.11343023180961609,
      "learning_rate": 2.159779378859812e-05,
      "loss": 0.0039,
      "step": 496
    },
    {
      "epoch": 2.7921348314606744,
      "grad_norm": 0.11694802343845367,
      "learning_rate": 2.15076553042032e-05,
      "loss": 0.0039,
      "step": 497
    },
    {
      "epoch": 2.797752808988764,
      "grad_norm": 0.45200011134147644,
      "learning_rate": 2.1417563099960274e-05,
      "loss": 0.0368,
      "step": 498
    },
    {
      "epoch": 2.803370786516854,
      "grad_norm": 0.16599901020526886,
      "learning_rate": 2.1327518369760894e-05,
      "loss": 0.006,
      "step": 499
    },
    {
      "epoch": 2.808988764044944,
      "grad_norm": 0.6932640075683594,
      "learning_rate": 2.1237522306867546e-05,
      "loss": 0.058,
      "step": 500
    },
    {
      "epoch": 2.8146067415730336,
      "grad_norm": 0.9597342014312744,
      "learning_rate": 2.1147576103897727e-05,
      "loss": 0.0392,
      "step": 501
    },
    {
      "epoch": 2.8202247191011236,
      "grad_norm": 0.873919665813446,
      "learning_rate": 2.105768095280822e-05,
      "loss": 0.0868,
      "step": 502
    },
    {
      "epoch": 2.8258426966292136,
      "grad_norm": 0.4497927725315094,
      "learning_rate": 2.096783804487928e-05,
      "loss": 0.0143,
      "step": 503
    },
    {
      "epoch": 2.831460674157303,
      "grad_norm": 0.5585697293281555,
      "learning_rate": 2.087804857069884e-05,
      "loss": 0.0202,
      "step": 504
    },
    {
      "epoch": 2.837078651685393,
      "grad_norm": 0.2345060110092163,
      "learning_rate": 2.078831372014671e-05,
      "loss": 0.0086,
      "step": 505
    },
    {
      "epoch": 2.842696629213483,
      "grad_norm": 0.8676149249076843,
      "learning_rate": 2.0698634682378875e-05,
      "loss": 0.0447,
      "step": 506
    },
    {
      "epoch": 2.8483146067415728,
      "grad_norm": 0.10784171521663666,
      "learning_rate": 2.0609012645811653e-05,
      "loss": 0.0029,
      "step": 507
    },
    {
      "epoch": 2.853932584269663,
      "grad_norm": 0.3755403161048889,
      "learning_rate": 2.0519448798106034e-05,
      "loss": 0.0169,
      "step": 508
    },
    {
      "epoch": 2.859550561797753,
      "grad_norm": 0.6184055209159851,
      "learning_rate": 2.0429944326151852e-05,
      "loss": 0.0416,
      "step": 509
    },
    {
      "epoch": 2.865168539325843,
      "grad_norm": 0.2468174397945404,
      "learning_rate": 2.0340500416052127e-05,
      "loss": 0.0116,
      "step": 510
    },
    {
      "epoch": 2.870786516853933,
      "grad_norm": 0.5160658359527588,
      "learning_rate": 2.0251118253107308e-05,
      "loss": 0.0508,
      "step": 511
    },
    {
      "epoch": 2.8764044943820224,
      "grad_norm": 0.8829122185707092,
      "learning_rate": 2.016179902179959e-05,
      "loss": 0.0615,
      "step": 512
    },
    {
      "epoch": 2.8820224719101124,
      "grad_norm": 0.5750426054000854,
      "learning_rate": 2.0072543905777205e-05,
      "loss": 0.0428,
      "step": 513
    },
    {
      "epoch": 2.8876404494382024,
      "grad_norm": 0.13187269866466522,
      "learning_rate": 1.9983354087838706e-05,
      "loss": 0.004,
      "step": 514
    },
    {
      "epoch": 2.893258426966292,
      "grad_norm": 0.48462826013565063,
      "learning_rate": 1.9894230749917366e-05,
      "loss": 0.0302,
      "step": 515
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 0.4523911476135254,
      "learning_rate": 1.980517507306545e-05,
      "loss": 0.0204,
      "step": 516
    },
    {
      "epoch": 2.904494382022472,
      "grad_norm": 0.2993704080581665,
      "learning_rate": 1.9716188237438592e-05,
      "loss": 0.018,
      "step": 517
    },
    {
      "epoch": 2.9101123595505616,
      "grad_norm": 0.25127974152565,
      "learning_rate": 1.9627271422280136e-05,
      "loss": 0.0182,
      "step": 518
    },
    {
      "epoch": 2.9157303370786516,
      "grad_norm": 1.011850118637085,
      "learning_rate": 1.9538425805905545e-05,
      "loss": 0.0759,
      "step": 519
    },
    {
      "epoch": 2.9213483146067416,
      "grad_norm": 0.2463807910680771,
      "learning_rate": 1.944965256568675e-05,
      "loss": 0.011,
      "step": 520
    },
    {
      "epoch": 2.9269662921348316,
      "grad_norm": 0.20098300278186798,
      "learning_rate": 1.9360952878036567e-05,
      "loss": 0.0077,
      "step": 521
    },
    {
      "epoch": 2.932584269662921,
      "grad_norm": 0.30252471566200256,
      "learning_rate": 1.9272327918393083e-05,
      "loss": 0.0123,
      "step": 522
    },
    {
      "epoch": 2.938202247191011,
      "grad_norm": 0.8741325736045837,
      "learning_rate": 1.9183778861204145e-05,
      "loss": 0.0549,
      "step": 523
    },
    {
      "epoch": 2.943820224719101,
      "grad_norm": 1.175453543663025,
      "learning_rate": 1.9095306879911696e-05,
      "loss": 0.0539,
      "step": 524
    },
    {
      "epoch": 2.949438202247191,
      "grad_norm": 0.7179183959960938,
      "learning_rate": 1.900691314693631e-05,
      "loss": 0.0516,
      "step": 525
    },
    {
      "epoch": 2.955056179775281,
      "grad_norm": 2.5897226333618164,
      "learning_rate": 1.8918598833661597e-05,
      "loss": 0.1772,
      "step": 526
    },
    {
      "epoch": 2.960674157303371,
      "grad_norm": 1.011610507965088,
      "learning_rate": 1.8830365110418754e-05,
      "loss": 0.0644,
      "step": 527
    },
    {
      "epoch": 2.966292134831461,
      "grad_norm": 0.8009464144706726,
      "learning_rate": 1.8742213146470954e-05,
      "loss": 0.0309,
      "step": 528
    },
    {
      "epoch": 2.9719101123595504,
      "grad_norm": 0.4852517247200012,
      "learning_rate": 1.8654144109997943e-05,
      "loss": 0.0306,
      "step": 529
    },
    {
      "epoch": 2.9775280898876404,
      "grad_norm": 0.5038325786590576,
      "learning_rate": 1.856615916808048e-05,
      "loss": 0.0372,
      "step": 530
    },
    {
      "epoch": 2.9831460674157304,
      "grad_norm": 0.33018678426742554,
      "learning_rate": 1.847825948668498e-05,
      "loss": 0.0272,
      "step": 531
    },
    {
      "epoch": 2.98876404494382,
      "grad_norm": 0.4440076947212219,
      "learning_rate": 1.8390446230647927e-05,
      "loss": 0.0334,
      "step": 532
    },
    {
      "epoch": 2.99438202247191,
      "grad_norm": 0.5508174896240234,
      "learning_rate": 1.8302720563660536e-05,
      "loss": 0.0323,
      "step": 533
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.7452078461647034,
      "learning_rate": 1.8215083648253313e-05,
      "loss": 0.0413,
      "step": 534
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.03986029326915741,
      "eval_runtime": 103.7714,
      "eval_samples_per_second": 0.896,
      "eval_steps_per_second": 0.453,
      "step": 534
    },
    {
      "epoch": 3.00561797752809,
      "grad_norm": 0.26543158292770386,
      "learning_rate": 1.8127536645780623e-05,
      "loss": 0.0125,
      "step": 535
    },
    {
      "epoch": 3.0112359550561796,
      "grad_norm": 0.31744107604026794,
      "learning_rate": 1.8040080716405304e-05,
      "loss": 0.0204,
      "step": 536
    },
    {
      "epoch": 3.0168539325842696,
      "grad_norm": 0.5241402387619019,
      "learning_rate": 1.7952717019083325e-05,
      "loss": 0.045,
      "step": 537
    },
    {
      "epoch": 3.0224719101123596,
      "grad_norm": 0.17052997648715973,
      "learning_rate": 1.7865446711548407e-05,
      "loss": 0.0119,
      "step": 538
    },
    {
      "epoch": 3.0280898876404496,
      "grad_norm": 0.4424309730529785,
      "learning_rate": 1.7778270950296665e-05,
      "loss": 0.0382,
      "step": 539
    },
    {
      "epoch": 3.033707865168539,
      "grad_norm": 0.2502543032169342,
      "learning_rate": 1.7691190890571298e-05,
      "loss": 0.0139,
      "step": 540
    },
    {
      "epoch": 3.039325842696629,
      "grad_norm": 0.15381747484207153,
      "learning_rate": 1.760420768634729e-05,
      "loss": 0.0108,
      "step": 541
    },
    {
      "epoch": 3.044943820224719,
      "grad_norm": 0.37762072682380676,
      "learning_rate": 1.7517322490316107e-05,
      "loss": 0.0287,
      "step": 542
    },
    {
      "epoch": 3.050561797752809,
      "grad_norm": 0.1941385418176651,
      "learning_rate": 1.7430536453870415e-05,
      "loss": 0.0131,
      "step": 543
    },
    {
      "epoch": 3.056179775280899,
      "grad_norm": 0.9759604930877686,
      "learning_rate": 1.7343850727088835e-05,
      "loss": 0.1008,
      "step": 544
    },
    {
      "epoch": 3.061797752808989,
      "grad_norm": 0.5458764433860779,
      "learning_rate": 1.7257266458720673e-05,
      "loss": 0.042,
      "step": 545
    },
    {
      "epoch": 3.067415730337079,
      "grad_norm": 0.2656683325767517,
      "learning_rate": 1.717078479617076e-05,
      "loss": 0.0249,
      "step": 546
    },
    {
      "epoch": 3.0730337078651684,
      "grad_norm": 0.5697004199028015,
      "learning_rate": 1.708440688548418e-05,
      "loss": 0.0521,
      "step": 547
    },
    {
      "epoch": 3.0786516853932584,
      "grad_norm": 0.2980285882949829,
      "learning_rate": 1.6998133871331113e-05,
      "loss": 0.0257,
      "step": 548
    },
    {
      "epoch": 3.0842696629213484,
      "grad_norm": 0.43190404772758484,
      "learning_rate": 1.691196689699165e-05,
      "loss": 0.0366,
      "step": 549
    },
    {
      "epoch": 3.0898876404494384,
      "grad_norm": 0.522339940071106,
      "learning_rate": 1.6825907104340683e-05,
      "loss": 0.0403,
      "step": 550
    },
    {
      "epoch": 3.095505617977528,
      "grad_norm": 0.20700271427631378,
      "learning_rate": 1.6739955633832724e-05,
      "loss": 0.0148,
      "step": 551
    },
    {
      "epoch": 3.101123595505618,
      "grad_norm": 0.4147394001483917,
      "learning_rate": 1.6654113624486824e-05,
      "loss": 0.0462,
      "step": 552
    },
    {
      "epoch": 3.106741573033708,
      "grad_norm": 0.2930881083011627,
      "learning_rate": 1.656838221387144e-05,
      "loss": 0.0171,
      "step": 553
    },
    {
      "epoch": 3.1123595505617976,
      "grad_norm": 0.45358917117118835,
      "learning_rate": 1.648276253808944e-05,
      "loss": 0.0399,
      "step": 554
    },
    {
      "epoch": 3.1179775280898876,
      "grad_norm": 0.4869868755340576,
      "learning_rate": 1.639725573176294e-05,
      "loss": 0.0357,
      "step": 555
    },
    {
      "epoch": 3.1235955056179776,
      "grad_norm": 0.882999062538147,
      "learning_rate": 1.6311862928018348e-05,
      "loss": 0.0672,
      "step": 556
    },
    {
      "epoch": 3.1292134831460676,
      "grad_norm": 0.24678051471710205,
      "learning_rate": 1.622658525847133e-05,
      "loss": 0.0178,
      "step": 557
    },
    {
      "epoch": 3.134831460674157,
      "grad_norm": 0.15869978070259094,
      "learning_rate": 1.6141423853211792e-05,
      "loss": 0.0131,
      "step": 558
    },
    {
      "epoch": 3.140449438202247,
      "grad_norm": 0.23944701254367828,
      "learning_rate": 1.6056379840788922e-05,
      "loss": 0.0245,
      "step": 559
    },
    {
      "epoch": 3.146067415730337,
      "grad_norm": 0.3650897443294525,
      "learning_rate": 1.597145434819623e-05,
      "loss": 0.023,
      "step": 560
    },
    {
      "epoch": 3.151685393258427,
      "grad_norm": 0.43158525228500366,
      "learning_rate": 1.5886648500856632e-05,
      "loss": 0.0321,
      "step": 561
    },
    {
      "epoch": 3.157303370786517,
      "grad_norm": 0.23515547811985016,
      "learning_rate": 1.5801963422607503e-05,
      "loss": 0.0147,
      "step": 562
    },
    {
      "epoch": 3.162921348314607,
      "grad_norm": 0.3177356719970703,
      "learning_rate": 1.5717400235685792e-05,
      "loss": 0.0194,
      "step": 563
    },
    {
      "epoch": 3.168539325842697,
      "grad_norm": 0.2217148095369339,
      "learning_rate": 1.563296006071317e-05,
      "loss": 0.01,
      "step": 564
    },
    {
      "epoch": 3.1741573033707864,
      "grad_norm": 0.38764169812202454,
      "learning_rate": 1.5548644016681168e-05,
      "loss": 0.024,
      "step": 565
    },
    {
      "epoch": 3.1797752808988764,
      "grad_norm": 0.5004209280014038,
      "learning_rate": 1.5464453220936363e-05,
      "loss": 0.0482,
      "step": 566
    },
    {
      "epoch": 3.1853932584269664,
      "grad_norm": 0.8537753224372864,
      "learning_rate": 1.5380388789165516e-05,
      "loss": 0.07,
      "step": 567
    },
    {
      "epoch": 3.191011235955056,
      "grad_norm": 0.6337342858314514,
      "learning_rate": 1.529645183538085e-05,
      "loss": 0.0433,
      "step": 568
    },
    {
      "epoch": 3.196629213483146,
      "grad_norm": 0.20846207439899445,
      "learning_rate": 1.5212643471905286e-05,
      "loss": 0.0127,
      "step": 569
    },
    {
      "epoch": 3.202247191011236,
      "grad_norm": 0.2741338312625885,
      "learning_rate": 1.5128964809357649e-05,
      "loss": 0.0165,
      "step": 570
    },
    {
      "epoch": 3.207865168539326,
      "grad_norm": 0.3528291881084442,
      "learning_rate": 1.5045416956637981e-05,
      "loss": 0.0181,
      "step": 571
    },
    {
      "epoch": 3.2134831460674156,
      "grad_norm": 0.3666711747646332,
      "learning_rate": 1.4962001020912869e-05,
      "loss": 0.0247,
      "step": 572
    },
    {
      "epoch": 3.2191011235955056,
      "grad_norm": 0.2700091004371643,
      "learning_rate": 1.4878718107600748e-05,
      "loss": 0.0113,
      "step": 573
    },
    {
      "epoch": 3.2247191011235956,
      "grad_norm": 0.43043816089630127,
      "learning_rate": 1.4795569320357235e-05,
      "loss": 0.0218,
      "step": 574
    },
    {
      "epoch": 3.2303370786516856,
      "grad_norm": 0.22852742671966553,
      "learning_rate": 1.4712555761060542e-05,
      "loss": 0.0184,
      "step": 575
    },
    {
      "epoch": 3.235955056179775,
      "grad_norm": 0.5052158236503601,
      "learning_rate": 1.4629678529796858e-05,
      "loss": 0.0323,
      "step": 576
    },
    {
      "epoch": 3.241573033707865,
      "grad_norm": 0.19395947456359863,
      "learning_rate": 1.454693872484576e-05,
      "loss": 0.0067,
      "step": 577
    },
    {
      "epoch": 3.247191011235955,
      "grad_norm": 0.7639420032501221,
      "learning_rate": 1.4464337442665648e-05,
      "loss": 0.0489,
      "step": 578
    },
    {
      "epoch": 3.252808988764045,
      "grad_norm": 0.48518991470336914,
      "learning_rate": 1.4381875777879272e-05,
      "loss": 0.0184,
      "step": 579
    },
    {
      "epoch": 3.258426966292135,
      "grad_norm": 0.6347991228103638,
      "learning_rate": 1.4299554823259176e-05,
      "loss": 0.0371,
      "step": 580
    },
    {
      "epoch": 3.264044943820225,
      "grad_norm": 0.1332576721906662,
      "learning_rate": 1.4217375669713209e-05,
      "loss": 0.0121,
      "step": 581
    },
    {
      "epoch": 3.2696629213483144,
      "grad_norm": 0.3283112347126007,
      "learning_rate": 1.413533940627012e-05,
      "loss": 0.0215,
      "step": 582
    },
    {
      "epoch": 3.2752808988764044,
      "grad_norm": 0.3344483971595764,
      "learning_rate": 1.4053447120065055e-05,
      "loss": 0.0215,
      "step": 583
    },
    {
      "epoch": 3.2808988764044944,
      "grad_norm": 0.40408533811569214,
      "learning_rate": 1.3971699896325238e-05,
      "loss": 0.016,
      "step": 584
    },
    {
      "epoch": 3.2865168539325844,
      "grad_norm": 0.7595210671424866,
      "learning_rate": 1.3890098818355495e-05,
      "loss": 0.0358,
      "step": 585
    },
    {
      "epoch": 3.292134831460674,
      "grad_norm": 0.39442968368530273,
      "learning_rate": 1.3808644967523988e-05,
      "loss": 0.0165,
      "step": 586
    },
    {
      "epoch": 3.297752808988764,
      "grad_norm": 0.6236672401428223,
      "learning_rate": 1.3727339423247795e-05,
      "loss": 0.0322,
      "step": 587
    },
    {
      "epoch": 3.303370786516854,
      "grad_norm": 0.8207044005393982,
      "learning_rate": 1.3646183262978707e-05,
      "loss": 0.0323,
      "step": 588
    },
    {
      "epoch": 3.308988764044944,
      "grad_norm": 0.5095283389091492,
      "learning_rate": 1.356517756218884e-05,
      "loss": 0.0172,
      "step": 589
    },
    {
      "epoch": 3.3146067415730336,
      "grad_norm": 0.17666132748126984,
      "learning_rate": 1.3484323394356485e-05,
      "loss": 0.006,
      "step": 590
    },
    {
      "epoch": 3.3202247191011236,
      "grad_norm": 0.2632010579109192,
      "learning_rate": 1.3403621830951799e-05,
      "loss": 0.0091,
      "step": 591
    },
    {
      "epoch": 3.3258426966292136,
      "grad_norm": 0.35296639800071716,
      "learning_rate": 1.3323073941422676e-05,
      "loss": 0.0095,
      "step": 592
    },
    {
      "epoch": 3.331460674157303,
      "grad_norm": 0.5716587901115417,
      "learning_rate": 1.3242680793180506e-05,
      "loss": 0.0439,
      "step": 593
    },
    {
      "epoch": 3.337078651685393,
      "grad_norm": 0.8416780829429626,
      "learning_rate": 1.3162443451586099e-05,
      "loss": 0.0442,
      "step": 594
    },
    {
      "epoch": 3.342696629213483,
      "grad_norm": 0.573371410369873,
      "learning_rate": 1.3082362979935503e-05,
      "loss": 0.0187,
      "step": 595
    },
    {
      "epoch": 3.348314606741573,
      "grad_norm": 0.5814575552940369,
      "learning_rate": 1.3002440439445976e-05,
      "loss": 0.0204,
      "step": 596
    },
    {
      "epoch": 3.353932584269663,
      "grad_norm": 0.7335706949234009,
      "learning_rate": 1.2922676889241853e-05,
      "loss": 0.0446,
      "step": 597
    },
    {
      "epoch": 3.359550561797753,
      "grad_norm": 0.38267916440963745,
      "learning_rate": 1.2843073386340574e-05,
      "loss": 0.0288,
      "step": 598
    },
    {
      "epoch": 3.365168539325843,
      "grad_norm": 0.42697104811668396,
      "learning_rate": 1.2763630985638659e-05,
      "loss": 0.0108,
      "step": 599
    },
    {
      "epoch": 3.370786516853933,
      "grad_norm": 0.663788378238678,
      "learning_rate": 1.2684350739897687e-05,
      "loss": 0.0668,
      "step": 600
    },
    {
      "epoch": 3.3764044943820224,
      "grad_norm": 0.9892685413360596,
      "learning_rate": 1.2605233699730395e-05,
      "loss": 0.0522,
      "step": 601
    },
    {
      "epoch": 3.3820224719101124,
      "grad_norm": 0.2274734079837799,
      "learning_rate": 1.2526280913586744e-05,
      "loss": 0.0118,
      "step": 602
    },
    {
      "epoch": 3.3876404494382024,
      "grad_norm": 0.25179412961006165,
      "learning_rate": 1.2447493427740025e-05,
      "loss": 0.0118,
      "step": 603
    },
    {
      "epoch": 3.393258426966292,
      "grad_norm": 0.33124208450317383,
      "learning_rate": 1.236887228627297e-05,
      "loss": 0.0092,
      "step": 604
    },
    {
      "epoch": 3.398876404494382,
      "grad_norm": 0.5263144373893738,
      "learning_rate": 1.2290418531063933e-05,
      "loss": 0.0292,
      "step": 605
    },
    {
      "epoch": 3.404494382022472,
      "grad_norm": 0.6951123476028442,
      "learning_rate": 1.2212133201773104e-05,
      "loss": 0.0305,
      "step": 606
    },
    {
      "epoch": 3.4101123595505616,
      "grad_norm": 0.7408294081687927,
      "learning_rate": 1.2134017335828707e-05,
      "loss": 0.0531,
      "step": 607
    },
    {
      "epoch": 3.4157303370786516,
      "grad_norm": 0.16380850970745087,
      "learning_rate": 1.2056071968413248e-05,
      "loss": 0.0055,
      "step": 608
    },
    {
      "epoch": 3.4213483146067416,
      "grad_norm": 0.23817315697669983,
      "learning_rate": 1.19782981324498e-05,
      "loss": 0.012,
      "step": 609
    },
    {
      "epoch": 3.4269662921348316,
      "grad_norm": 0.7689966559410095,
      "learning_rate": 1.1900696858588337e-05,
      "loss": 0.0389,
      "step": 610
    },
    {
      "epoch": 3.432584269662921,
      "grad_norm": 0.3634156882762909,
      "learning_rate": 1.1823269175192064e-05,
      "loss": 0.0374,
      "step": 611
    },
    {
      "epoch": 3.438202247191011,
      "grad_norm": 0.1352304369211197,
      "learning_rate": 1.174601610832376e-05,
      "loss": 0.0052,
      "step": 612
    },
    {
      "epoch": 3.443820224719101,
      "grad_norm": 1.0599766969680786,
      "learning_rate": 1.1668938681732236e-05,
      "loss": 0.0866,
      "step": 613
    },
    {
      "epoch": 3.449438202247191,
      "grad_norm": 0.12189646810293198,
      "learning_rate": 1.1592037916838708e-05,
      "loss": 0.012,
      "step": 614
    },
    {
      "epoch": 3.455056179775281,
      "grad_norm": 0.5154374837875366,
      "learning_rate": 1.1515314832723318e-05,
      "loss": 0.037,
      "step": 615
    },
    {
      "epoch": 3.460674157303371,
      "grad_norm": 0.9575198292732239,
      "learning_rate": 1.1438770446111576e-05,
      "loss": 0.0396,
      "step": 616
    },
    {
      "epoch": 3.466292134831461,
      "grad_norm": 0.9717501401901245,
      "learning_rate": 1.136240577136094e-05,
      "loss": 0.0653,
      "step": 617
    },
    {
      "epoch": 3.4719101123595504,
      "grad_norm": 0.5639912486076355,
      "learning_rate": 1.128622182044734e-05,
      "loss": 0.024,
      "step": 618
    },
    {
      "epoch": 3.4775280898876404,
      "grad_norm": 0.5595723390579224,
      "learning_rate": 1.121021960295176e-05,
      "loss": 0.0492,
      "step": 619
    },
    {
      "epoch": 3.4831460674157304,
      "grad_norm": 0.3951021432876587,
      "learning_rate": 1.113440012604688e-05,
      "loss": 0.0314,
      "step": 620
    },
    {
      "epoch": 3.48876404494382,
      "grad_norm": 0.5793301463127136,
      "learning_rate": 1.1058764394483726e-05,
      "loss": 0.0377,
      "step": 621
    },
    {
      "epoch": 3.49438202247191,
      "grad_norm": 0.5964634418487549,
      "learning_rate": 1.0983313410578366e-05,
      "loss": 0.0445,
      "step": 622
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.25717127323150635,
      "learning_rate": 1.0908048174198588e-05,
      "loss": 0.0096,
      "step": 623
    },
    {
      "epoch": 3.50561797752809,
      "grad_norm": 0.6476510167121887,
      "learning_rate": 1.0832969682750676e-05,
      "loss": 0.038,
      "step": 624
    },
    {
      "epoch": 3.51123595505618,
      "grad_norm": 0.5877466201782227,
      "learning_rate": 1.0758078931166214e-05,
      "loss": 0.0373,
      "step": 625
    },
    {
      "epoch": 3.5168539325842696,
      "grad_norm": 0.5986835360527039,
      "learning_rate": 1.068337691188887e-05,
      "loss": 0.0462,
      "step": 626
    },
    {
      "epoch": 3.5224719101123596,
      "grad_norm": 0.29510265588760376,
      "learning_rate": 1.0608864614861241e-05,
      "loss": 0.0121,
      "step": 627
    },
    {
      "epoch": 3.5280898876404496,
      "grad_norm": 0.21877045929431915,
      "learning_rate": 1.0534543027511749e-05,
      "loss": 0.0092,
      "step": 628
    },
    {
      "epoch": 3.533707865168539,
      "grad_norm": 0.4875641465187073,
      "learning_rate": 1.0460413134741564e-05,
      "loss": 0.0242,
      "step": 629
    },
    {
      "epoch": 3.539325842696629,
      "grad_norm": 0.3078957498073578,
      "learning_rate": 1.0386475918911548e-05,
      "loss": 0.0181,
      "step": 630
    },
    {
      "epoch": 3.544943820224719,
      "grad_norm": 0.6624496579170227,
      "learning_rate": 1.0312732359829206e-05,
      "loss": 0.0532,
      "step": 631
    },
    {
      "epoch": 3.550561797752809,
      "grad_norm": 0.20464101433753967,
      "learning_rate": 1.023918343473573e-05,
      "loss": 0.013,
      "step": 632
    },
    {
      "epoch": 3.556179775280899,
      "grad_norm": 0.2514359951019287,
      "learning_rate": 1.0165830118293066e-05,
      "loss": 0.012,
      "step": 633
    },
    {
      "epoch": 3.561797752808989,
      "grad_norm": 0.3207147717475891,
      "learning_rate": 1.0092673382570966e-05,
      "loss": 0.0134,
      "step": 634
    },
    {
      "epoch": 3.567415730337079,
      "grad_norm": 0.22768127918243408,
      "learning_rate": 1.001971419703411e-05,
      "loss": 0.0171,
      "step": 635
    },
    {
      "epoch": 3.5730337078651684,
      "grad_norm": 1.3362916707992554,
      "learning_rate": 9.946953528529263e-06,
      "loss": 0.0634,
      "step": 636
    },
    {
      "epoch": 3.5786516853932584,
      "grad_norm": 0.6229888796806335,
      "learning_rate": 9.874392341272476e-06,
      "loss": 0.04,
      "step": 637
    },
    {
      "epoch": 3.5842696629213484,
      "grad_norm": 0.10309257358312607,
      "learning_rate": 9.802031596836303e-06,
      "loss": 0.0054,
      "step": 638
    },
    {
      "epoch": 3.5898876404494384,
      "grad_norm": 0.31220731139183044,
      "learning_rate": 9.729872254137038e-06,
      "loss": 0.0168,
      "step": 639
    },
    {
      "epoch": 3.595505617977528,
      "grad_norm": 0.5726065635681152,
      "learning_rate": 9.657915269422003e-06,
      "loss": 0.0307,
      "step": 640
    },
    {
      "epoch": 3.601123595505618,
      "grad_norm": 0.23769521713256836,
      "learning_rate": 9.586161596256959e-06,
      "loss": 0.0143,
      "step": 641
    },
    {
      "epoch": 3.606741573033708,
      "grad_norm": 0.7170079946517944,
      "learning_rate": 9.51461218551335e-06,
      "loss": 0.0268,
      "step": 642
    },
    {
      "epoch": 3.6123595505617976,
      "grad_norm": 0.31180527806282043,
      "learning_rate": 9.443267985355762e-06,
      "loss": 0.0115,
      "step": 643
    },
    {
      "epoch": 3.6179775280898876,
      "grad_norm": 0.17889536917209625,
      "learning_rate": 9.372129941229375e-06,
      "loss": 0.0077,
      "step": 644
    },
    {
      "epoch": 3.6235955056179776,
      "grad_norm": 0.4150654971599579,
      "learning_rate": 9.30119899584741e-06,
      "loss": 0.0184,
      "step": 645
    },
    {
      "epoch": 3.629213483146067,
      "grad_norm": 0.14647138118743896,
      "learning_rate": 9.23047608917863e-06,
      "loss": 0.0103,
      "step": 646
    },
    {
      "epoch": 3.634831460674157,
      "grad_norm": 0.849490761756897,
      "learning_rate": 9.159962158434882e-06,
      "loss": 0.0414,
      "step": 647
    },
    {
      "epoch": 3.640449438202247,
      "grad_norm": 0.4456069767475128,
      "learning_rate": 9.08965813805871e-06,
      "loss": 0.0213,
      "step": 648
    },
    {
      "epoch": 3.646067415730337,
      "grad_norm": 0.6877830028533936,
      "learning_rate": 9.019564959710953e-06,
      "loss": 0.0274,
      "step": 649
    },
    {
      "epoch": 3.6516853932584272,
      "grad_norm": 0.42328599095344543,
      "learning_rate": 8.94968355225837e-06,
      "loss": 0.043,
      "step": 650
    },
    {
      "epoch": 3.657303370786517,
      "grad_norm": 0.512356698513031,
      "learning_rate": 8.880014841761366e-06,
      "loss": 0.029,
      "step": 651
    },
    {
      "epoch": 3.662921348314607,
      "grad_norm": 0.4591677784919739,
      "learning_rate": 8.810559751461723e-06,
      "loss": 0.0256,
      "step": 652
    },
    {
      "epoch": 3.668539325842697,
      "grad_norm": 0.35644879937171936,
      "learning_rate": 8.74131920177035e-06,
      "loss": 0.019,
      "step": 653
    },
    {
      "epoch": 3.6741573033707864,
      "grad_norm": 0.2139647752046585,
      "learning_rate": 8.672294110255077e-06,
      "loss": 0.0091,
      "step": 654
    },
    {
      "epoch": 3.6797752808988764,
      "grad_norm": 0.2063017040491104,
      "learning_rate": 8.603485391628504e-06,
      "loss": 0.0073,
      "step": 655
    },
    {
      "epoch": 3.6853932584269664,
      "grad_norm": 0.3322356641292572,
      "learning_rate": 8.534893957735896e-06,
      "loss": 0.0127,
      "step": 656
    },
    {
      "epoch": 3.691011235955056,
      "grad_norm": 0.38087961077690125,
      "learning_rate": 8.46652071754309e-06,
      "loss": 0.0157,
      "step": 657
    },
    {
      "epoch": 3.696629213483146,
      "grad_norm": 0.24375629425048828,
      "learning_rate": 8.398366577124428e-06,
      "loss": 0.0083,
      "step": 658
    },
    {
      "epoch": 3.702247191011236,
      "grad_norm": 0.9016838669776917,
      "learning_rate": 8.330432439650766e-06,
      "loss": 0.0616,
      "step": 659
    },
    {
      "epoch": 3.7078651685393256,
      "grad_norm": 0.20957644283771515,
      "learning_rate": 8.26271920537752e-06,
      "loss": 0.0086,
      "step": 660
    },
    {
      "epoch": 3.7134831460674156,
      "grad_norm": 0.1785842329263687,
      "learning_rate": 8.195227771632727e-06,
      "loss": 0.0131,
      "step": 661
    },
    {
      "epoch": 3.7191011235955056,
      "grad_norm": 0.6621516942977905,
      "learning_rate": 8.127959032805132e-06,
      "loss": 0.036,
      "step": 662
    },
    {
      "epoch": 3.7247191011235956,
      "grad_norm": 0.6775228381156921,
      "learning_rate": 8.060913880332338e-06,
      "loss": 0.0439,
      "step": 663
    },
    {
      "epoch": 3.7303370786516856,
      "grad_norm": 0.16565072536468506,
      "learning_rate": 7.994093202689065e-06,
      "loss": 0.0073,
      "step": 664
    },
    {
      "epoch": 3.735955056179775,
      "grad_norm": 0.37281668186187744,
      "learning_rate": 7.927497885375262e-06,
      "loss": 0.017,
      "step": 665
    },
    {
      "epoch": 3.741573033707865,
      "grad_norm": 0.6357326507568359,
      "learning_rate": 7.861128810904447e-06,
      "loss": 0.0388,
      "step": 666
    },
    {
      "epoch": 3.747191011235955,
      "grad_norm": 0.3931409418582916,
      "learning_rate": 7.794986858791983e-06,
      "loss": 0.0268,
      "step": 667
    },
    {
      "epoch": 3.752808988764045,
      "grad_norm": 0.5897842049598694,
      "learning_rate": 7.729072905543475e-06,
      "loss": 0.0271,
      "step": 668
    },
    {
      "epoch": 3.758426966292135,
      "grad_norm": 0.8317533135414124,
      "learning_rate": 7.663387824643067e-06,
      "loss": 0.0432,
      "step": 669
    },
    {
      "epoch": 3.764044943820225,
      "grad_norm": 0.3414580523967743,
      "learning_rate": 7.597932486541928e-06,
      "loss": 0.0175,
      "step": 670
    },
    {
      "epoch": 3.7696629213483144,
      "grad_norm": 0.5969368815422058,
      "learning_rate": 7.53270775864669e-06,
      "loss": 0.0207,
      "step": 671
    },
    {
      "epoch": 3.7752808988764044,
      "grad_norm": 0.10669532418251038,
      "learning_rate": 7.4677145053080075e-06,
      "loss": 0.0034,
      "step": 672
    },
    {
      "epoch": 3.7808988764044944,
      "grad_norm": 0.17226198315620422,
      "learning_rate": 7.402953587809017e-06,
      "loss": 0.0068,
      "step": 673
    },
    {
      "epoch": 3.7865168539325844,
      "grad_norm": 0.3509337306022644,
      "learning_rate": 7.3384258643539705e-06,
      "loss": 0.0149,
      "step": 674
    },
    {
      "epoch": 3.7921348314606744,
      "grad_norm": 0.17272280156612396,
      "learning_rate": 7.274132190056881e-06,
      "loss": 0.0075,
      "step": 675
    },
    {
      "epoch": 3.797752808988764,
      "grad_norm": 0.5887287259101868,
      "learning_rate": 7.210073416930163e-06,
      "loss": 0.0318,
      "step": 676
    },
    {
      "epoch": 3.803370786516854,
      "grad_norm": 0.339693546295166,
      "learning_rate": 7.146250393873338e-06,
      "loss": 0.0104,
      "step": 677
    },
    {
      "epoch": 3.808988764044944,
      "grad_norm": 0.33855369687080383,
      "learning_rate": 7.082663966661788e-06,
      "loss": 0.0158,
      "step": 678
    },
    {
      "epoch": 3.8146067415730336,
      "grad_norm": 0.37947162985801697,
      "learning_rate": 7.019314977935574e-06,
      "loss": 0.0225,
      "step": 679
    },
    {
      "epoch": 3.8202247191011236,
      "grad_norm": 0.6488027572631836,
      "learning_rate": 6.956204267188246e-06,
      "loss": 0.0227,
      "step": 680
    },
    {
      "epoch": 3.8258426966292136,
      "grad_norm": 1.0712857246398926,
      "learning_rate": 6.893332670755712e-06,
      "loss": 0.0816,
      "step": 681
    },
    {
      "epoch": 3.831460674157303,
      "grad_norm": 0.7791913747787476,
      "learning_rate": 6.8307010218051555e-06,
      "loss": 0.0384,
      "step": 682
    },
    {
      "epoch": 3.837078651685393,
      "grad_norm": 0.30304697155952454,
      "learning_rate": 6.768310150324031e-06,
      "loss": 0.0112,
      "step": 683
    },
    {
      "epoch": 3.842696629213483,
      "grad_norm": 0.42380958795547485,
      "learning_rate": 6.706160883109022e-06,
      "loss": 0.0352,
      "step": 684
    },
    {
      "epoch": 3.8483146067415728,
      "grad_norm": 0.7866467833518982,
      "learning_rate": 6.6442540437551e-06,
      "loss": 0.0527,
      "step": 685
    },
    {
      "epoch": 3.853932584269663,
      "grad_norm": 0.4341125786304474,
      "learning_rate": 6.582590452644599e-06,
      "loss": 0.0308,
      "step": 686
    },
    {
      "epoch": 3.859550561797753,
      "grad_norm": 0.6707876920700073,
      "learning_rate": 6.521170926936376e-06,
      "loss": 0.0314,
      "step": 687
    },
    {
      "epoch": 3.865168539325843,
      "grad_norm": 1.0970014333724976,
      "learning_rate": 6.459996280554962e-06,
      "loss": 0.079,
      "step": 688
    },
    {
      "epoch": 3.870786516853933,
      "grad_norm": 0.1410064697265625,
      "learning_rate": 6.3990673241797555e-06,
      "loss": 0.0057,
      "step": 689
    },
    {
      "epoch": 3.8764044943820224,
      "grad_norm": 0.5654655694961548,
      "learning_rate": 6.338384865234301e-06,
      "loss": 0.0298,
      "step": 690
    },
    {
      "epoch": 3.8820224719101124,
      "grad_norm": 1.8998111486434937,
      "learning_rate": 6.277949707875605e-06,
      "loss": 0.1302,
      "step": 691
    },
    {
      "epoch": 3.8876404494382024,
      "grad_norm": 0.13512280583381653,
      "learning_rate": 6.217762652983458e-06,
      "loss": 0.0104,
      "step": 692
    },
    {
      "epoch": 3.893258426966292,
      "grad_norm": 0.6335107088088989,
      "learning_rate": 6.157824498149814e-06,
      "loss": 0.0326,
      "step": 693
    },
    {
      "epoch": 3.898876404494382,
      "grad_norm": 0.30760717391967773,
      "learning_rate": 6.09813603766822e-06,
      "loss": 0.0113,
      "step": 694
    },
    {
      "epoch": 3.904494382022472,
      "grad_norm": 0.4556509852409363,
      "learning_rate": 6.0386980625233514e-06,
      "loss": 0.0343,
      "step": 695
    },
    {
      "epoch": 3.9101123595505616,
      "grad_norm": 0.6732022762298584,
      "learning_rate": 5.979511360380429e-06,
      "loss": 0.0265,
      "step": 696
    },
    {
      "epoch": 3.9157303370786516,
      "grad_norm": 0.3264159858226776,
      "learning_rate": 5.920576715574858e-06,
      "loss": 0.0149,
      "step": 697
    },
    {
      "epoch": 3.9213483146067416,
      "grad_norm": 0.5009930729866028,
      "learning_rate": 5.8618949091017835e-06,
      "loss": 0.0164,
      "step": 698
    },
    {
      "epoch": 3.9269662921348316,
      "grad_norm": 0.5599589347839355,
      "learning_rate": 5.803466718605816e-06,
      "loss": 0.022,
      "step": 699
    },
    {
      "epoch": 3.932584269662921,
      "grad_norm": 0.38082897663116455,
      "learning_rate": 5.745292918370629e-06,
      "loss": 0.0135,
      "step": 700
    },
    {
      "epoch": 3.938202247191011,
      "grad_norm": 0.7346242666244507,
      "learning_rate": 5.687374279308755e-06,
      "loss": 0.0454,
      "step": 701
    },
    {
      "epoch": 3.943820224719101,
      "grad_norm": 0.7033613920211792,
      "learning_rate": 5.629711568951373e-06,
      "loss": 0.0319,
      "step": 702
    },
    {
      "epoch": 3.949438202247191,
      "grad_norm": 0.39324983954429626,
      "learning_rate": 5.572305551438126e-06,
      "loss": 0.013,
      "step": 703
    },
    {
      "epoch": 3.955056179775281,
      "grad_norm": 0.22702859342098236,
      "learning_rate": 5.515156987506978e-06,
      "loss": 0.01,
      "step": 704
    },
    {
      "epoch": 3.960674157303371,
      "grad_norm": 1.2352648973464966,
      "learning_rate": 5.458266634484147e-06,
      "loss": 0.0676,
      "step": 705
    },
    {
      "epoch": 3.966292134831461,
      "grad_norm": 0.30919378995895386,
      "learning_rate": 5.401635246274078e-06,
      "loss": 0.0136,
      "step": 706
    },
    {
      "epoch": 3.9719101123595504,
      "grad_norm": 0.9959920644760132,
      "learning_rate": 5.345263573349457e-06,
      "loss": 0.0473,
      "step": 707
    },
    {
      "epoch": 3.9775280898876404,
      "grad_norm": 0.5619927644729614,
      "learning_rate": 5.289152362741226e-06,
      "loss": 0.0246,
      "step": 708
    },
    {
      "epoch": 3.9831460674157304,
      "grad_norm": 0.6252041459083557,
      "learning_rate": 5.2333023580287185e-06,
      "loss": 0.0334,
      "step": 709
    },
    {
      "epoch": 3.98876404494382,
      "grad_norm": 0.36888137459754944,
      "learning_rate": 5.177714299329806e-06,
      "loss": 0.0143,
      "step": 710
    },
    {
      "epoch": 3.99438202247191,
      "grad_norm": 0.3327784240245819,
      "learning_rate": 5.1223889232910854e-06,
      "loss": 0.01,
      "step": 711
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.1074856519699097,
      "learning_rate": 5.0673269630780975e-06,
      "loss": 0.0427,
      "step": 712
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.04249923303723335,
      "eval_runtime": 113.3512,
      "eval_samples_per_second": 0.82,
      "eval_steps_per_second": 0.415,
      "step": 712
    },
    {
      "epoch": 4.00561797752809,
      "grad_norm": 0.3952793478965759,
      "learning_rate": 5.012529148365627e-06,
      "loss": 0.0216,
      "step": 713
    },
    {
      "epoch": 4.01123595505618,
      "grad_norm": 0.26263877749443054,
      "learning_rate": 4.957996205328047e-06,
      "loss": 0.0126,
      "step": 714
    },
    {
      "epoch": 4.01685393258427,
      "grad_norm": 0.47734686732292175,
      "learning_rate": 4.903728856629677e-06,
      "loss": 0.0171,
      "step": 715
    },
    {
      "epoch": 4.022471910112359,
      "grad_norm": 0.38923901319503784,
      "learning_rate": 4.849727821415204e-06,
      "loss": 0.0159,
      "step": 716
    },
    {
      "epoch": 4.02808988764045,
      "grad_norm": 0.21543067693710327,
      "learning_rate": 4.795993815300157e-06,
      "loss": 0.0078,
      "step": 717
    },
    {
      "epoch": 4.033707865168539,
      "grad_norm": 0.6070635318756104,
      "learning_rate": 4.742527550361433e-06,
      "loss": 0.0351,
      "step": 718
    },
    {
      "epoch": 4.03932584269663,
      "grad_norm": 0.42708006501197815,
      "learning_rate": 4.689329735127865e-06,
      "loss": 0.0214,
      "step": 719
    },
    {
      "epoch": 4.044943820224719,
      "grad_norm": 0.3410634994506836,
      "learning_rate": 4.636401074570801e-06,
      "loss": 0.0127,
      "step": 720
    },
    {
      "epoch": 4.050561797752809,
      "grad_norm": 0.7788808941841125,
      "learning_rate": 4.583742270094779e-06,
      "loss": 0.0361,
      "step": 721
    },
    {
      "epoch": 4.056179775280899,
      "grad_norm": 0.32117387652397156,
      "learning_rate": 4.5313540195282586e-06,
      "loss": 0.0123,
      "step": 722
    },
    {
      "epoch": 4.061797752808989,
      "grad_norm": 0.5241531133651733,
      "learning_rate": 4.479237017114338e-06,
      "loss": 0.019,
      "step": 723
    },
    {
      "epoch": 4.067415730337078,
      "grad_norm": 1.1185176372528076,
      "learning_rate": 4.427391953501553e-06,
      "loss": 0.0518,
      "step": 724
    },
    {
      "epoch": 4.073033707865169,
      "grad_norm": 0.23951058089733124,
      "learning_rate": 4.375819515734761e-06,
      "loss": 0.0127,
      "step": 725
    },
    {
      "epoch": 4.078651685393258,
      "grad_norm": 0.672022819519043,
      "learning_rate": 4.324520387246001e-06,
      "loss": 0.0546,
      "step": 726
    },
    {
      "epoch": 4.084269662921348,
      "grad_norm": 0.26243385672569275,
      "learning_rate": 4.2734952478454465e-06,
      "loss": 0.0134,
      "step": 727
    },
    {
      "epoch": 4.089887640449438,
      "grad_norm": 0.194898322224617,
      "learning_rate": 4.222744773712398e-06,
      "loss": 0.0064,
      "step": 728
    },
    {
      "epoch": 4.095505617977528,
      "grad_norm": 0.16315990686416626,
      "learning_rate": 4.172269637386328e-06,
      "loss": 0.0075,
      "step": 729
    },
    {
      "epoch": 4.101123595505618,
      "grad_norm": 0.2204105406999588,
      "learning_rate": 4.122070507757972e-06,
      "loss": 0.0086,
      "step": 730
    },
    {
      "epoch": 4.106741573033708,
      "grad_norm": 0.654830276966095,
      "learning_rate": 4.0721480500604425e-06,
      "loss": 0.0501,
      "step": 731
    },
    {
      "epoch": 4.112359550561798,
      "grad_norm": 0.3605179488658905,
      "learning_rate": 4.0225029258604215e-06,
      "loss": 0.0284,
      "step": 732
    },
    {
      "epoch": 4.117977528089888,
      "grad_norm": 0.1989099383354187,
      "learning_rate": 3.973135793049423e-06,
      "loss": 0.0058,
      "step": 733
    },
    {
      "epoch": 4.123595505617978,
      "grad_norm": 0.6416724324226379,
      "learning_rate": 3.924047305835038e-06,
      "loss": 0.0341,
      "step": 734
    },
    {
      "epoch": 4.129213483146067,
      "grad_norm": 0.486705482006073,
      "learning_rate": 3.875238114732272e-06,
      "loss": 0.0138,
      "step": 735
    },
    {
      "epoch": 4.134831460674158,
      "grad_norm": 0.19389277696609497,
      "learning_rate": 3.826708866554934e-06,
      "loss": 0.0068,
      "step": 736
    },
    {
      "epoch": 4.140449438202247,
      "grad_norm": 0.23491749167442322,
      "learning_rate": 3.7784602044070666e-06,
      "loss": 0.0084,
      "step": 737
    },
    {
      "epoch": 4.146067415730337,
      "grad_norm": 0.6786869764328003,
      "learning_rate": 3.7304927676744206e-06,
      "loss": 0.0272,
      "step": 738
    },
    {
      "epoch": 4.151685393258427,
      "grad_norm": 1.1582865715026855,
      "learning_rate": 3.6828071920159716e-06,
      "loss": 0.0492,
      "step": 739
    },
    {
      "epoch": 4.157303370786517,
      "grad_norm": 0.23606882989406586,
      "learning_rate": 3.6354041093555e-06,
      "loss": 0.0065,
      "step": 740
    },
    {
      "epoch": 4.162921348314606,
      "grad_norm": 0.20275209844112396,
      "learning_rate": 3.588284147873233e-06,
      "loss": 0.0098,
      "step": 741
    },
    {
      "epoch": 4.168539325842697,
      "grad_norm": 0.3813026249408722,
      "learning_rate": 3.541447931997516e-06,
      "loss": 0.0167,
      "step": 742
    },
    {
      "epoch": 4.174157303370786,
      "grad_norm": 0.4306042194366455,
      "learning_rate": 3.494896082396501e-06,
      "loss": 0.0168,
      "step": 743
    },
    {
      "epoch": 4.179775280898877,
      "grad_norm": 1.4228516817092896,
      "learning_rate": 3.448629215969981e-06,
      "loss": 0.0712,
      "step": 744
    },
    {
      "epoch": 4.185393258426966,
      "grad_norm": 0.17501208186149597,
      "learning_rate": 3.4026479458411613e-06,
      "loss": 0.0054,
      "step": 745
    },
    {
      "epoch": 4.191011235955056,
      "grad_norm": 0.1424276977777481,
      "learning_rate": 3.3569528813485798e-06,
      "loss": 0.0038,
      "step": 746
    },
    {
      "epoch": 4.196629213483146,
      "grad_norm": 0.782694399356842,
      "learning_rate": 3.3115446280379853e-06,
      "loss": 0.0312,
      "step": 747
    },
    {
      "epoch": 4.202247191011236,
      "grad_norm": 0.5243803858757019,
      "learning_rate": 3.2664237876543657e-06,
      "loss": 0.0123,
      "step": 748
    },
    {
      "epoch": 4.207865168539326,
      "grad_norm": 0.8657116293907166,
      "learning_rate": 3.221590958133913e-06,
      "loss": 0.0258,
      "step": 749
    },
    {
      "epoch": 4.213483146067416,
      "grad_norm": 0.3644671142101288,
      "learning_rate": 3.17704673359617e-06,
      "loss": 0.0111,
      "step": 750
    },
    {
      "epoch": 4.219101123595506,
      "grad_norm": 0.3677932024002075,
      "learning_rate": 3.13279170433608e-06,
      "loss": 0.0106,
      "step": 751
    },
    {
      "epoch": 4.224719101123595,
      "grad_norm": 0.3966246545314789,
      "learning_rate": 3.088826456816238e-06,
      "loss": 0.0199,
      "step": 752
    },
    {
      "epoch": 4.230337078651686,
      "grad_norm": 0.2065843641757965,
      "learning_rate": 3.0451515736590607e-06,
      "loss": 0.0081,
      "step": 753
    },
    {
      "epoch": 4.235955056179775,
      "grad_norm": 0.6749172806739807,
      "learning_rate": 3.001767633639105e-06,
      "loss": 0.0369,
      "step": 754
    },
    {
      "epoch": 4.241573033707865,
      "grad_norm": 0.4460354149341583,
      "learning_rate": 2.9586752116753657e-06,
      "loss": 0.0115,
      "step": 755
    },
    {
      "epoch": 4.247191011235955,
      "grad_norm": 0.3362507224082947,
      "learning_rate": 2.9158748788236904e-06,
      "loss": 0.0124,
      "step": 756
    },
    {
      "epoch": 4.252808988764045,
      "grad_norm": 0.35451585054397583,
      "learning_rate": 2.873367202269195e-06,
      "loss": 0.0078,
      "step": 757
    },
    {
      "epoch": 4.258426966292135,
      "grad_norm": 0.15520529448986053,
      "learning_rate": 2.831152745318738e-06,
      "loss": 0.0033,
      "step": 758
    },
    {
      "epoch": 4.264044943820225,
      "grad_norm": 0.3928739130496979,
      "learning_rate": 2.7892320673934573e-06,
      "loss": 0.0143,
      "step": 759
    },
    {
      "epoch": 4.269662921348314,
      "grad_norm": 0.6960135698318481,
      "learning_rate": 2.7476057240213878e-06,
      "loss": 0.0304,
      "step": 760
    },
    {
      "epoch": 4.275280898876405,
      "grad_norm": 0.35365140438079834,
      "learning_rate": 2.7062742668300667e-06,
      "loss": 0.0331,
      "step": 761
    },
    {
      "epoch": 4.280898876404494,
      "grad_norm": 0.37262749671936035,
      "learning_rate": 2.6652382435392252e-06,
      "loss": 0.0088,
      "step": 762
    },
    {
      "epoch": 4.286516853932584,
      "grad_norm": 0.6310207843780518,
      "learning_rate": 2.6244981979535454e-06,
      "loss": 0.0598,
      "step": 763
    },
    {
      "epoch": 4.292134831460674,
      "grad_norm": 0.41596585512161255,
      "learning_rate": 2.584054669955446e-06,
      "loss": 0.0175,
      "step": 764
    },
    {
      "epoch": 4.297752808988764,
      "grad_norm": 0.9878078103065491,
      "learning_rate": 2.5439081954979348e-06,
      "loss": 0.0465,
      "step": 765
    },
    {
      "epoch": 4.303370786516854,
      "grad_norm": 0.1687011420726776,
      "learning_rate": 2.5040593065974867e-06,
      "loss": 0.0051,
      "step": 766
    },
    {
      "epoch": 4.308988764044944,
      "grad_norm": 0.3809362053871155,
      "learning_rate": 2.46450853132702e-06,
      "loss": 0.0144,
      "step": 767
    },
    {
      "epoch": 4.314606741573034,
      "grad_norm": 0.6166444420814514,
      "learning_rate": 2.4252563938088724e-06,
      "loss": 0.0431,
      "step": 768
    },
    {
      "epoch": 4.320224719101123,
      "grad_norm": 0.7357404232025146,
      "learning_rate": 2.3863034142078854e-06,
      "loss": 0.0266,
      "step": 769
    },
    {
      "epoch": 4.325842696629214,
      "grad_norm": 0.1773853451013565,
      "learning_rate": 2.347650108724472e-06,
      "loss": 0.0134,
      "step": 770
    },
    {
      "epoch": 4.331460674157303,
      "grad_norm": 0.6882417798042297,
      "learning_rate": 2.3092969895878197e-06,
      "loss": 0.0316,
      "step": 771
    },
    {
      "epoch": 4.337078651685394,
      "grad_norm": 0.5818739533424377,
      "learning_rate": 2.2712445650490662e-06,
      "loss": 0.0436,
      "step": 772
    },
    {
      "epoch": 4.342696629213483,
      "grad_norm": 0.4639415740966797,
      "learning_rate": 2.23349333937459e-06,
      "loss": 0.0171,
      "step": 773
    },
    {
      "epoch": 4.348314606741573,
      "grad_norm": 0.4959847331047058,
      "learning_rate": 2.196043812839302e-06,
      "loss": 0.0211,
      "step": 774
    },
    {
      "epoch": 4.353932584269663,
      "grad_norm": 0.5400422215461731,
      "learning_rate": 2.1588964817200462e-06,
      "loss": 0.0219,
      "step": 775
    },
    {
      "epoch": 4.359550561797753,
      "grad_norm": 0.3332066833972931,
      "learning_rate": 2.1220518382889966e-06,
      "loss": 0.0142,
      "step": 776
    },
    {
      "epoch": 4.365168539325842,
      "grad_norm": 0.6896002888679504,
      "learning_rate": 2.0855103708071533e-06,
      "loss": 0.0298,
      "step": 777
    },
    {
      "epoch": 4.370786516853933,
      "grad_norm": 0.8346617817878723,
      "learning_rate": 2.049272563517851e-06,
      "loss": 0.0245,
      "step": 778
    },
    {
      "epoch": 4.376404494382022,
      "grad_norm": 0.7429971694946289,
      "learning_rate": 2.0133388966403738e-06,
      "loss": 0.0367,
      "step": 779
    },
    {
      "epoch": 4.382022471910112,
      "grad_norm": 1.1408625841140747,
      "learning_rate": 1.9777098463635458e-06,
      "loss": 0.0555,
      "step": 780
    },
    {
      "epoch": 4.387640449438202,
      "grad_norm": 0.422626793384552,
      "learning_rate": 1.942385884839476e-06,
      "loss": 0.0112,
      "step": 781
    },
    {
      "epoch": 4.393258426966292,
      "grad_norm": 0.5706059336662292,
      "learning_rate": 1.9073674801772467e-06,
      "loss": 0.0297,
      "step": 782
    },
    {
      "epoch": 4.398876404494382,
      "grad_norm": 0.8903041481971741,
      "learning_rate": 1.872655096436754e-06,
      "loss": 0.0271,
      "step": 783
    },
    {
      "epoch": 4.404494382022472,
      "grad_norm": 0.3558393716812134,
      "learning_rate": 1.8382491936225238e-06,
      "loss": 0.0163,
      "step": 784
    },
    {
      "epoch": 4.410112359550562,
      "grad_norm": 0.5852181315422058,
      "learning_rate": 1.804150227677645e-06,
      "loss": 0.0316,
      "step": 785
    },
    {
      "epoch": 4.415730337078652,
      "grad_norm": 0.9531477093696594,
      "learning_rate": 1.770358650477716e-06,
      "loss": 0.0366,
      "step": 786
    },
    {
      "epoch": 4.421348314606742,
      "grad_norm": 0.2978372871875763,
      "learning_rate": 1.7368749098248343e-06,
      "loss": 0.0065,
      "step": 787
    },
    {
      "epoch": 4.426966292134831,
      "grad_norm": 1.303717851638794,
      "learning_rate": 1.7036994494417092e-06,
      "loss": 0.0684,
      "step": 788
    },
    {
      "epoch": 4.432584269662922,
      "grad_norm": 0.31201040744781494,
      "learning_rate": 1.6708327089657332e-06,
      "loss": 0.0079,
      "step": 789
    },
    {
      "epoch": 4.438202247191011,
      "grad_norm": 0.3328584134578705,
      "learning_rate": 1.6382751239431976e-06,
      "loss": 0.0095,
      "step": 790
    },
    {
      "epoch": 4.443820224719101,
      "grad_norm": 0.7108306288719177,
      "learning_rate": 1.606027125823481e-06,
      "loss": 0.0337,
      "step": 791
    },
    {
      "epoch": 4.449438202247191,
      "grad_norm": 0.41732537746429443,
      "learning_rate": 1.5740891419533665e-06,
      "loss": 0.0112,
      "step": 792
    },
    {
      "epoch": 4.455056179775281,
      "grad_norm": 0.9513748288154602,
      "learning_rate": 1.5424615955713567e-06,
      "loss": 0.0758,
      "step": 793
    },
    {
      "epoch": 4.460674157303371,
      "grad_norm": 0.3340839743614197,
      "learning_rate": 1.5111449058020783e-06,
      "loss": 0.0073,
      "step": 794
    },
    {
      "epoch": 4.466292134831461,
      "grad_norm": 0.19679497182369232,
      "learning_rate": 1.480139487650714e-06,
      "loss": 0.0059,
      "step": 795
    },
    {
      "epoch": 4.47191011235955,
      "grad_norm": 0.43229249119758606,
      "learning_rate": 1.449445751997519e-06,
      "loss": 0.0099,
      "step": 796
    },
    {
      "epoch": 4.477528089887641,
      "grad_norm": 0.16445647180080414,
      "learning_rate": 1.4190641055923547e-06,
      "loss": 0.0061,
      "step": 797
    },
    {
      "epoch": 4.48314606741573,
      "grad_norm": 0.9145157337188721,
      "learning_rate": 1.3889949510493329e-06,
      "loss": 0.0565,
      "step": 798
    },
    {
      "epoch": 4.48876404494382,
      "grad_norm": 0.5635059475898743,
      "learning_rate": 1.3592386868414326e-06,
      "loss": 0.0243,
      "step": 799
    },
    {
      "epoch": 4.49438202247191,
      "grad_norm": 0.5030380487442017,
      "learning_rate": 1.3297957072952727e-06,
      "loss": 0.0166,
      "step": 800
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.30129754543304443,
      "learning_rate": 1.3006664025858344e-06,
      "loss": 0.0109,
      "step": 801
    },
    {
      "epoch": 4.50561797752809,
      "grad_norm": 0.28306442499160767,
      "learning_rate": 1.2718511587313381e-06,
      "loss": 0.0084,
      "step": 802
    },
    {
      "epoch": 4.51123595505618,
      "grad_norm": 0.295474648475647,
      "learning_rate": 1.2433503575880885e-06,
      "loss": 0.0091,
      "step": 803
    },
    {
      "epoch": 4.51685393258427,
      "grad_norm": 0.4292944371700287,
      "learning_rate": 1.2151643768454428e-06,
      "loss": 0.0138,
      "step": 804
    },
    {
      "epoch": 4.52247191011236,
      "grad_norm": 0.3386789560317993,
      "learning_rate": 1.1872935900207882e-06,
      "loss": 0.0127,
      "step": 805
    },
    {
      "epoch": 4.52808988764045,
      "grad_norm": 0.8302147388458252,
      "learning_rate": 1.1597383664546019e-06,
      "loss": 0.0376,
      "step": 806
    },
    {
      "epoch": 4.533707865168539,
      "grad_norm": 0.281240850687027,
      "learning_rate": 1.1324990713055484e-06,
      "loss": 0.0081,
      "step": 807
    },
    {
      "epoch": 4.539325842696629,
      "grad_norm": 0.5681421756744385,
      "learning_rate": 1.1055760655456465e-06,
      "loss": 0.0136,
      "step": 808
    },
    {
      "epoch": 4.544943820224719,
      "grad_norm": 0.3962855935096741,
      "learning_rate": 1.0789697059554933e-06,
      "loss": 0.0132,
      "step": 809
    },
    {
      "epoch": 4.550561797752809,
      "grad_norm": 0.8572372794151306,
      "learning_rate": 1.0526803451195139e-06,
      "loss": 0.0451,
      "step": 810
    },
    {
      "epoch": 4.556179775280899,
      "grad_norm": 0.3541194498538971,
      "learning_rate": 1.0267083314213056e-06,
      "loss": 0.0086,
      "step": 811
    },
    {
      "epoch": 4.561797752808989,
      "grad_norm": 0.33338576555252075,
      "learning_rate": 1.0010540090390209e-06,
      "loss": 0.0113,
      "step": 812
    },
    {
      "epoch": 4.567415730337078,
      "grad_norm": 0.41696467995643616,
      "learning_rate": 9.75717717940805e-07,
      "loss": 0.0119,
      "step": 813
    },
    {
      "epoch": 4.573033707865169,
      "grad_norm": 0.6767743229866028,
      "learning_rate": 9.506997938802886e-07,
      "loss": 0.0166,
      "step": 814
    },
    {
      "epoch": 4.578651685393258,
      "grad_norm": 0.797034502029419,
      "learning_rate": 9.260005683921269e-07,
      "loss": 0.0351,
      "step": 815
    },
    {
      "epoch": 4.584269662921348,
      "grad_norm": 0.2832901179790497,
      "learning_rate": 9.01620368787634e-07,
      "loss": 0.0076,
      "step": 816
    },
    {
      "epoch": 4.589887640449438,
      "grad_norm": 0.6661422252655029,
      "learning_rate": 8.775595181504225e-07,
      "loss": 0.03,
      "step": 817
    },
    {
      "epoch": 4.595505617977528,
      "grad_norm": 0.6341362595558167,
      "learning_rate": 8.538183353321295e-07,
      "loss": 0.0258,
      "step": 818
    },
    {
      "epoch": 4.601123595505618,
      "grad_norm": 0.20943298935890198,
      "learning_rate": 8.303971349481937e-07,
      "loss": 0.0064,
      "step": 819
    },
    {
      "epoch": 4.606741573033708,
      "grad_norm": 0.36725786328315735,
      "learning_rate": 8.072962273736772e-07,
      "loss": 0.0103,
      "step": 820
    },
    {
      "epoch": 4.612359550561798,
      "grad_norm": 0.8325411081314087,
      "learning_rate": 7.84515918739162e-07,
      "loss": 0.0471,
      "step": 821
    },
    {
      "epoch": 4.617977528089888,
      "grad_norm": 0.41093429923057556,
      "learning_rate": 7.620565109266897e-07,
      "loss": 0.0165,
      "step": 822
    },
    {
      "epoch": 4.623595505617978,
      "grad_norm": 0.19981437921524048,
      "learning_rate": 7.399183015657645e-07,
      "loss": 0.0081,
      "step": 823
    },
    {
      "epoch": 4.629213483146067,
      "grad_norm": 0.32819196581840515,
      "learning_rate": 7.181015840293931e-07,
      "loss": 0.0104,
      "step": 824
    },
    {
      "epoch": 4.634831460674158,
      "grad_norm": 0.5680370926856995,
      "learning_rate": 6.966066474302285e-07,
      "loss": 0.0306,
      "step": 825
    },
    {
      "epoch": 4.640449438202247,
      "grad_norm": 0.8771331906318665,
      "learning_rate": 6.75433776616699e-07,
      "loss": 0.0243,
      "step": 826
    },
    {
      "epoch": 4.646067415730337,
      "grad_norm": 0.4826637804508209,
      "learning_rate": 6.545832521692691e-07,
      "loss": 0.0104,
      "step": 827
    },
    {
      "epoch": 4.651685393258427,
      "grad_norm": 0.4190489947795868,
      "learning_rate": 6.340553503967062e-07,
      "loss": 0.0142,
      "step": 828
    },
    {
      "epoch": 4.657303370786517,
      "grad_norm": 0.38745802640914917,
      "learning_rate": 6.138503433324061e-07,
      "loss": 0.0262,
      "step": 829
    },
    {
      "epoch": 4.662921348314606,
      "grad_norm": 0.6288148760795593,
      "learning_rate": 5.939684987308098e-07,
      "loss": 0.0165,
      "step": 830
    },
    {
      "epoch": 4.668539325842697,
      "grad_norm": 0.9109583497047424,
      "learning_rate": 5.744100800638447e-07,
      "loss": 0.0367,
      "step": 831
    },
    {
      "epoch": 4.674157303370786,
      "grad_norm": 0.8810983300209045,
      "learning_rate": 5.55175346517442e-07,
      "loss": 0.0346,
      "step": 832
    },
    {
      "epoch": 4.679775280898877,
      "grad_norm": 0.8256559371948242,
      "learning_rate": 5.362645529880806e-07,
      "loss": 0.0264,
      "step": 833
    },
    {
      "epoch": 4.685393258426966,
      "grad_norm": 1.3403141498565674,
      "learning_rate": 5.176779500794316e-07,
      "loss": 0.0473,
      "step": 834
    },
    {
      "epoch": 4.691011235955056,
      "grad_norm": 0.5344443321228027,
      "learning_rate": 4.994157840990304e-07,
      "loss": 0.0179,
      "step": 835
    },
    {
      "epoch": 4.696629213483146,
      "grad_norm": 0.2876614034175873,
      "learning_rate": 4.81478297055013e-07,
      "loss": 0.0071,
      "step": 836
    },
    {
      "epoch": 4.702247191011236,
      "grad_norm": 0.430186927318573,
      "learning_rate": 4.6386572665289563e-07,
      "loss": 0.0245,
      "step": 837
    },
    {
      "epoch": 4.707865168539326,
      "grad_norm": 0.29024621844291687,
      "learning_rate": 4.4657830629245e-07,
      "loss": 0.0072,
      "step": 838
    },
    {
      "epoch": 4.713483146067416,
      "grad_norm": 0.1778210699558258,
      "learning_rate": 4.296162650645891e-07,
      "loss": 0.0051,
      "step": 839
    },
    {
      "epoch": 4.719101123595506,
      "grad_norm": 0.7500289678573608,
      "learning_rate": 4.1297982774834153e-07,
      "loss": 0.0591,
      "step": 840
    },
    {
      "epoch": 4.724719101123595,
      "grad_norm": 0.5910874009132385,
      "learning_rate": 3.9666921480786823e-07,
      "loss": 0.0355,
      "step": 841
    },
    {
      "epoch": 4.730337078651686,
      "grad_norm": 0.5801687240600586,
      "learning_rate": 3.8068464238954494e-07,
      "loss": 0.0234,
      "step": 842
    },
    {
      "epoch": 4.735955056179775,
      "grad_norm": 0.3310309648513794,
      "learning_rate": 3.650263223190953e-07,
      "loss": 0.0107,
      "step": 843
    },
    {
      "epoch": 4.741573033707866,
      "grad_norm": 0.2846485376358032,
      "learning_rate": 3.496944620987846e-07,
      "loss": 0.0081,
      "step": 844
    },
    {
      "epoch": 4.747191011235955,
      "grad_norm": 0.8934855461120605,
      "learning_rate": 3.346892649046668e-07,
      "loss": 0.0374,
      "step": 845
    },
    {
      "epoch": 4.752808988764045,
      "grad_norm": 0.15845240652561188,
      "learning_rate": 3.2001092958389713e-07,
      "loss": 0.0048,
      "step": 846
    },
    {
      "epoch": 4.758426966292134,
      "grad_norm": 0.7819644212722778,
      "learning_rate": 3.056596506520959e-07,
      "loss": 0.0319,
      "step": 847
    },
    {
      "epoch": 4.764044943820225,
      "grad_norm": 0.5686397552490234,
      "learning_rate": 2.91635618290767e-07,
      "loss": 0.0294,
      "step": 848
    },
    {
      "epoch": 4.769662921348314,
      "grad_norm": 1.052743911743164,
      "learning_rate": 2.779390183447805e-07,
      "loss": 0.0574,
      "step": 849
    },
    {
      "epoch": 4.775280898876405,
      "grad_norm": 1.2367295026779175,
      "learning_rate": 2.645700323199135e-07,
      "loss": 0.0506,
      "step": 850
    },
    {
      "epoch": 4.780898876404494,
      "grad_norm": 0.5808574557304382,
      "learning_rate": 2.515288373804353e-07,
      "loss": 0.0289,
      "step": 851
    },
    {
      "epoch": 4.786516853932584,
      "grad_norm": 1.069515585899353,
      "learning_rate": 2.388156063467678e-07,
      "loss": 0.0541,
      "step": 852
    },
    {
      "epoch": 4.792134831460674,
      "grad_norm": 0.8200427889823914,
      "learning_rate": 2.2643050769319274e-07,
      "loss": 0.0378,
      "step": 853
    },
    {
      "epoch": 4.797752808988764,
      "grad_norm": 1.0340576171875,
      "learning_rate": 2.143737055456202e-07,
      "loss": 0.0446,
      "step": 854
    },
    {
      "epoch": 4.803370786516854,
      "grad_norm": 0.38605374097824097,
      "learning_rate": 2.0264535967941257e-07,
      "loss": 0.0202,
      "step": 855
    },
    {
      "epoch": 4.808988764044944,
      "grad_norm": 0.6522224545478821,
      "learning_rate": 1.91245625517264e-07,
      "loss": 0.0186,
      "step": 856
    },
    {
      "epoch": 4.814606741573034,
      "grad_norm": 0.6203607320785522,
      "learning_rate": 1.8017465412714653e-07,
      "loss": 0.0157,
      "step": 857
    },
    {
      "epoch": 4.820224719101123,
      "grad_norm": 0.41697314381599426,
      "learning_rate": 1.6943259222030327e-07,
      "loss": 0.0145,
      "step": 858
    },
    {
      "epoch": 4.825842696629214,
      "grad_norm": 0.915151059627533,
      "learning_rate": 1.590195821493168e-07,
      "loss": 0.048,
      "step": 859
    },
    {
      "epoch": 4.831460674157303,
      "grad_norm": 0.4919649362564087,
      "learning_rate": 1.4893576190619374e-07,
      "loss": 0.0142,
      "step": 860
    },
    {
      "epoch": 4.837078651685394,
      "grad_norm": 0.6264019012451172,
      "learning_rate": 1.3918126512057206e-07,
      "loss": 0.0249,
      "step": 861
    },
    {
      "epoch": 4.842696629213483,
      "grad_norm": 0.4064338207244873,
      "learning_rate": 1.2975622105792228e-07,
      "loss": 0.0199,
      "step": 862
    },
    {
      "epoch": 4.848314606741573,
      "grad_norm": 0.3244265615940094,
      "learning_rate": 1.2066075461785175e-07,
      "loss": 0.0138,
      "step": 863
    },
    {
      "epoch": 4.853932584269663,
      "grad_norm": 1.3744829893112183,
      "learning_rate": 1.1189498633243923e-07,
      "loss": 0.0554,
      "step": 864
    },
    {
      "epoch": 4.859550561797753,
      "grad_norm": 0.11797956377267838,
      "learning_rate": 1.0345903236464171e-07,
      "loss": 0.0034,
      "step": 865
    },
    {
      "epoch": 4.865168539325842,
      "grad_norm": 0.6155065298080444,
      "learning_rate": 9.53530045067541e-08,
      "loss": 0.0238,
      "step": 866
    },
    {
      "epoch": 4.870786516853933,
      "grad_norm": 0.3314250707626343,
      "learning_rate": 8.757701017893527e-08,
      "loss": 0.0131,
      "step": 867
    },
    {
      "epoch": 4.876404494382022,
      "grad_norm": 0.37583237886428833,
      "learning_rate": 8.013115242776204e-08,
      "loss": 0.0106,
      "step": 868
    },
    {
      "epoch": 4.882022471910112,
      "grad_norm": 0.1802099049091339,
      "learning_rate": 7.301552992489691e-08,
      "loss": 0.004,
      "step": 869
    },
    {
      "epoch": 4.887640449438202,
      "grad_norm": 0.17950153350830078,
      "learning_rate": 6.623023696575025e-08,
      "loss": 0.0059,
      "step": 870
    },
    {
      "epoch": 4.893258426966292,
      "grad_norm": 0.13575655221939087,
      "learning_rate": 5.977536346824796e-08,
      "loss": 0.0063,
      "step": 871
    },
    {
      "epoch": 4.898876404494382,
      "grad_norm": 0.42933982610702515,
      "learning_rate": 5.365099497163517e-08,
      "loss": 0.031,
      "step": 872
    },
    {
      "epoch": 4.904494382022472,
      "grad_norm": 0.30198338627815247,
      "learning_rate": 4.785721263534104e-08,
      "loss": 0.0134,
      "step": 873
    },
    {
      "epoch": 4.910112359550562,
      "grad_norm": 1.240272045135498,
      "learning_rate": 4.2394093237904664e-08,
      "loss": 0.0354,
      "step": 874
    },
    {
      "epoch": 4.915730337078652,
      "grad_norm": 0.8472585082054138,
      "learning_rate": 3.7261709175961946e-08,
      "loss": 0.021,
      "step": 875
    },
    {
      "epoch": 4.921348314606742,
      "grad_norm": 0.786842405796051,
      "learning_rate": 3.246012846327695e-08,
      "loss": 0.0243,
      "step": 876
    },
    {
      "epoch": 4.926966292134831,
      "grad_norm": 0.74837327003479,
      "learning_rate": 2.7989414729848172e-08,
      "loss": 0.0233,
      "step": 877
    },
    {
      "epoch": 4.932584269662922,
      "grad_norm": 0.8897414207458496,
      "learning_rate": 2.3849627221059213e-08,
      "loss": 0.04,
      "step": 878
    },
    {
      "epoch": 4.938202247191011,
      "grad_norm": 0.12949657440185547,
      "learning_rate": 2.0040820796904392e-08,
      "loss": 0.0046,
      "step": 879
    },
    {
      "epoch": 4.943820224719101,
      "grad_norm": 0.5521718263626099,
      "learning_rate": 1.6563045931247688e-08,
      "loss": 0.0314,
      "step": 880
    },
    {
      "epoch": 4.949438202247191,
      "grad_norm": 0.5571111440658569,
      "learning_rate": 1.3416348711162152e-08,
      "loss": 0.0365,
      "step": 881
    },
    {
      "epoch": 4.955056179775281,
      "grad_norm": 0.5595490336418152,
      "learning_rate": 1.060077083632205e-08,
      "loss": 0.0333,
      "step": 882
    },
    {
      "epoch": 4.960674157303371,
      "grad_norm": 0.7512447834014893,
      "learning_rate": 8.116349618442209e-09,
      "loss": 0.0248,
      "step": 883
    },
    {
      "epoch": 4.966292134831461,
      "grad_norm": 0.2456454485654831,
      "learning_rate": 5.963117980786747e-09,
      "loss": 0.0078,
      "step": 884
    },
    {
      "epoch": 4.97191011235955,
      "grad_norm": 0.7919929027557373,
      "learning_rate": 4.141104457733302e-09,
      "loss": 0.0165,
      "step": 885
    },
    {
      "epoch": 4.97752808988764,
      "grad_norm": 0.5855762958526611,
      "learning_rate": 2.650333194401111e-09,
      "loss": 0.0179,
      "step": 886
    },
    {
      "epoch": 4.98314606741573,
      "grad_norm": 0.22347038984298706,
      "learning_rate": 1.490823946317943e-09,
      "loss": 0.0065,
      "step": 887
    },
    {
      "epoch": 4.98876404494382,
      "grad_norm": 0.45435476303100586,
      "learning_rate": 6.625920791647477e-10,
      "loss": 0.0182,
      "step": 888
    },
    {
      "epoch": 4.99438202247191,
      "grad_norm": 0.7594291567802429,
      "learning_rate": 1.656485685813669e-10,
      "loss": 0.0358,
      "step": 889
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.23471428453922272,
      "learning_rate": 0.0,
      "loss": 0.0045,
      "step": 890
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.03954489156603813,
      "eval_runtime": 98.5968,
      "eval_samples_per_second": 0.943,
      "eval_steps_per_second": 0.477,
      "step": 890
    },
    {
      "epoch": 5.0,
      "step": 890,
      "total_flos": 1.332601587499008e+17,
      "train_loss": 0.11717486925124913,
      "train_runtime": 5804.1352,
      "train_samples_per_second": 0.306,
      "train_steps_per_second": 0.153
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 890,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.332601587499008e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
