{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1040,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  
  "log_history": [
    {
      "epoch": 0.004807692307692308,
      "grad_norm": 3.2186036106943265,
      "learning_rate": 6.25e-07,
      "loss": 11.3074,
      "step": 1
    },
    {
      "epoch": 0.009615384615384616,
      "grad_norm": 3.8998964456521037,
      "learning_rate": 1.25e-06,
      "loss": 11.619,
      "step": 2
    },
    {
      "epoch": 0.014423076923076924,
      "grad_norm": 4.231601930596243,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 11.0758,
      "step": 3
    },
    {
      "epoch": 0.019230769230769232,
      "grad_norm": 2.880662059689913,
      "learning_rate": 2.5e-06,
      "loss": 11.5563,
      "step": 4
    },
    {
      "epoch": 0.02403846153846154,
      "grad_norm": 3.0410393216671046,
      "learning_rate": 3.125e-06,
      "loss": 11.8171,
      "step": 5
    },
    {
      "epoch": 0.028846153846153848,
      "grad_norm": 4.3851595832610295,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 12.151,
      "step": 6
    },
    {
      "epoch": 0.03365384615384615,
      "grad_norm": 3.001752603516922,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 11.0601,
      "step": 7
    },
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 3.0798650601355133,
      "learning_rate": 5e-06,
      "loss": 11.4121,
      "step": 8
    },
    {
      "epoch": 0.04326923076923077,
      "grad_norm": 3.1217109641662066,
      "learning_rate": 5.625e-06,
      "loss": 10.6379,
      "step": 9
    },
    {
      "epoch": 0.04807692307692308,
      "grad_norm": 3.5842486382319834,
      "learning_rate": 6.25e-06,
      "loss": 11.7668,
      "step": 10
    },
    {
      "epoch": 0.052884615384615384,
      "grad_norm": 3.2011520297512788,
      "learning_rate": 6.875e-06,
      "loss": 11.2549,
      "step": 11
    },
    {
      "epoch": 0.057692307692307696,
      "grad_norm": 3.3186824614400408,
      "learning_rate": 7.500000000000001e-06,
      "loss": 11.0669,
      "step": 12
    },
    {
      "epoch": 0.0625,
      "grad_norm": 3.5773459174784574,
      "learning_rate": 8.125000000000001e-06,
      "loss": 11.3244,
      "step": 13
    },
    {
      "epoch": 0.0673076923076923,
      "grad_norm": 3.7417541672531556,
      "learning_rate": 8.750000000000001e-06,
      "loss": 10.8638,
      "step": 14
    },
    {
      "epoch": 0.07211538461538461,
      "grad_norm": 3.606748548737136,
      "learning_rate": 9.375000000000001e-06,
      "loss": 11.1884,
      "step": 15
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 3.7122905468718077,
      "learning_rate": 1e-05,
      "loss": 11.2372,
      "step": 16
    },
    {
      "epoch": 0.08173076923076923,
      "grad_norm": 3.511737863615911,
      "learning_rate": 1.0625e-05,
      "loss": 11.7137,
      "step": 17
    },
    {
      "epoch": 0.08653846153846154,
      "grad_norm": 3.7547555681007214,
      "learning_rate": 1.125e-05,
      "loss": 10.9992,
      "step": 18
    },
    {
      "epoch": 0.09134615384615384,
      "grad_norm": 3.8405480886138146,
      "learning_rate": 1.1875e-05,
      "loss": 11.1427,
      "step": 19
    },
    {
      "epoch": 0.09615384615384616,
      "grad_norm": 4.679502643742891,
      "learning_rate": 1.25e-05,
      "loss": 11.0695,
      "step": 20
    },
    {
      "epoch": 0.10096153846153846,
      "grad_norm": 4.455018506616647,
      "learning_rate": 1.3125e-05,
      "loss": 11.3893,
      "step": 21
    },
    {
      "epoch": 0.10576923076923077,
      "grad_norm": 4.96435798654656,
      "learning_rate": 1.375e-05,
      "loss": 10.8234,
      "step": 22
    },
    {
      "epoch": 0.11057692307692307,
      "grad_norm": 5.007120390763096,
      "learning_rate": 1.4375e-05,
      "loss": 11.0161,
      "step": 23
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 5.542783875072987,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 11.2095,
      "step": 24
    },
    {
      "epoch": 0.1201923076923077,
      "grad_norm": 5.646112663718677,
      "learning_rate": 1.5625e-05,
      "loss": 11.0138,
      "step": 25
    },
    {
      "epoch": 0.125,
      "grad_norm": 6.047251721418737,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 10.7416,
      "step": 26
    },
    {
      "epoch": 0.12980769230769232,
      "grad_norm": 6.784208334771576,
      "learning_rate": 1.6875e-05,
      "loss": 10.3242,
      "step": 27
    },
    {
      "epoch": 0.1346153846153846,
      "grad_norm": 6.511065541996067,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 10.5728,
      "step": 28
    },
    {
      "epoch": 0.13942307692307693,
      "grad_norm": 7.263790603442038,
      "learning_rate": 1.8125e-05,
      "loss": 10.9094,
      "step": 29
    },
    {
      "epoch": 0.14423076923076922,
      "grad_norm": 7.91204811237733,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 10.3671,
      "step": 30
    },
    {
      "epoch": 0.14903846153846154,
      "grad_norm": 7.52663579988435,
      "learning_rate": 1.9375e-05,
      "loss": 10.4343,
      "step": 31
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 8.263133470021284,
      "learning_rate": 2e-05,
      "loss": 9.9155,
      "step": 32
    },
    {
      "epoch": 0.15865384615384615,
      "grad_norm": 8.42661327518277,
      "learning_rate": 1.9999951432210905e-05,
      "loss": 9.3852,
      "step": 33
    },
    {
      "epoch": 0.16346153846153846,
      "grad_norm": 8.406404648928898,
      "learning_rate": 1.9999805729315383e-05,
      "loss": 9.8921,
      "step": 34
    },
    {
      "epoch": 0.16826923076923078,
      "grad_norm": 10.207941869026877,
      "learning_rate": 1.999956289272873e-05,
      "loss": 8.7434,
      "step": 35
    },
    {
      "epoch": 0.17307692307692307,
      "grad_norm": 9.891557091086451,
      "learning_rate": 1.999922292480975e-05,
      "loss": 8.5715,
      "step": 36
    },
    {
      "epoch": 0.1778846153846154,
      "grad_norm": 10.448321838156929,
      "learning_rate": 1.9998785828860744e-05,
      "loss": 8.4007,
      "step": 37
    },
    {
      "epoch": 0.18269230769230768,
      "grad_norm": 10.927824759757488,
      "learning_rate": 1.9998251609127465e-05,
      "loss": 7.6377,
      "step": 38
    },
    {
      "epoch": 0.1875,
      "grad_norm": 11.394612213195861,
      "learning_rate": 1.999762027079909e-05,
      "loss": 6.826,
      "step": 39
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 11.796117011399204,
      "learning_rate": 1.9996891820008165e-05,
      "loss": 6.0708,
      "step": 40
    },
    {
      "epoch": 0.1971153846153846,
      "grad_norm": 11.974179218540808,
      "learning_rate": 1.9996066263830533e-05,
      "loss": 5.9311,
      "step": 41
    },
    {
      "epoch": 0.20192307692307693,
      "grad_norm": 12.579936065238417,
      "learning_rate": 1.9995143610285275e-05,
      "loss": 5.0329,
      "step": 42
    },
    {
      "epoch": 0.20673076923076922,
      "grad_norm": 12.86657538869355,
      "learning_rate": 1.9994123868334655e-05,
      "loss": 4.9112,
      "step": 43
    },
    {
      "epoch": 0.21153846153846154,
      "grad_norm": 11.75017842417206,
      "learning_rate": 1.9993007047883988e-05,
      "loss": 4.2767,
      "step": 44
    },
    {
      "epoch": 0.21634615384615385,
      "grad_norm": 12.051509370432385,
      "learning_rate": 1.999179315978157e-05,
      "loss": 3.4825,
      "step": 45
    },
    {
      "epoch": 0.22115384615384615,
      "grad_norm": 11.644161890544748,
      "learning_rate": 1.999048221581858e-05,
      "loss": 2.5849,
      "step": 46
    },
    {
      "epoch": 0.22596153846153846,
      "grad_norm": 10.673386748757132,
      "learning_rate": 1.9989074228728942e-05,
      "loss": 2.0189,
      "step": 47
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 9.954893231681044,
      "learning_rate": 1.9987569212189224e-05,
      "loss": 1.7453,
      "step": 48
    },
    {
      "epoch": 0.23557692307692307,
      "grad_norm": 9.808547654065798,
      "learning_rate": 1.9985967180818493e-05,
      "loss": 1.3449,
      "step": 49
    },
    {
      "epoch": 0.2403846153846154,
      "grad_norm": 8.129808084135677,
      "learning_rate": 1.998426815017817e-05,
      "loss": 1.1229,
      "step": 50
    },
    {
      "epoch": 0.24519230769230768,
      "grad_norm": 6.685951175373984,
      "learning_rate": 1.998247213677188e-05,
      "loss": 0.9557,
      "step": 51
    },
    {
      "epoch": 0.25,
      "grad_norm": 5.7369032150298365,
      "learning_rate": 1.9980579158045322e-05,
      "loss": 0.7131,
      "step": 52
    },
    {
      "epoch": 0.2548076923076923,
      "grad_norm": 2.90418989883748,
      "learning_rate": 1.9978589232386036e-05,
      "loss": 0.3078,
      "step": 53
    },
    {
      "epoch": 0.25961538461538464,
      "grad_norm": 2.0101888973511346,
      "learning_rate": 1.997650237912329e-05,
      "loss": 0.2665,
      "step": 54
    },
    {
      "epoch": 0.2644230769230769,
      "grad_norm": 1.2708888192919614,
      "learning_rate": 1.997431861852785e-05,
      "loss": 0.2245,
      "step": 55
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 0.8524286416734762,
      "learning_rate": 1.9972037971811802e-05,
      "loss": 0.1908,
      "step": 56
    },
    {
      "epoch": 0.27403846153846156,
      "grad_norm": 1.0576157328342186,
      "learning_rate": 1.996966046112834e-05,
      "loss": 0.2322,
      "step": 57
    },
    {
      "epoch": 0.27884615384615385,
      "grad_norm": 0.616703315869907,
      "learning_rate": 1.996718610957155e-05,
      "loss": 0.167,
      "step": 58
    },
    {
      "epoch": 0.28365384615384615,
      "grad_norm": 0.5962319237929377,
      "learning_rate": 1.9964614941176194e-05,
      "loss": 0.1336,
      "step": 59
    },
    {
      "epoch": 0.28846153846153844,
      "grad_norm": 0.9168867133396781,
      "learning_rate": 1.9961946980917457e-05,
      "loss": 0.2158,
      "step": 60
    },
    {
      "epoch": 0.2932692307692308,
      "grad_norm": 1.264076053799961,
      "learning_rate": 1.995918225471073e-05,
      "loss": 0.2141,
      "step": 61
    },
    {
      "epoch": 0.2980769230769231,
      "grad_norm": 0.5852279529312575,
      "learning_rate": 1.9956320789411338e-05,
      "loss": 0.1302,
      "step": 62
    },
    {
      "epoch": 0.30288461538461536,
      "grad_norm": 0.553405635739514,
      "learning_rate": 1.9953362612814294e-05,
      "loss": 0.1449,
      "step": 63
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 1.3819161059129617,
      "learning_rate": 1.9950307753654016e-05,
      "loss": 0.1532,
      "step": 64
    },
    {
      "epoch": 0.3125,
      "grad_norm": 0.4444786158024725,
      "learning_rate": 1.994715624160405e-05,
      "loss": 0.1058,
      "step": 65
    },
    {
      "epoch": 0.3173076923076923,
      "grad_norm": 1.8733404406662901,
      "learning_rate": 1.99439081072768e-05,
      "loss": 0.1523,
      "step": 66
    },
    {
      "epoch": 0.32211538461538464,
      "grad_norm": 1.2055772833736411,
      "learning_rate": 1.9940563382223196e-05,
      "loss": 0.2734,
      "step": 67
    },
    {
      "epoch": 0.3269230769230769,
      "grad_norm": 0.648940303660776,
      "learning_rate": 1.9937122098932428e-05,
      "loss": 0.1024,
      "step": 68
    },
    {
      "epoch": 0.3317307692307692,
      "grad_norm": 0.6418312844287622,
      "learning_rate": 1.9933584290831593e-05,
      "loss": 0.0942,
      "step": 69
    },
    {
      "epoch": 0.33653846153846156,
      "grad_norm": 0.9679873446626074,
      "learning_rate": 1.9929949992285397e-05,
      "loss": 0.1016,
      "step": 70
    },
    {
      "epoch": 0.34134615384615385,
      "grad_norm": 0.7096644714594711,
      "learning_rate": 1.992621923859581e-05,
      "loss": 0.0996,
      "step": 71
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 0.4202646111621219,
      "learning_rate": 1.9922392066001724e-05,
      "loss": 0.1164,
      "step": 72
    },
    {
      "epoch": 0.35096153846153844,
      "grad_norm": 0.5916961859558945,
      "learning_rate": 1.99184685116786e-05,
      "loss": 0.0984,
      "step": 73
    },
    {
      "epoch": 0.3557692307692308,
      "grad_norm": 0.6094216289179475,
      "learning_rate": 1.9914448613738107e-05,
      "loss": 0.1237,
      "step": 74
    },
    {
      "epoch": 0.3605769230769231,
      "grad_norm": 0.410338399320306,
      "learning_rate": 1.991033241122776e-05,
      "loss": 0.107,
      "step": 75
    },
    {
      "epoch": 0.36538461538461536,
      "grad_norm": 0.6069858777975874,
      "learning_rate": 1.9906119944130527e-05,
      "loss": 0.1061,
      "step": 76
    },
    {
      "epoch": 0.3701923076923077,
      "grad_norm": 0.9717383292150138,
      "learning_rate": 1.9901811253364458e-05,
      "loss": 0.1321,
      "step": 77
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.86045813429993,
      "learning_rate": 1.9897406380782262e-05,
      "loss": 0.085,
      "step": 78
    },
    {
      "epoch": 0.3798076923076923,
      "grad_norm": 0.9001835949148579,
      "learning_rate": 1.989290536917093e-05,
      "loss": 0.1248,
      "step": 79
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 0.744115724419877,
      "learning_rate": 1.9888308262251286e-05,
      "loss": 0.1076,
      "step": 80
    },
    {
      "epoch": 0.3894230769230769,
      "grad_norm": 0.4142118842634531,
      "learning_rate": 1.988361510467761e-05,
      "loss": 0.1327,
      "step": 81
    },
    {
      "epoch": 0.3942307692307692,
      "grad_norm": 0.9166225815760708,
      "learning_rate": 1.9878825942037147e-05,
      "loss": 0.1945,
      "step": 82
    },
    {
      "epoch": 0.39903846153846156,
      "grad_norm": 0.6054230412525934,
      "learning_rate": 1.9873940820849714e-05,
      "loss": 0.1008,
      "step": 83
    },
    {
      "epoch": 0.40384615384615385,
      "grad_norm": 0.460206706680963,
      "learning_rate": 1.9868959788567213e-05,
      "loss": 0.1226,
      "step": 84
    },
    {
      "epoch": 0.40865384615384615,
      "grad_norm": 0.788932772968083,
      "learning_rate": 1.9863882893573188e-05,
      "loss": 0.1083,
      "step": 85
    },
    {
      "epoch": 0.41346153846153844,
      "grad_norm": 0.3019109261695885,
      "learning_rate": 1.985871018518236e-05,
      "loss": 0.0887,
      "step": 86
    },
    {
      "epoch": 0.4182692307692308,
      "grad_norm": 0.31709989842571784,
      "learning_rate": 1.9853441713640123e-05,
      "loss": 0.0933,
      "step": 87
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 0.4201804359573233,
      "learning_rate": 1.9848077530122083e-05,
      "loss": 0.1532,
      "step": 88
    },
    {
      "epoch": 0.42788461538461536,
      "grad_norm": 0.18072656229547063,
      "learning_rate": 1.9842617686733546e-05,
      "loss": 0.0876,
      "step": 89
    },
    {
      "epoch": 0.4326923076923077,
      "grad_norm": 0.1970601605196324,
      "learning_rate": 1.9837062236509013e-05,
      "loss": 0.097,
      "step": 90
    },
    {
      "epoch": 0.4375,
      "grad_norm": 0.38552799763992274,
      "learning_rate": 1.983141123341168e-05,
      "loss": 0.1207,
      "step": 91
    },
    {
      "epoch": 0.4423076923076923,
      "grad_norm": 0.22753263323246137,
      "learning_rate": 1.9825664732332886e-05,
      "loss": 0.1193,
      "step": 92
    },
    {
      "epoch": 0.44711538461538464,
      "grad_norm": 0.30426300684230684,
      "learning_rate": 1.9819822789091597e-05,
      "loss": 0.0901,
      "step": 93
    },
    {
      "epoch": 0.4519230769230769,
      "grad_norm": 0.28462489108210615,
      "learning_rate": 1.981388546043388e-05,
      "loss": 0.0661,
      "step": 94
    },
    {
      "epoch": 0.4567307692307692,
      "grad_norm": 0.23167386371584883,
      "learning_rate": 1.9807852804032306e-05,
      "loss": 0.1016,
      "step": 95
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 0.20522904210056322,
      "learning_rate": 1.9801724878485438e-05,
      "loss": 0.0762,
      "step": 96
    },
    {
      "epoch": 0.46634615384615385,
      "grad_norm": 0.1529892762236927,
      "learning_rate": 1.979550174331724e-05,
      "loss": 0.0994,
      "step": 97
    },
    {
      "epoch": 0.47115384615384615,
      "grad_norm": 0.2711535014649242,
      "learning_rate": 1.9789183458976485e-05,
      "loss": 0.0886,
      "step": 98
    },
    {
      "epoch": 0.47596153846153844,
      "grad_norm": 0.17281859795040574,
      "learning_rate": 1.97827700868362e-05,
      "loss": 0.1057,
      "step": 99
    },
    {
      "epoch": 0.4807692307692308,
      "grad_norm": 0.13497257519276804,
      "learning_rate": 1.977626168919305e-05,
      "loss": 0.0828,
      "step": 100
    },
    {
      "epoch": 0.4855769230769231,
      "grad_norm": 0.1452951333381849,
      "learning_rate": 1.9769658329266718e-05,
      "loss": 0.0817,
      "step": 101
    },
    {
      "epoch": 0.49038461538461536,
      "grad_norm": 0.4727535394667618,
      "learning_rate": 1.9762960071199334e-05,
      "loss": 0.0942,
      "step": 102
    },
    {
      "epoch": 0.4951923076923077,
      "grad_norm": 0.5427515880294354,
      "learning_rate": 1.9756166980054812e-05,
      "loss": 0.1462,
      "step": 103
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.15222491540197328,
      "learning_rate": 1.9749279121818235e-05,
      "loss": 0.0888,
      "step": 104
    },
    {
      "epoch": 0.5048076923076923,
      "grad_norm": 0.14654940033251662,
      "learning_rate": 1.9742296563395218e-05,
      "loss": 0.0671,
      "step": 105
    },
    {
      "epoch": 0.5096153846153846,
      "grad_norm": 0.12490327104306094,
      "learning_rate": 1.9735219372611232e-05,
      "loss": 0.0822,
      "step": 106
    },
    {
      "epoch": 0.5144230769230769,
      "grad_norm": 0.5041368639949741,
      "learning_rate": 1.9728047618210995e-05,
      "loss": 0.1884,
      "step": 107
    },
    {
      "epoch": 0.5192307692307693,
      "grad_norm": 0.17256981303256924,
      "learning_rate": 1.9720781369857747e-05,
      "loss": 0.08,
      "step": 108
    },
    {
      "epoch": 0.5240384615384616,
      "grad_norm": 0.16065337850736802,
      "learning_rate": 1.9713420698132614e-05,
      "loss": 0.1053,
      "step": 109
    },
    {
      "epoch": 0.5288461538461539,
      "grad_norm": 0.2952310737082293,
      "learning_rate": 1.970596567453391e-05,
      "loss": 0.1009,
      "step": 110
    },
    {
      "epoch": 0.5336538461538461,
      "grad_norm": 0.16542154649744106,
      "learning_rate": 1.9698416371476434e-05,
      "loss": 0.071,
      "step": 111
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 0.13295728577634064,
      "learning_rate": 1.969077286229078e-05,
      "loss": 0.0864,
      "step": 112
    },
    {
      "epoch": 0.5432692307692307,
      "grad_norm": 0.2447869305017937,
      "learning_rate": 1.9683035221222617e-05,
      "loss": 0.0754,
      "step": 113
    },
    {
      "epoch": 0.5480769230769231,
      "grad_norm": 0.17032760655242107,
      "learning_rate": 1.9675203523431964e-05,
      "loss": 0.1055,
      "step": 114
    },
    {
      "epoch": 0.5528846153846154,
      "grad_norm": 0.18309583688130115,
      "learning_rate": 1.9667277844992476e-05,
      "loss": 0.0774,
      "step": 115
    },
    {
      "epoch": 0.5576923076923077,
      "grad_norm": 0.15617984497729698,
      "learning_rate": 1.9659258262890683e-05,
      "loss": 0.0908,
      "step": 116
    },
    {
      "epoch": 0.5625,
      "grad_norm": 0.21112490304566195,
      "learning_rate": 1.9651144855025265e-05,
      "loss": 0.068,
      "step": 117
    },
    {
      "epoch": 0.5673076923076923,
      "grad_norm": 0.11706571985527736,
      "learning_rate": 1.964293770020628e-05,
      "loss": 0.1012,
      "step": 118
    },
    {
      "epoch": 0.5721153846153846,
      "grad_norm": 0.18457107297609474,
      "learning_rate": 1.9634636878154393e-05,
      "loss": 0.1021,
      "step": 119
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 0.1407983425084999,
      "learning_rate": 1.962624246950012e-05,
      "loss": 0.0782,
      "step": 120
    },
    {
      "epoch": 0.5817307692307693,
      "grad_norm": 0.1294899382859157,
      "learning_rate": 1.9617754555783045e-05,
      "loss": 0.0837,
      "step": 121
    },
    {
      "epoch": 0.5865384615384616,
      "grad_norm": 0.2956020806173405,
      "learning_rate": 1.9609173219450998e-05,
      "loss": 0.1015,
      "step": 122
    },
    {
      "epoch": 0.5913461538461539,
      "grad_norm": 0.2875666263216629,
      "learning_rate": 1.960049854385929e-05,
      "loss": 0.0874,
      "step": 123
    },
    {
      "epoch": 0.5961538461538461,
      "grad_norm": 0.38096322344195555,
      "learning_rate": 1.9591730613269878e-05,
      "loss": 0.1014,
      "step": 124
    },
    {
      "epoch": 0.6009615384615384,
      "grad_norm": 0.17337610269368336,
      "learning_rate": 1.9582869512850576e-05,
      "loss": 0.087,
      "step": 125
    },
    {
      "epoch": 0.6057692307692307,
      "grad_norm": 0.12260465350936786,
      "learning_rate": 1.957391532867418e-05,
      "loss": 0.0619,
      "step": 126
    },
    {
      "epoch": 0.6105769230769231,
      "grad_norm": 0.373691529053846,
      "learning_rate": 1.956486814771769e-05,
      "loss": 0.0928,
      "step": 127
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.2699401269013864,
      "learning_rate": 1.955572805786141e-05,
      "loss": 0.0995,
      "step": 128
    },
    {
      "epoch": 0.6201923076923077,
      "grad_norm": 0.3061264002433714,
      "learning_rate": 1.9546495147888134e-05,
      "loss": 0.1034,
      "step": 129
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.488196979420658,
      "learning_rate": 1.953716950748227e-05,
      "loss": 0.1227,
      "step": 130
    },
    {
      "epoch": 0.6298076923076923,
      "grad_norm": 0.15777296998716,
      "learning_rate": 1.9527751227228964e-05,
      "loss": 0.0733,
      "step": 131
    },
    {
      "epoch": 0.6346153846153846,
      "grad_norm": 0.23176896592310964,
      "learning_rate": 1.9518240398613226e-05,
      "loss": 0.0857,
      "step": 132
    },
    {
      "epoch": 0.6394230769230769,
      "grad_norm": 0.1605539042118438,
      "learning_rate": 1.9508637114019037e-05,
      "loss": 0.0625,
      "step": 133
    },
    {
      "epoch": 0.6442307692307693,
      "grad_norm": 0.10687308050515391,
      "learning_rate": 1.9498941466728462e-05,
      "loss": 0.0639,
      "step": 134
    },
    {
      "epoch": 0.6490384615384616,
      "grad_norm": 0.17832141398602266,
      "learning_rate": 1.9489153550920726e-05,
      "loss": 0.0966,
      "step": 135
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 0.12807248026642828,
      "learning_rate": 1.947927346167132e-05,
      "loss": 0.0874,
      "step": 136
    },
    {
      "epoch": 0.6586538461538461,
      "grad_norm": 1.004978462724531,
      "learning_rate": 1.946930129495106e-05,
      "loss": 0.2415,
      "step": 137
    },
    {
      "epoch": 0.6634615384615384,
      "grad_norm": 0.5217887566491667,
      "learning_rate": 1.945923714762516e-05,
      "loss": 0.0851,
      "step": 138
    },
    {
      "epoch": 0.6682692307692307,
      "grad_norm": 0.13770659134575503,
      "learning_rate": 1.9449081117452304e-05,
      "loss": 0.0814,
      "step": 139
    },
    {
      "epoch": 0.6730769230769231,
      "grad_norm": 0.3064377001224982,
      "learning_rate": 1.9438833303083677e-05,
      "loss": 0.1138,
      "step": 140
    },
    {
      "epoch": 0.6778846153846154,
      "grad_norm": 0.13282528296493804,
      "learning_rate": 1.9428493804062013e-05,
      "loss": 0.0663,
      "step": 141
    },
    {
      "epoch": 0.6826923076923077,
      "grad_norm": 0.16668578051923236,
      "learning_rate": 1.9418062720820636e-05,
      "loss": 0.1023,
      "step": 142
    },
    {
      "epoch": 0.6875,
      "grad_norm": 0.3402339060908556,
      "learning_rate": 1.9407540154682473e-05,
      "loss": 0.0694,
      "step": 143
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 0.13719490700283696,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 0.0589,
      "step": 144
    },
    {
      "epoch": 0.6971153846153846,
      "grad_norm": 0.11777357768257533,
      "learning_rate": 1.9386220983449652e-05,
      "loss": 0.0652,
      "step": 145
    },
    {
      "epoch": 0.7019230769230769,
      "grad_norm": 0.22285236004701664,
      "learning_rate": 1.9375424585439994e-05,
      "loss": 0.1097,
      "step": 146
    },
    {
      "epoch": 0.7067307692307693,
      "grad_norm": 0.11208718091123993,
      "learning_rate": 1.9364537118701542e-05,
      "loss": 0.0819,
      "step": 147
    },
    {
      "epoch": 0.7115384615384616,
      "grad_norm": 0.14611795428386373,
      "learning_rate": 1.935355868899034e-05,
      "loss": 0.0936,
      "step": 148
    },
    {
      "epoch": 0.7163461538461539,
      "grad_norm": 0.23494135019990098,
      "learning_rate": 1.9342489402945997e-05,
      "loss": 0.0954,
      "step": 149
    },
    {
      "epoch": 0.7211538461538461,
      "grad_norm": 0.14000988388659807,
      "learning_rate": 1.9331329368090664e-05,
      "loss": 0.075,
      "step": 150
    },
    {
      "epoch": 0.7259615384615384,
      "grad_norm": 0.09089792582712386,
      "learning_rate": 1.932007869282799e-05,
      "loss": 0.0516,
      "step": 151
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 0.26884128546264124,
      "learning_rate": 1.9308737486442045e-05,
      "loss": 0.0793,
      "step": 152
    },
    {
      "epoch": 0.7355769230769231,
      "grad_norm": 0.17332650729605312,
      "learning_rate": 1.9297305859096305e-05,
      "loss": 0.0931,
      "step": 153
    },
    {
      "epoch": 0.7403846153846154,
      "grad_norm": 0.31317367252589295,
      "learning_rate": 1.9285783921832537e-05,
      "loss": 0.0829,
      "step": 154
    },
    {
      "epoch": 0.7451923076923077,
      "grad_norm": 0.11561514077851837,
      "learning_rate": 1.927417178656975e-05,
      "loss": 0.0562,
      "step": 155
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.38849208376158995,
      "learning_rate": 1.926246956610309e-05,
      "loss": 0.101,
      "step": 156
    },
    {
      "epoch": 0.7548076923076923,
      "grad_norm": 0.14099000092546168,
      "learning_rate": 1.9250677374102752e-05,
      "loss": 0.0704,
      "step": 157
    },
    {
      "epoch": 0.7596153846153846,
      "grad_norm": 0.1150187713844569,
      "learning_rate": 1.9238795325112867e-05,
      "loss": 0.0703,
      "step": 158
    },
    {
      "epoch": 0.7644230769230769,
      "grad_norm": 0.0976832382254209,
      "learning_rate": 1.9226823534550418e-05,
      "loss": 0.056,
      "step": 159
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 0.16489594728944734,
      "learning_rate": 1.921476211870408e-05,
      "loss": 0.0868,
      "step": 160
    },
    {
      "epoch": 0.7740384615384616,
      "grad_norm": 0.26741949027804574,
      "learning_rate": 1.9202611194733107e-05,
      "loss": 0.1208,
      "step": 161
    },
    {
      "epoch": 0.7788461538461539,
      "grad_norm": 0.11057330903383285,
      "learning_rate": 1.9190370880666206e-05,
      "loss": 0.0642,
      "step": 162
    },
    {
      "epoch": 0.7836538461538461,
      "grad_norm": 0.15686620933921425,
      "learning_rate": 1.9178041295400383e-05,
      "loss": 0.0765,
      "step": 163
    },
    {
      "epoch": 0.7884615384615384,
      "grad_norm": 0.22477822358029917,
      "learning_rate": 1.9165622558699763e-05,
      "loss": 0.098,
      "step": 164
    },
    {
      "epoch": 0.7932692307692307,
      "grad_norm": 0.11075338638492375,
      "learning_rate": 1.9153114791194475e-05,
      "loss": 0.079,
      "step": 165
    },
    {
      "epoch": 0.7980769230769231,
      "grad_norm": 0.20914673424047697,
      "learning_rate": 1.9140518114379433e-05,
      "loss": 0.092,
      "step": 166
    },
    {
      "epoch": 0.8028846153846154,
      "grad_norm": 0.24054790587421168,
      "learning_rate": 1.912783265061319e-05,
      "loss": 0.0757,
      "step": 167
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 0.14483617422357292,
      "learning_rate": 1.9115058523116734e-05,
      "loss": 0.0526,
      "step": 168
    },
    {
      "epoch": 0.8125,
      "grad_norm": 0.09291074823317302,
      "learning_rate": 1.9102195855972287e-05,
      "loss": 0.0573,
      "step": 169
    },
    {
      "epoch": 0.8173076923076923,
      "grad_norm": 0.10925013754811734,
      "learning_rate": 1.908924477412211e-05,
      "loss": 0.0526,
      "step": 170
    },
    {
      "epoch": 0.8221153846153846,
      "grad_norm": 0.7708107505269884,
      "learning_rate": 1.9076205403367287e-05,
      "loss": 0.1171,
      "step": 171
    },
    {
      "epoch": 0.8269230769230769,
      "grad_norm": 0.14733376566268724,
      "learning_rate": 1.9063077870366504e-05,
      "loss": 0.0882,
      "step": 172
    },
    {
      "epoch": 0.8317307692307693,
      "grad_norm": 0.27733578284422816,
      "learning_rate": 1.90498623026348e-05,
      "loss": 0.0524,
      "step": 173
    },
    {
      "epoch": 0.8365384615384616,
      "grad_norm": 0.14577691995852646,
      "learning_rate": 1.903655882854237e-05,
      "loss": 0.082,
      "step": 174
    },
    {
      "epoch": 0.8413461538461539,
      "grad_norm": 0.1084886025016621,
      "learning_rate": 1.9023167577313267e-05,
      "loss": 0.0747,
      "step": 175
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 0.32289094001477414,
      "learning_rate": 1.900968867902419e-05,
      "loss": 0.0955,
      "step": 176
    },
    {
      "epoch": 0.8509615384615384,
      "grad_norm": 0.11007673090369277,
      "learning_rate": 1.8996122264603202e-05,
      "loss": 0.0596,
      "step": 177
    },
    {
      "epoch": 0.8557692307692307,
      "grad_norm": 0.1099629156799583,
      "learning_rate": 1.898246846582844e-05,
      "loss": 0.0518,
      "step": 178
    },
    {
      "epoch": 0.8605769230769231,
      "grad_norm": 0.1509568160143062,
      "learning_rate": 1.8968727415326885e-05,
      "loss": 0.0855,
      "step": 179
    },
    {
      "epoch": 0.8653846153846154,
      "grad_norm": 0.19104508288312103,
      "learning_rate": 1.895489924657301e-05,
      "loss": 0.0765,
      "step": 180
    },
    {
      "epoch": 0.8701923076923077,
      "grad_norm": 0.12699456261738035,
      "learning_rate": 1.894098409388754e-05,
      "loss": 0.085,
      "step": 181
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.1765818125430728,
      "learning_rate": 1.8926982092436117e-05,
      "loss": 0.0842,
      "step": 182
    },
    {
      "epoch": 0.8798076923076923,
      "grad_norm": 0.11938691784271126,
      "learning_rate": 1.8912893378227984e-05,
      "loss": 0.0675,
      "step": 183
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 0.15639868000749302,
      "learning_rate": 1.8898718088114688e-05,
      "loss": 0.0823,
      "step": 184
    },
    {
      "epoch": 0.8894230769230769,
      "grad_norm": 0.10780181753894158,
      "learning_rate": 1.8884456359788725e-05,
      "loss": 0.0767,
      "step": 185
    },
    {
      "epoch": 0.8942307692307693,
      "grad_norm": 0.1155378368652306,
      "learning_rate": 1.887010833178222e-05,
      "loss": 0.0597,
      "step": 186
    },
    {
      "epoch": 0.8990384615384616,
      "grad_norm": 0.13354557542592715,
      "learning_rate": 1.8855674143465567e-05,
      "loss": 0.074,
      "step": 187
    },
    {
      "epoch": 0.9038461538461539,
      "grad_norm": 0.19706167431060925,
      "learning_rate": 1.8841153935046098e-05,
      "loss": 0.0749,
      "step": 188
    },
    {
      "epoch": 0.9086538461538461,
      "grad_norm": 0.09826543706439073,
      "learning_rate": 1.8826547847566692e-05,
      "loss": 0.0491,
      "step": 189
    },
    {
      "epoch": 0.9134615384615384,
      "grad_norm": 0.12388515997310187,
      "learning_rate": 1.8811856022904423e-05,
      "loss": 0.0727,
      "step": 190
    },
    {
      "epoch": 0.9182692307692307,
      "grad_norm": 0.6378039504454973,
      "learning_rate": 1.8797078603769184e-05,
      "loss": 0.1041,
      "step": 191
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.09682576482982055,
      "learning_rate": 1.8782215733702286e-05,
      "loss": 0.0667,
      "step": 192
    },
    {
      "epoch": 0.9278846153846154,
      "grad_norm": 0.10471019258814095,
      "learning_rate": 1.876726755707508e-05,
      "loss": 0.0607,
      "step": 193
    },
    {
      "epoch": 0.9326923076923077,
      "grad_norm": 0.119794225299334,
      "learning_rate": 1.8752234219087538e-05,
      "loss": 0.0811,
      "step": 194
    },
    {
      "epoch": 0.9375,
      "grad_norm": 0.13619902544846568,
      "learning_rate": 1.8737115865766865e-05,
      "loss": 0.0843,
      "step": 195
    },
    {
      "epoch": 0.9423076923076923,
      "grad_norm": 0.09469804215409384,
      "learning_rate": 1.8721912643966055e-05,
      "loss": 0.0528,
      "step": 196
    },
    {
      "epoch": 0.9471153846153846,
      "grad_norm": 0.0867052665843276,
      "learning_rate": 1.8706624701362485e-05,
      "loss": 0.0457,
      "step": 197
    },
    {
      "epoch": 0.9519230769230769,
      "grad_norm": 0.07641311746721899,
      "learning_rate": 1.8691252186456465e-05,
      "loss": 0.0509,
      "step": 198
    },
    {
      "epoch": 0.9567307692307693,
      "grad_norm": 0.3143946887620458,
      "learning_rate": 1.8675795248569816e-05,
      "loss": 0.0739,
      "step": 199
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 0.7297986150328197,
      "learning_rate": 1.866025403784439e-05,
      "loss": 0.1551,
      "step": 200
    },
    {
      "epoch": 0.9663461538461539,
      "grad_norm": 0.19810635999539816,
      "learning_rate": 1.8644628705240636e-05,
      "loss": 0.0656,
      "step": 201
    },
    {
      "epoch": 0.9711538461538461,
      "grad_norm": 0.14218637226502015,
      "learning_rate": 1.862891940253613e-05,
      "loss": 0.0776,
      "step": 202
    },
    {
      "epoch": 0.9759615384615384,
      "grad_norm": 0.16215548902718732,
      "learning_rate": 1.8613126282324092e-05,
      "loss": 0.0692,
      "step": 203
    },
    {
      "epoch": 0.9807692307692307,
      "grad_norm": 0.1298697715545982,
      "learning_rate": 1.8597249498011906e-05,
      "loss": 0.0547,
      "step": 204
    },
    {
      "epoch": 0.9855769230769231,
      "grad_norm": 0.2511303502205514,
      "learning_rate": 1.858128920381963e-05,
      "loss": 0.0657,
      "step": 205
    },
    {
      "epoch": 0.9903846153846154,
      "grad_norm": 0.16832380593779284,
      "learning_rate": 1.8565245554778516e-05,
      "loss": 0.0578,
      "step": 206
    },
    {
      "epoch": 0.9951923076923077,
      "grad_norm": 0.19374977274923516,
      "learning_rate": 1.854911870672947e-05,
      "loss": 0.0693,
      "step": 207
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.08607978977391051,
      "learning_rate": 1.8532908816321557e-05,
      "loss": 0.0582,
      "step": 208
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.06950412690639496,
      "eval_runtime": 654.4198,
      "eval_samples_per_second": 0.636,
      "eval_steps_per_second": 0.636,
      "step": 208
    },
    {
      "epoch": 1.0048076923076923,
      "grad_norm": 0.19735700761964212,
      "learning_rate": 1.8516616041010495e-05,
      "loss": 0.0714,
      "step": 209
    },
    {
      "epoch": 1.0096153846153846,
      "grad_norm": 0.11291056207111348,
      "learning_rate": 1.8500240539057093e-05,
      "loss": 0.0753,
      "step": 210
    },
    {
      "epoch": 1.0144230769230769,
      "grad_norm": 0.09419652060146232,
      "learning_rate": 1.848378246952574e-05,
      "loss": 0.0695,
      "step": 211
    },
    {
      "epoch": 1.0192307692307692,
      "grad_norm": 0.1306122403028074,
      "learning_rate": 1.8467241992282842e-05,
      "loss": 0.0629,
      "step": 212
    },
    {
      "epoch": 1.0240384615384615,
      "grad_norm": 0.09325967848526182,
      "learning_rate": 1.8450619267995283e-05,
      "loss": 0.0608,
      "step": 213
    },
    {
      "epoch": 1.0288461538461537,
      "grad_norm": 0.20979207509572104,
      "learning_rate": 1.843391445812886e-05,
      "loss": 0.072,
      "step": 214
    },
    {
      "epoch": 1.0336538461538463,
      "grad_norm": 0.10020786552634017,
      "learning_rate": 1.84171277249467e-05,
      "loss": 0.0706,
      "step": 215
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 0.1167427597173162,
      "learning_rate": 1.8400259231507716e-05,
      "loss": 0.0752,
      "step": 216
    },
    {
      "epoch": 1.0432692307692308,
      "grad_norm": 0.10556631834814689,
      "learning_rate": 1.8383309141664992e-05,
      "loss": 0.0627,
      "step": 217
    },
    {
      "epoch": 1.0480769230769231,
      "grad_norm": 0.07461606613536971,
      "learning_rate": 1.83662776200642e-05,
      "loss": 0.0446,
      "step": 218
    },
    {
      "epoch": 1.0528846153846154,
      "grad_norm": 0.1925103602980762,
      "learning_rate": 1.8349164832142015e-05,
      "loss": 0.0678,
      "step": 219
    },
    {
      "epoch": 1.0576923076923077,
      "grad_norm": 0.12458491352701737,
      "learning_rate": 1.833197094412449e-05,
      "loss": 0.0842,
      "step": 220
    },
    {
      "epoch": 1.0625,
      "grad_norm": 0.1443674157379261,
      "learning_rate": 1.8314696123025456e-05,
      "loss": 0.0813,
      "step": 221
    },
    {
      "epoch": 1.0673076923076923,
      "grad_norm": 0.1414059295998895,
      "learning_rate": 1.8297340536644877e-05,
      "loss": 0.0622,
      "step": 222
    },
    {
      "epoch": 1.0721153846153846,
      "grad_norm": 0.13584048046934732,
      "learning_rate": 1.827990435356725e-05,
      "loss": 0.0817,
      "step": 223
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 0.15732739480471164,
      "learning_rate": 1.826238774315995e-05,
      "loss": 0.0656,
      "step": 224
    },
    {
      "epoch": 1.0817307692307692,
      "grad_norm": 0.12411414049291435,
      "learning_rate": 1.8244790875571582e-05,
      "loss": 0.0729,
      "step": 225
    },
    {
      "epoch": 1.0865384615384615,
      "grad_norm": 0.198337153321623,
      "learning_rate": 1.8227113921730336e-05,
      "loss": 0.0666,
      "step": 226
    },
    {
      "epoch": 1.0913461538461537,
      "grad_norm": 0.18420245096916052,
      "learning_rate": 1.8209357053342325e-05,
      "loss": 0.0778,
      "step": 227
    },
    {
      "epoch": 1.0961538461538463,
      "grad_norm": 0.10415151742530535,
      "learning_rate": 1.819152044288992e-05,
      "loss": 0.0703,
      "step": 228
    },
    {
      "epoch": 1.1009615384615385,
      "grad_norm": 0.18301821016518943,
      "learning_rate": 1.8173604263630066e-05,
      "loss": 0.0685,
      "step": 229
    },
    {
      "epoch": 1.1057692307692308,
      "grad_norm": 0.10909211334351836,
      "learning_rate": 1.8155608689592604e-05,
      "loss": 0.0593,
      "step": 230
    },
    {
      "epoch": 1.1105769230769231,
      "grad_norm": 0.06715119901732607,
      "learning_rate": 1.8137533895578585e-05,
      "loss": 0.0472,
      "step": 231
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 0.41281564320344344,
      "learning_rate": 1.811938005715857e-05,
      "loss": 0.1301,
      "step": 232
    },
    {
      "epoch": 1.1201923076923077,
      "grad_norm": 0.08827599031514025,
      "learning_rate": 1.8101147350670905e-05,
      "loss": 0.0634,
      "step": 233
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.19871065322829223,
      "learning_rate": 1.8082835953220055e-05,
      "loss": 0.0717,
      "step": 234
    },
    {
      "epoch": 1.1298076923076923,
      "grad_norm": 0.11252309877012083,
      "learning_rate": 1.806444604267483e-05,
      "loss": 0.0683,
      "step": 235
    },
    {
      "epoch": 1.1346153846153846,
      "grad_norm": 0.0952326021500478,
      "learning_rate": 1.8045977797666685e-05,
      "loss": 0.0638,
      "step": 236
    },
    {
      "epoch": 1.1394230769230769,
      "grad_norm": 0.20874961867062022,
      "learning_rate": 1.8027431397587993e-05,
      "loss": 0.0699,
      "step": 237
    },
    {
      "epoch": 1.1442307692307692,
      "grad_norm": 0.09796467471628928,
      "learning_rate": 1.8008807022590283e-05,
      "loss": 0.0598,
      "step": 238
    },
    {
      "epoch": 1.1490384615384615,
      "grad_norm": 0.12612255360573366,
      "learning_rate": 1.7990104853582494e-05,
      "loss": 0.0615,
      "step": 239
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 0.2006682541795925,
      "learning_rate": 1.7971325072229227e-05,
      "loss": 0.0603,
      "step": 240
    },
    {
      "epoch": 1.1586538461538463,
      "grad_norm": 0.17340920799270249,
      "learning_rate": 1.7952467860948975e-05,
      "loss": 0.0713,
      "step": 241
    },
    {
      "epoch": 1.1634615384615385,
      "grad_norm": 0.09728762302917576,
      "learning_rate": 1.7933533402912354e-05,
      "loss": 0.0563,
      "step": 242
    },
    {
      "epoch": 1.1682692307692308,
      "grad_norm": 0.20328600607198632,
      "learning_rate": 1.791452188204031e-05,
      "loss": 0.0703,
      "step": 243
    },
    {
      "epoch": 1.1730769230769231,
      "grad_norm": 0.07744403383714431,
      "learning_rate": 1.7895433483002356e-05,
      "loss": 0.0574,
      "step": 244
    },
    {
      "epoch": 1.1778846153846154,
      "grad_norm": 0.08003308834507718,
      "learning_rate": 1.7876268391214756e-05,
      "loss": 0.0634,
      "step": 245
    },
    {
      "epoch": 1.1826923076923077,
      "grad_norm": 0.1254918031678337,
      "learning_rate": 1.785702679283874e-05,
      "loss": 0.0695,
      "step": 246
    },
    {
      "epoch": 1.1875,
      "grad_norm": 0.4090559826951936,
      "learning_rate": 1.7837708874778683e-05,
      "loss": 0.1043,
      "step": 247
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 0.14680145403862374,
      "learning_rate": 1.78183148246803e-05,
      "loss": 0.0562,
      "step": 248
    },
    {
      "epoch": 1.1971153846153846,
      "grad_norm": 0.08038298275411511,
      "learning_rate": 1.7798844830928818e-05,
      "loss": 0.0459,
      "step": 249
    },
    {
      "epoch": 1.2019230769230769,
      "grad_norm": 0.10454130997308889,
      "learning_rate": 1.777929908264715e-05,
      "loss": 0.0725,
      "step": 250
    },
    {
      "epoch": 1.2067307692307692,
      "grad_norm": 0.22302424675427032,
      "learning_rate": 1.775967776969405e-05,
      "loss": 0.0714,
      "step": 251
    },
    {
      "epoch": 1.2115384615384615,
      "grad_norm": 0.06615684679012175,
      "learning_rate": 1.7739981082662275e-05,
      "loss": 0.0404,
      "step": 252
    },
    {
      "epoch": 1.2163461538461537,
      "grad_norm": 0.07085897181279474,
      "learning_rate": 1.772020921287674e-05,
      "loss": 0.0505,
      "step": 253
    },
    {
      "epoch": 1.2211538461538463,
      "grad_norm": 0.12534323816081902,
      "learning_rate": 1.7700362352392632e-05,
      "loss": 0.0646,
      "step": 254
    },
    {
      "epoch": 1.2259615384615385,
      "grad_norm": 0.11341388917833031,
      "learning_rate": 1.7680440693993586e-05,
      "loss": 0.0596,
      "step": 255
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 0.10123959640825818,
      "learning_rate": 1.766044443118978e-05,
      "loss": 0.0695,
      "step": 256
    },
    {
      "epoch": 1.2355769230769231,
      "grad_norm": 0.06328981691635872,
      "learning_rate": 1.7640373758216075e-05,
      "loss": 0.0458,
      "step": 257
    },
    {
      "epoch": 1.2403846153846154,
      "grad_norm": 0.1777143049254178,
      "learning_rate": 1.762022887003011e-05,
      "loss": 0.0712,
      "step": 258
    },
    {
      "epoch": 1.2451923076923077,
      "grad_norm": 0.16624164717288886,
      "learning_rate": 1.7600009962310417e-05,
      "loss": 0.0628,
      "step": 259
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.08768875515032101,
      "learning_rate": 1.757971723145453e-05,
      "loss": 0.044,
      "step": 260
    },
    {
      "epoch": 1.2548076923076923,
      "grad_norm": 0.11319931713601325,
      "learning_rate": 1.7559350874577066e-05,
      "loss": 0.054,
      "step": 261
    },
    {
      "epoch": 1.2596153846153846,
      "grad_norm": 0.11043518345984162,
      "learning_rate": 1.75389110895078e-05,
      "loss": 0.071,
      "step": 262
    },
    {
      "epoch": 1.2644230769230769,
      "grad_norm": 0.09493909979114237,
      "learning_rate": 1.7518398074789776e-05,
      "loss": 0.0584,
      "step": 263
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 0.1635742457455695,
      "learning_rate": 1.7497812029677344e-05,
      "loss": 0.0589,
      "step": 264
    },
    {
      "epoch": 1.2740384615384617,
      "grad_norm": 0.07944066562284541,
      "learning_rate": 1.7477153154134244e-05,
      "loss": 0.0571,
      "step": 265
    },
    {
      "epoch": 1.2788461538461537,
      "grad_norm": 0.11137526457830102,
      "learning_rate": 1.7456421648831658e-05,
      "loss": 0.0664,
      "step": 266
    },
    {
      "epoch": 1.2836538461538463,
      "grad_norm": 0.07797578183224979,
      "learning_rate": 1.743561771514626e-05,
      "loss": 0.0484,
      "step": 267
    },
    {
      "epoch": 1.2884615384615383,
      "grad_norm": 0.18434250558846346,
      "learning_rate": 1.741474155515827e-05,
      "loss": 0.0514,
      "step": 268
    },
    {
      "epoch": 1.2932692307692308,
      "grad_norm": 0.10632717592967073,
      "learning_rate": 1.739379337164946e-05,
      "loss": 0.0569,
      "step": 269
    },
    {
      "epoch": 1.2980769230769231,
      "grad_norm": 0.5572828339364999,
      "learning_rate": 1.737277336810124e-05,
      "loss": 0.1226,
      "step": 270
    },
    {
      "epoch": 1.3028846153846154,
      "grad_norm": 0.09815855797649532,
      "learning_rate": 1.7351681748692622e-05,
      "loss": 0.0678,
      "step": 271
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.07680629590297543,
      "learning_rate": 1.7330518718298263e-05,
      "loss": 0.0523,
      "step": 272
    },
    {
      "epoch": 1.3125,
      "grad_norm": 0.07502527491645014,
      "learning_rate": 1.7309284482486494e-05,
      "loss": 0.0531,
      "step": 273
    },
    {
      "epoch": 1.3173076923076923,
      "grad_norm": 0.2621231399240285,
      "learning_rate": 1.7287979247517285e-05,
      "loss": 0.0743,
      "step": 274
    },
    {
      "epoch": 1.3221153846153846,
      "grad_norm": 0.0671933560970033,
      "learning_rate": 1.7266603220340273e-05,
      "loss": 0.0494,
      "step": 275
    },
    {
      "epoch": 1.3269230769230769,
      "grad_norm": 0.056846353093888205,
      "learning_rate": 1.7245156608592727e-05,
      "loss": 0.0388,
      "step": 276
    },
    {
      "epoch": 1.3317307692307692,
      "grad_norm": 0.09159401439242945,
      "learning_rate": 1.7223639620597556e-05,
      "loss": 0.0512,
      "step": 277
    },
    {
      "epoch": 1.3365384615384617,
      "grad_norm": 0.1009841796453895,
      "learning_rate": 1.7202052465361268e-05,
      "loss": 0.0517,
      "step": 278
    },
    {
      "epoch": 1.3413461538461537,
      "grad_norm": 0.06362283552107784,
      "learning_rate": 1.718039535257194e-05,
      "loss": 0.0559,
      "step": 279
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 0.09301307082315464,
      "learning_rate": 1.7158668492597186e-05,
      "loss": 0.0541,
      "step": 280
    },
    {
      "epoch": 1.3509615384615383,
      "grad_norm": 0.0638527665806663,
      "learning_rate": 1.7136872096482123e-05,
      "loss": 0.0381,
      "step": 281
    },
    {
      "epoch": 1.3557692307692308,
      "grad_norm": 0.1319070543261215,
      "learning_rate": 1.7115006375947304e-05,
      "loss": 0.0516,
      "step": 282
    },
    {
      "epoch": 1.3605769230769231,
      "grad_norm": 0.06961569102682343,
      "learning_rate": 1.7093071543386667e-05,
      "loss": 0.0539,
      "step": 283
    },
    {
      "epoch": 1.3653846153846154,
      "grad_norm": 0.0667907683001732,
      "learning_rate": 1.7071067811865477e-05,
      "loss": 0.0515,
      "step": 284
    },
    {
      "epoch": 1.3701923076923077,
      "grad_norm": 0.07213499453655442,
      "learning_rate": 1.7048995395118253e-05,
      "loss": 0.0492,
      "step": 285
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.1334235342694401,
      "learning_rate": 1.7026854507546694e-05,
      "loss": 0.0675,
      "step": 286
    },
    {
      "epoch": 1.3798076923076923,
      "grad_norm": 0.06258980938990068,
      "learning_rate": 1.7004645364217584e-05,
      "loss": 0.0549,
      "step": 287
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 0.1235385758513032,
      "learning_rate": 1.698236818086073e-05,
      "loss": 0.055,
      "step": 288
    },
    {
      "epoch": 1.3894230769230769,
      "grad_norm": 0.1375456523568836,
      "learning_rate": 1.6960023173866834e-05,
      "loss": 0.0671,
      "step": 289
    },
    {
      "epoch": 1.3942307692307692,
      "grad_norm": 0.08174484266693752,
      "learning_rate": 1.693761056028542e-05,
      "loss": 0.0624,
      "step": 290
    },
    {
      "epoch": 1.3990384615384617,
      "grad_norm": 0.1673040215040546,
      "learning_rate": 1.6915130557822698e-05,
      "loss": 0.074,
      "step": 291
    },
    {
      "epoch": 1.4038461538461537,
      "grad_norm": 0.09028914787866955,
      "learning_rate": 1.689258338483947e-05,
      "loss": 0.05,
      "step": 292
    },
    {
      "epoch": 1.4086538461538463,
      "grad_norm": 0.09466242244576858,
      "learning_rate": 1.686996926034902e-05,
      "loss": 0.0635,
      "step": 293
    },
    {
      "epoch": 1.4134615384615383,
      "grad_norm": 0.20720379856297044,
      "learning_rate": 1.6847288404014937e-05,
      "loss": 0.0645,
      "step": 294
    },
    {
      "epoch": 1.4182692307692308,
      "grad_norm": 0.07527452760463124,
      "learning_rate": 1.682454103614904e-05,
      "loss": 0.0497,
      "step": 295
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 0.07049172871814165,
      "learning_rate": 1.6801727377709195e-05,
      "loss": 0.043,
      "step": 296
    },
    {
      "epoch": 1.4278846153846154,
      "grad_norm": 0.07596360737890544,
      "learning_rate": 1.67788476502972e-05,
      "loss": 0.0557,
      "step": 297
    },
    {
      "epoch": 1.4326923076923077,
      "grad_norm": 0.05828557239205926,
      "learning_rate": 1.6755902076156606e-05,
      "loss": 0.0399,
      "step": 298
    },
    {
      "epoch": 1.4375,
      "grad_norm": 0.07824829415465828,
      "learning_rate": 1.6732890878170573e-05,
      "loss": 0.0515,
      "step": 299
    },
    {
      "epoch": 1.4423076923076923,
      "grad_norm": 0.1196376326344117,
      "learning_rate": 1.67098142798597e-05,
      "loss": 0.0418,
      "step": 300
    },
    {
      "epoch": 1.4471153846153846,
      "grad_norm": 0.208608847489839,
      "learning_rate": 1.668667250537987e-05,
      "loss": 0.0542,
      "step": 301
    },
    {
      "epoch": 1.4519230769230769,
      "grad_norm": 0.06973984010172055,
      "learning_rate": 1.6663465779520042e-05,
      "loss": 0.0583,
      "step": 302
    },
    {
      "epoch": 1.4567307692307692,
      "grad_norm": 0.12376975606199504,
      "learning_rate": 1.6640194327700087e-05,
      "loss": 0.0712,
      "step": 303
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 0.12332711526490642,
      "learning_rate": 1.6616858375968596e-05,
      "loss": 0.0636,
      "step": 304
    },
    {
      "epoch": 1.4663461538461537,
      "grad_norm": 0.1349428594198696,
      "learning_rate": 1.659345815100069e-05,
      "loss": 0.0709,
      "step": 305
    },
    {
      "epoch": 1.4711538461538463,
      "grad_norm": 0.06211861634913082,
      "learning_rate": 1.6569993880095807e-05,
      "loss": 0.0553,
      "step": 306
    },
    {
      "epoch": 1.4759615384615383,
      "grad_norm": 0.09659105062635664,
      "learning_rate": 1.6546465791175498e-05,
      "loss": 0.0531,
      "step": 307
    },
    {
      "epoch": 1.4807692307692308,
      "grad_norm": 0.09342666882873249,
      "learning_rate": 1.6522874112781213e-05,
      "loss": 0.0611,
      "step": 308
    },
    {
      "epoch": 1.4855769230769231,
      "grad_norm": 0.13551326826844226,
      "learning_rate": 1.6499219074072087e-05,
      "loss": 0.0621,
      "step": 309
    },
    {
      "epoch": 1.4903846153846154,
      "grad_norm": 0.40519149384401454,
      "learning_rate": 1.6475500904822707e-05,
      "loss": 0.0925,
      "step": 310
    },
    {
      "epoch": 1.4951923076923077,
      "grad_norm": 0.13621027271382902,
      "learning_rate": 1.645171983542088e-05,
      "loss": 0.0541,
      "step": 311
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.12744298236084015,
      "learning_rate": 1.6427876096865394e-05,
      "loss": 0.0515,
      "step": 312
    },
    {
      "epoch": 1.5048076923076923,
      "grad_norm": 0.16697093329903734,
      "learning_rate": 1.640396992076379e-05,
      "loss": 0.0624,
      "step": 313
    },
    {
      "epoch": 1.5096153846153846,
      "grad_norm": 0.07574158025536687,
      "learning_rate": 1.6380001539330088e-05,
      "loss": 0.041,
      "step": 314
    },
    {
      "epoch": 1.5144230769230769,
      "grad_norm": 0.10311992452298722,
      "learning_rate": 1.6355971185382547e-05,
      "loss": 0.04,
      "step": 315
    },
    {
      "epoch": 1.5192307692307692,
      "grad_norm": 0.0566873338353878,
      "learning_rate": 1.6331879092341402e-05,
      "loss": 0.0421,
      "step": 316
    },
    {
      "epoch": 1.5240384615384617,
      "grad_norm": 0.07891283160930698,
      "learning_rate": 1.6307725494226586e-05,
      "loss": 0.0492,
      "step": 317
    },
    {
      "epoch": 1.5288461538461537,
      "grad_norm": 0.08875113755957653,
      "learning_rate": 1.6283510625655474e-05,
      "loss": 0.0538,
      "step": 318
    },
    {
      "epoch": 1.5336538461538463,
      "grad_norm": 0.08283494999691045,
      "learning_rate": 1.6259234721840595e-05,
      "loss": 0.053,
      "step": 319
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 0.09904395464311501,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 0.054,
      "step": 320
    },
    {
      "epoch": 1.5432692307692308,
      "grad_norm": 0.058660964706435,
      "learning_rate": 1.6210500752291682e-05,
      "loss": 0.0399,
      "step": 321
    },
    {
      "epoch": 1.5480769230769231,
      "grad_norm": 0.06374339235337988,
      "learning_rate": 1.6186043159937884e-05,
      "loss": 0.0467,
      "step": 322
    },
    {
      "epoch": 1.5528846153846154,
      "grad_norm": 0.09778530971508288,
      "learning_rate": 1.616152547909618e-05,
      "loss": 0.0457,
      "step": 323
    },
    {
      "epoch": 1.5576923076923077,
      "grad_norm": 0.36462121325432806,
      "learning_rate": 1.6136947947920477e-05,
      "loss": 0.0748,
      "step": 324
    },
    {
      "epoch": 1.5625,
      "grad_norm": 0.09239179644812816,
      "learning_rate": 1.611231080514605e-05,
      "loss": 0.0579,
      "step": 325
    },
    {
      "epoch": 1.5673076923076923,
      "grad_norm": 0.06730495555474843,
      "learning_rate": 1.608761429008721e-05,
      "loss": 0.0532,
      "step": 326
    },
    {
      "epoch": 1.5721153846153846,
      "grad_norm": 0.10886449227713088,
      "learning_rate": 1.606285864263498e-05,
      "loss": 0.0622,
      "step": 327
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 0.08178603746607098,
      "learning_rate": 1.6038044103254775e-05,
      "loss": 0.0533,
      "step": 328
    },
    {
      "epoch": 1.5817307692307692,
      "grad_norm": 0.1434429242946264,
      "learning_rate": 1.601317091298406e-05,
      "loss": 0.0639,
      "step": 329
    },
    {
      "epoch": 1.5865384615384617,
      "grad_norm": 0.11938184136262624,
      "learning_rate": 1.5988239313430004e-05,
      "loss": 0.0483,
      "step": 330
    },
    {
      "epoch": 1.5913461538461537,
      "grad_norm": 0.07971565140902881,
      "learning_rate": 1.5963249546767144e-05,
      "loss": 0.0495,
      "step": 331
    },
    {
      "epoch": 1.5961538461538463,
      "grad_norm": 0.07934370009237511,
      "learning_rate": 1.5938201855735017e-05,
      "loss": 0.0525,
      "step": 332
    },
    {
      "epoch": 1.6009615384615383,
      "grad_norm": 0.09279871482369388,
      "learning_rate": 1.5913096483635827e-05,
      "loss": 0.0638,
      "step": 333
    },
    {
      "epoch": 1.6057692307692308,
      "grad_norm": 0.07068479380844515,
      "learning_rate": 1.5887933674332048e-05,
      "loss": 0.0542,
      "step": 334
    },
    {
      "epoch": 1.6105769230769231,
      "grad_norm": 0.05887956005606745,
      "learning_rate": 1.5862713672244092e-05,
      "loss": 0.0477,
      "step": 335
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.10711009110932575,
      "learning_rate": 1.5837436722347902e-05,
      "loss": 0.0441,
      "step": 336
    },
    {
      "epoch": 1.6201923076923077,
      "grad_norm": 0.11856312379019476,
      "learning_rate": 1.5812103070172592e-05,
      "loss": 0.0528,
      "step": 337
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.06402193688442184,
      "learning_rate": 1.578671296179806e-05,
      "loss": 0.0484,
      "step": 338
    },
    {
      "epoch": 1.6298076923076923,
      "grad_norm": 0.08028034353753223,
      "learning_rate": 1.5761266643852587e-05,
      "loss": 0.0406,
      "step": 339
    },
    {
      "epoch": 1.6346153846153846,
      "grad_norm": 0.09030790144178646,
      "learning_rate": 1.573576436351046e-05,
      "loss": 0.0682,
      "step": 340
    },
    {
      "epoch": 1.6394230769230769,
      "grad_norm": 0.10235321626530283,
      "learning_rate": 1.5710206368489555e-05,
      "loss": 0.0582,
      "step": 341
    },
    {
      "epoch": 1.6442307692307692,
      "grad_norm": 0.14013582185700324,
      "learning_rate": 1.5684592907048925e-05,
      "loss": 0.0511,
      "step": 342
    },
    {
      "epoch": 1.6490384615384617,
      "grad_norm": 0.06538327397582218,
      "learning_rate": 1.5658924227986415e-05,
      "loss": 0.0461,
      "step": 343
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 0.05809399874450898,
      "learning_rate": 1.563320058063622e-05,
      "loss": 0.0486,
      "step": 344
    },
    {
      "epoch": 1.6586538461538463,
      "grad_norm": 0.09327733600875882,
      "learning_rate": 1.560742221486648e-05,
      "loss": 0.0473,
      "step": 345
    },
    {
      "epoch": 1.6634615384615383,
      "grad_norm": 0.09647014445352887,
      "learning_rate": 1.5581589381076843e-05,
      "loss": 0.0493,
      "step": 346
    },
    {
      "epoch": 1.6682692307692308,
      "grad_norm": 0.16237378366590272,
      "learning_rate": 1.5555702330196024e-05,
      "loss": 0.0574,
      "step": 347
    },
    {
      "epoch": 1.6730769230769231,
      "grad_norm": 0.06395732604198012,
      "learning_rate": 1.5529761313679396e-05,
      "loss": 0.046,
      "step": 348
    },
    {
      "epoch": 1.6778846153846154,
      "grad_norm": 0.05731317689738296,
      "learning_rate": 1.5503766583506522e-05,
      "loss": 0.0348,
      "step": 349
    },
    {
      "epoch": 1.6826923076923077,
      "grad_norm": 0.06001703347969085,
      "learning_rate": 1.5477718392178716e-05,
      "loss": 0.0471,
      "step": 350
    },
    {
      "epoch": 1.6875,
      "grad_norm": 0.07369001708664508,
      "learning_rate": 1.545161699271659e-05,
      "loss": 0.0525,
      "step": 351
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 0.09070803277029246,
      "learning_rate": 1.5425462638657597e-05,
      "loss": 0.0481,
      "step": 352
    },
    {
      "epoch": 1.6971153846153846,
      "grad_norm": 0.08207707080403119,
      "learning_rate": 1.5399255584053568e-05,
      "loss": 0.0529,
      "step": 353
    },
    {
      "epoch": 1.7019230769230769,
      "grad_norm": 0.07503528594070227,
      "learning_rate": 1.5372996083468242e-05,
      "loss": 0.0465,
      "step": 354
    },
    {
      "epoch": 1.7067307692307692,
      "grad_norm": 0.06932216639248161,
      "learning_rate": 1.5346684391974792e-05,
      "loss": 0.0505,
      "step": 355
    },
    {
      "epoch": 1.7115384615384617,
      "grad_norm": 0.06039270395500656,
      "learning_rate": 1.5320320765153367e-05,
      "loss": 0.0508,
      "step": 356
    },
    {
      "epoch": 1.7163461538461537,
      "grad_norm": 0.05470970536503218,
      "learning_rate": 1.529390545908857e-05,
      "loss": 0.0401,
      "step": 357
    },
    {
      "epoch": 1.7211538461538463,
      "grad_norm": 0.148646770795989,
      "learning_rate": 1.526743873036701e-05,
      "loss": 0.0636,
      "step": 358
    },
    {
      "epoch": 1.7259615384615383,
      "grad_norm": 0.08160437998865314,
      "learning_rate": 1.5240920836074777e-05,
      "loss": 0.0537,
      "step": 359
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 0.06386477555341427,
      "learning_rate": 1.5214352033794981e-05,
      "loss": 0.0463,
      "step": 360
    },
    {
      "epoch": 1.7355769230769231,
      "grad_norm": 0.15913617441219755,
      "learning_rate": 1.5187732581605217e-05,
      "loss": 0.0753,
      "step": 361
    },
    {
      "epoch": 1.7403846153846154,
      "grad_norm": 0.05632098361765832,
      "learning_rate": 1.5161062738075068e-05,
      "loss": 0.0373,
      "step": 362
    },
    {
      "epoch": 1.7451923076923077,
      "grad_norm": 0.06305308335442596,
      "learning_rate": 1.5134342762263606e-05,
      "loss": 0.0499,
      "step": 363
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.07044344995233799,
      "learning_rate": 1.5107572913716859e-05,
      "loss": 0.0501,
      "step": 364
    },
    {
      "epoch": 1.7548076923076923,
      "grad_norm": 0.063343716681451,
      "learning_rate": 1.5080753452465296e-05,
      "loss": 0.0502,
      "step": 365
    },
    {
      "epoch": 1.7596153846153846,
      "grad_norm": 0.11584435216892863,
      "learning_rate": 1.505388463902131e-05,
      "loss": 0.0674,
      "step": 366
    },
    {
      "epoch": 1.7644230769230769,
      "grad_norm": 0.053383668514866735,
      "learning_rate": 1.502696673437667e-05,
      "loss": 0.0377,
      "step": 367
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 0.31553236043927735,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.0741,
      "step": 368
    },
    {
      "epoch": 1.7740384615384617,
      "grad_norm": 0.08618297884892488,
      "learning_rate": 1.4972984697834238e-05,
      "loss": 0.0542,
      "step": 369
    },
    {
      "epoch": 1.7788461538461537,
      "grad_norm": 0.07982695797359929,
      "learning_rate": 1.4945921090294076e-05,
      "loss": 0.0558,
      "step": 370
    },
    {
      "epoch": 1.7836538461538463,
      "grad_norm": 0.07106658627008242,
      "learning_rate": 1.4918809440263435e-05,
      "loss": 0.0491,
      "step": 371
    },
    {
      "epoch": 1.7884615384615383,
      "grad_norm": 0.15453844416734075,
      "learning_rate": 1.4891650011092896e-05,
      "loss": 0.0605,
      "step": 372
    },
    {
      "epoch": 1.7932692307692308,
      "grad_norm": 0.057030629847779624,
      "learning_rate": 1.486444306659714e-05,
      "loss": 0.0439,
      "step": 373
    },
    {
      "epoch": 1.7980769230769231,
      "grad_norm": 0.14271145836874272,
      "learning_rate": 1.4837188871052399e-05,
      "loss": 0.051,
      "step": 374
    },
    {
      "epoch": 1.8028846153846154,
      "grad_norm": 0.07069194001451831,
      "learning_rate": 1.4809887689193878e-05,
      "loss": 0.0392,
      "step": 375
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 0.19664214307784833,
      "learning_rate": 1.4782539786213184e-05,
      "loss": 0.0713,
      "step": 376
    },
    {
      "epoch": 1.8125,
      "grad_norm": 0.05925092563223736,
      "learning_rate": 1.4755145427755755e-05,
      "loss": 0.0455,
      "step": 377
    },
    {
      "epoch": 1.8173076923076923,
      "grad_norm": 0.8192733601726551,
      "learning_rate": 1.4727704879918272e-05,
      "loss": 0.1151,
      "step": 378
    },
    {
      "epoch": 1.8221153846153846,
      "grad_norm": 0.2016188228735544,
      "learning_rate": 1.4700218409246087e-05,
      "loss": 0.0598,
      "step": 379
    },
    {
      "epoch": 1.8269230769230769,
      "grad_norm": 0.061374954523764,
      "learning_rate": 1.4672686282730622e-05,
      "loss": 0.0452,
      "step": 380
    },
    {
      "epoch": 1.8317307692307692,
      "grad_norm": 0.061842525612584895,
      "learning_rate": 1.4645108767806778e-05,
      "loss": 0.0375,
      "step": 381
    },
    {
      "epoch": 1.8365384615384617,
      "grad_norm": 0.07469030775305018,
      "learning_rate": 1.4617486132350343e-05,
      "loss": 0.048,
      "step": 382
    },
    {
      "epoch": 1.8413461538461537,
      "grad_norm": 0.0928888974580352,
      "learning_rate": 1.4589818644675378e-05,
      "loss": 0.049,
      "step": 383
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 0.31683336763685055,
      "learning_rate": 1.4562106573531632e-05,
      "loss": 0.0723,
      "step": 384
    },
    {
      "epoch": 1.8509615384615383,
      "grad_norm": 0.09953047724237649,
      "learning_rate": 1.4534350188101905e-05,
      "loss": 0.0469,
      "step": 385
    },
    {
      "epoch": 1.8557692307692308,
      "grad_norm": 0.10594287597506612,
      "learning_rate": 1.4506549757999456e-05,
      "loss": 0.045,
      "step": 386
    },
    {
      "epoch": 1.8605769230769231,
      "grad_norm": 0.06428527544751217,
      "learning_rate": 1.4478705553265363e-05,
      "loss": 0.0419,
      "step": 387
    },
    {
      "epoch": 1.8653846153846154,
      "grad_norm": 0.11722784409441438,
      "learning_rate": 1.4450817844365924e-05,
      "loss": 0.0377,
      "step": 388
    },
    {
      "epoch": 1.8701923076923077,
      "grad_norm": 0.2511823024117316,
      "learning_rate": 1.4422886902190014e-05,
      "loss": 0.0543,
      "step": 389
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.05916599567333166,
      "learning_rate": 1.4394912998046451e-05,
      "loss": 0.0432,
      "step": 390
    },
    {
      "epoch": 1.8798076923076923,
      "grad_norm": 0.606569660945422,
      "learning_rate": 1.436689640366137e-05,
      "loss": 0.1017,
      "step": 391
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 0.0666905741738044,
      "learning_rate": 1.4338837391175582e-05,
      "loss": 0.0486,
      "step": 392
    },
    {
      "epoch": 1.8894230769230769,
      "grad_norm": 0.05834918033243259,
      "learning_rate": 1.4310736233141926e-05,
      "loss": 0.0522,
      "step": 393
    },
    {
      "epoch": 1.8942307692307692,
      "grad_norm": 0.07373308182411534,
      "learning_rate": 1.4282593202522627e-05,
      "loss": 0.054,
      "step": 394
    },
    {
      "epoch": 1.8990384615384617,
      "grad_norm": 0.07207291293955602,
      "learning_rate": 1.4254408572686642e-05,
      "loss": 0.0372,
      "step": 395
    },
    {
      "epoch": 1.9038461538461537,
      "grad_norm": 0.09662627463196465,
      "learning_rate": 1.4226182617406996e-05,
      "loss": 0.0337,
      "step": 396
    },
    {
      "epoch": 1.9086538461538463,
      "grad_norm": 0.1442134236541625,
      "learning_rate": 1.4197915610858143e-05,
      "loss": 0.054,
      "step": 397
    },
    {
      "epoch": 1.9134615384615383,
      "grad_norm": 0.18091310076926892,
      "learning_rate": 1.4169607827613284e-05,
      "loss": 0.045,
      "step": 398
    },
    {
      "epoch": 1.9182692307692308,
      "grad_norm": 0.06868751752986343,
      "learning_rate": 1.4141259542641706e-05,
      "loss": 0.0558,
      "step": 399
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.10163547176693943,
      "learning_rate": 1.4112871031306118e-05,
      "loss": 0.0434,
      "step": 400
    },
    {
      "epoch": 1.9278846153846154,
      "grad_norm": 0.054825774414479285,
      "learning_rate": 1.4084442569359964e-05,
      "loss": 0.0343,
      "step": 401
    },
    {
      "epoch": 1.9326923076923077,
      "grad_norm": 0.07026608771893644,
      "learning_rate": 1.4055974432944753e-05,
      "loss": 0.0475,
      "step": 402
    },
    {
      "epoch": 1.9375,
      "grad_norm": 0.10291762156742337,
      "learning_rate": 1.4027466898587375e-05,
      "loss": 0.0457,
      "step": 403
    },
    {
      "epoch": 1.9423076923076923,
      "grad_norm": 0.05316342813300211,
      "learning_rate": 1.3998920243197408e-05,
      "loss": 0.0499,
      "step": 404
    },
    {
      "epoch": 1.9471153846153846,
      "grad_norm": 0.06550782320800706,
      "learning_rate": 1.3970334744064451e-05,
      "loss": 0.0423,
      "step": 405
    },
    {
      "epoch": 1.9519230769230769,
      "grad_norm": 0.07804292825918316,
      "learning_rate": 1.3941710678855396e-05,
      "loss": 0.0443,
      "step": 406
    },
    {
      "epoch": 1.9567307692307692,
      "grad_norm": 0.07429907715562341,
      "learning_rate": 1.391304832561175e-05,
      "loss": 0.0402,
      "step": 407
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 0.2142184511801161,
      "learning_rate": 1.3884347962746949e-05,
      "loss": 0.0512,
      "step": 408
    },
    {
      "epoch": 1.9663461538461537,
      "grad_norm": 0.16021824250410513,
      "learning_rate": 1.3855609869043618e-05,
      "loss": 0.0452,
      "step": 409
    },
    {
      "epoch": 1.9711538461538463,
      "grad_norm": 0.0574800483304515,
      "learning_rate": 1.3826834323650899e-05,
      "loss": 0.0471,
      "step": 410
    },
    {
      "epoch": 1.9759615384615383,
      "grad_norm": 0.2739769913395487,
      "learning_rate": 1.3798021606081713e-05,
      "loss": 0.0642,
      "step": 411
    },
    {
      "epoch": 1.9807692307692308,
      "grad_norm": 0.06370708535612292,
      "learning_rate": 1.3769171996210053e-05,
      "loss": 0.0458,
      "step": 412
    },
    {
      "epoch": 1.9855769230769231,
      "grad_norm": 0.05907578608282777,
      "learning_rate": 1.3740285774268282e-05,
      "loss": 0.0321,
      "step": 413
    },
    {
      "epoch": 1.9903846153846154,
      "grad_norm": 0.11365263077952556,
      "learning_rate": 1.371136322084438e-05,
      "loss": 0.0456,
      "step": 414
    },
    {
      "epoch": 1.9951923076923077,
      "grad_norm": 0.06425980700440233,
      "learning_rate": 1.3682404616879246e-05,
      "loss": 0.0411,
      "step": 415
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.05172307684674768,
      "learning_rate": 1.3653410243663953e-05,
      "loss": 0.0382,
      "step": 416
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.04690802842378616,
      "eval_runtime": 648.0024,
      "eval_samples_per_second": 0.642,
      "eval_steps_per_second": 0.642,
      "step": 416
    },
    {
      "epoch": 2.0048076923076925,
      "grad_norm": 0.09848495623936511,
      "learning_rate": 1.3624380382837017e-05,
      "loss": 0.0532,
      "step": 417
    },
    {
      "epoch": 2.0096153846153846,
      "grad_norm": 0.14640042824025878,
      "learning_rate": 1.3595315316381676e-05,
      "loss": 0.0616,
      "step": 418
    },
    {
      "epoch": 2.014423076923077,
      "grad_norm": 0.10158301734086574,
      "learning_rate": 1.3566215326623131e-05,
      "loss": 0.0553,
      "step": 419
    },
    {
      "epoch": 2.019230769230769,
      "grad_norm": 0.06701565480160106,
      "learning_rate": 1.3537080696225815e-05,
      "loss": 0.0427,
      "step": 420
    },
    {
      "epoch": 2.0240384615384617,
      "grad_norm": 0.08065295773275843,
      "learning_rate": 1.3507911708190646e-05,
      "loss": 0.0557,
      "step": 421
    },
    {
      "epoch": 2.0288461538461537,
      "grad_norm": 0.10523082495380351,
      "learning_rate": 1.3478708645852272e-05,
      "loss": 0.0586,
      "step": 422
    },
    {
      "epoch": 2.0336538461538463,
      "grad_norm": 0.18697465455083528,
      "learning_rate": 1.3449471792876333e-05,
      "loss": 0.0546,
      "step": 423
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 0.06191401932950177,
      "learning_rate": 1.342020143325669e-05,
      "loss": 0.0481,
      "step": 424
    },
    {
      "epoch": 2.043269230769231,
      "grad_norm": 0.1012516190589754,
      "learning_rate": 1.3390897851312667e-05,
      "loss": 0.0517,
      "step": 425
    },
    {
      "epoch": 2.048076923076923,
      "grad_norm": 0.062042326792237604,
      "learning_rate": 1.336156133168631e-05,
      "loss": 0.0415,
      "step": 426
    },
    {
      "epoch": 2.0528846153846154,
      "grad_norm": 0.049348906228104686,
      "learning_rate": 1.3332192159339595e-05,
      "loss": 0.0412,
      "step": 427
    },
    {
      "epoch": 2.0576923076923075,
      "grad_norm": 0.11500491480015866,
      "learning_rate": 1.3302790619551673e-05,
      "loss": 0.0529,
      "step": 428
    },
    {
      "epoch": 2.0625,
      "grad_norm": 0.10009329207701417,
      "learning_rate": 1.3273356997916106e-05,
      "loss": 0.0537,
      "step": 429
    },
    {
      "epoch": 2.0673076923076925,
      "grad_norm": 0.05575617754588954,
      "learning_rate": 1.3243891580338074e-05,
      "loss": 0.0327,
      "step": 430
    },
    {
      "epoch": 2.0721153846153846,
      "grad_norm": 0.09660414425989514,
      "learning_rate": 1.3214394653031616e-05,
      "loss": 0.0412,
      "step": 431
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 0.07125554996804558,
      "learning_rate": 1.3184866502516846e-05,
      "loss": 0.0365,
      "step": 432
    },
    {
      "epoch": 2.081730769230769,
      "grad_norm": 0.08794465421436039,
      "learning_rate": 1.3155307415617156e-05,
      "loss": 0.0378,
      "step": 433
    },
    {
      "epoch": 2.0865384615384617,
      "grad_norm": 0.06719561932248863,
      "learning_rate": 1.3125717679456447e-05,
      "loss": 0.0415,
      "step": 434
    },
    {
      "epoch": 2.0913461538461537,
      "grad_norm": 0.055242495800776406,
      "learning_rate": 1.309609758145633e-05,
      "loss": 0.04,
      "step": 435
    },
    {
      "epoch": 2.0961538461538463,
      "grad_norm": 0.11987350538965214,
      "learning_rate": 1.3066447409333345e-05,
      "loss": 0.0529,
      "step": 436
    },
    {
      "epoch": 2.1009615384615383,
      "grad_norm": 0.055232425359608324,
      "learning_rate": 1.3036767451096148e-05,
      "loss": 0.0376,
      "step": 437
    },
    {
      "epoch": 2.105769230769231,
      "grad_norm": 0.08626281876977679,
      "learning_rate": 1.300705799504273e-05,
      "loss": 0.0498,
      "step": 438
    },
    {
      "epoch": 2.110576923076923,
      "grad_norm": 0.06593649496995174,
      "learning_rate": 1.2977319329757616e-05,
      "loss": 0.0443,
      "step": 439
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 0.09144469012116273,
      "learning_rate": 1.2947551744109044e-05,
      "loss": 0.0347,
      "step": 440
    },
    {
      "epoch": 2.1201923076923075,
      "grad_norm": 0.08166204013694911,
      "learning_rate": 1.2917755527246179e-05,
      "loss": 0.0526,
      "step": 441
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.07982399626487136,
      "learning_rate": 1.28879309685963e-05,
      "loss": 0.0449,
      "step": 442
    },
    {
      "epoch": 2.1298076923076925,
      "grad_norm": 0.057533200133422106,
      "learning_rate": 1.2858078357861979e-05,
      "loss": 0.0401,
      "step": 443
    },
    {
      "epoch": 2.1346153846153846,
      "grad_norm": 0.05062049428129828,
      "learning_rate": 1.2828197985018276e-05,
      "loss": 0.0296,
      "step": 444
    },
    {
      "epoch": 2.139423076923077,
      "grad_norm": 0.05921737454708383,
      "learning_rate": 1.2798290140309924e-05,
      "loss": 0.0342,
      "step": 445
    },
    {
      "epoch": 2.144230769230769,
      "grad_norm": 0.05546966347530244,
      "learning_rate": 1.2768355114248493e-05,
      "loss": 0.0488,
      "step": 446
    },
    {
      "epoch": 2.1490384615384617,
      "grad_norm": 0.1066134320685923,
      "learning_rate": 1.2738393197609602e-05,
      "loss": 0.0495,
      "step": 447
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 0.07375393764414992,
      "learning_rate": 1.2708404681430054e-05,
      "loss": 0.0494,
      "step": 448
    },
    {
      "epoch": 2.1586538461538463,
      "grad_norm": 0.27768663595460347,
      "learning_rate": 1.2678389857005033e-05,
      "loss": 0.0811,
      "step": 449
    },
    {
      "epoch": 2.1634615384615383,
      "grad_norm": 0.09006865215382909,
      "learning_rate": 1.2648349015885272e-05,
      "loss": 0.0418,
      "step": 450
    },
    {
      "epoch": 2.168269230769231,
      "grad_norm": 0.12002261259190176,
      "learning_rate": 1.2618282449874221e-05,
      "loss": 0.0448,
      "step": 451
    },
    {
      "epoch": 2.173076923076923,
      "grad_norm": 0.10333013231103437,
      "learning_rate": 1.2588190451025209e-05,
      "loss": 0.0493,
      "step": 452
    },
    {
      "epoch": 2.1778846153846154,
      "grad_norm": 0.06483469059771585,
      "learning_rate": 1.2558073311638604e-05,
      "loss": 0.0519,
      "step": 453
    },
    {
      "epoch": 2.1826923076923075,
      "grad_norm": 0.07068005887228106,
      "learning_rate": 1.2527931324258975e-05,
      "loss": 0.0493,
      "step": 454
    },
    {
      "epoch": 2.1875,
      "grad_norm": 0.06564365920729505,
      "learning_rate": 1.249776478167227e-05,
      "loss": 0.0401,
      "step": 455
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 0.0597517327695461,
      "learning_rate": 1.2467573976902936e-05,
      "loss": 0.0405,
      "step": 456
    },
    {
      "epoch": 2.1971153846153846,
      "grad_norm": 0.04953460199640968,
      "learning_rate": 1.2437359203211109e-05,
      "loss": 0.0297,
      "step": 457
    },
    {
      "epoch": 2.201923076923077,
      "grad_norm": 0.06446685813631807,
      "learning_rate": 1.2407120754089733e-05,
      "loss": 0.043,
      "step": 458
    },
    {
      "epoch": 2.206730769230769,
      "grad_norm": 0.06021480449689346,
      "learning_rate": 1.2376858923261732e-05,
      "loss": 0.0393,
      "step": 459
    },
    {
      "epoch": 2.2115384615384617,
      "grad_norm": 0.06466792906923734,
      "learning_rate": 1.2346574004677154e-05,
      "loss": 0.0507,
      "step": 460
    },
    {
      "epoch": 2.2163461538461537,
      "grad_norm": 0.053227151424238416,
      "learning_rate": 1.2316266292510305e-05,
      "loss": 0.0406,
      "step": 461
    },
    {
      "epoch": 2.2211538461538463,
      "grad_norm": 0.06128895045249356,
      "learning_rate": 1.2285936081156897e-05,
      "loss": 0.0395,
      "step": 462
    },
    {
      "epoch": 2.2259615384615383,
      "grad_norm": 0.1947871186137165,
      "learning_rate": 1.2255583665231196e-05,
      "loss": 0.0571,
      "step": 463
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 0.07326628756759755,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 0.0449,
      "step": 464
    },
    {
      "epoch": 2.235576923076923,
      "grad_norm": 0.06225356778255313,
      "learning_rate": 1.2194813399195518e-05,
      "loss": 0.0336,
      "step": 465
    },
    {
      "epoch": 2.2403846153846154,
      "grad_norm": 0.06683460972601274,
      "learning_rate": 1.2164396139381029e-05,
      "loss": 0.0389,
      "step": 466
    },
    {
      "epoch": 2.2451923076923075,
      "grad_norm": 0.06921540134308174,
      "learning_rate": 1.2133957855579501e-05,
      "loss": 0.0499,
      "step": 467
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.09498798944000077,
      "learning_rate": 1.210349884345496e-05,
      "loss": 0.0478,
      "step": 468
    },
    {
      "epoch": 2.2548076923076925,
      "grad_norm": 0.06648747178468425,
      "learning_rate": 1.2073019398872778e-05,
      "loss": 0.0506,
      "step": 469
    },
    {
      "epoch": 2.2596153846153846,
      "grad_norm": 0.05110424812489118,
      "learning_rate": 1.2042519817896805e-05,
      "loss": 0.0287,
      "step": 470
    },
    {
      "epoch": 2.264423076923077,
      "grad_norm": 0.05029982063289216,
      "learning_rate": 1.2012000396786485e-05,
      "loss": 0.0335,
      "step": 471
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 0.14040426614010731,
      "learning_rate": 1.1981461431993978e-05,
      "loss": 0.0498,
      "step": 472
    },
    {
      "epoch": 2.2740384615384617,
      "grad_norm": 0.0676757231382729,
      "learning_rate": 1.1950903220161286e-05,
      "loss": 0.0447,
      "step": 473
    },
    {
      "epoch": 2.2788461538461537,
      "grad_norm": 0.09331669187614401,
      "learning_rate": 1.1920326058117364e-05,
      "loss": 0.0474,
      "step": 474
    },
    {
      "epoch": 2.2836538461538463,
      "grad_norm": 0.06560855886057793,
      "learning_rate": 1.1889730242875243e-05,
      "loss": 0.0346,
      "step": 475
    },
    {
      "epoch": 2.2884615384615383,
      "grad_norm": 0.08290694618137075,
      "learning_rate": 1.1859116071629148e-05,
      "loss": 0.0326,
      "step": 476
    },
    {
      "epoch": 2.293269230769231,
      "grad_norm": 0.05110857749576266,
      "learning_rate": 1.1828483841751597e-05,
      "loss": 0.031,
      "step": 477
    },
    {
      "epoch": 2.298076923076923,
      "grad_norm": 0.08550541993147129,
      "learning_rate": 1.1797833850790527e-05,
      "loss": 0.0403,
      "step": 478
    },
    {
      "epoch": 2.3028846153846154,
      "grad_norm": 0.0572490553285052,
      "learning_rate": 1.1767166396466404e-05,
      "loss": 0.0399,
      "step": 479
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 0.15350767111460348,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.06,
      "step": 480
    },
    {
      "epoch": 2.3125,
      "grad_norm": 0.06156538163463442,
      "learning_rate": 1.1705780289456069e-05,
      "loss": 0.0517,
      "step": 481
    },
    {
      "epoch": 2.3173076923076925,
      "grad_norm": 0.07045189201120243,
      "learning_rate": 1.1675062233047365e-05,
      "loss": 0.0439,
      "step": 482
    },
    {
      "epoch": 2.3221153846153846,
      "grad_norm": 0.09154119872081805,
      "learning_rate": 1.1644327905824808e-05,
      "loss": 0.0425,
      "step": 483
    },
    {
      "epoch": 2.326923076923077,
      "grad_norm": 0.07182180657901398,
      "learning_rate": 1.1613577606328068e-05,
      "loss": 0.0381,
      "step": 484
    },
    {
      "epoch": 2.331730769230769,
      "grad_norm": 0.05748972081021909,
      "learning_rate": 1.1582811633251949e-05,
      "loss": 0.0341,
      "step": 485
    },
    {
      "epoch": 2.3365384615384617,
      "grad_norm": 0.053632498066069026,
      "learning_rate": 1.1552030285443516e-05,
      "loss": 0.033,
      "step": 486
    },
    {
      "epoch": 2.3413461538461537,
      "grad_norm": 0.06204178472636896,
      "learning_rate": 1.1521233861899168e-05,
      "loss": 0.046,
      "step": 487
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 0.08242724286411429,
      "learning_rate": 1.1490422661761744e-05,
      "loss": 0.0478,
      "step": 488
    },
    {
      "epoch": 2.3509615384615383,
      "grad_norm": 0.1034362070015731,
      "learning_rate": 1.1459596984317622e-05,
      "loss": 0.0328,
      "step": 489
    },
    {
      "epoch": 2.355769230769231,
      "grad_norm": 0.051941044582495224,
      "learning_rate": 1.1428757128993801e-05,
      "loss": 0.0374,
      "step": 490
    },
    {
      "epoch": 2.360576923076923,
      "grad_norm": 0.0610377944509296,
      "learning_rate": 1.1397903395354996e-05,
      "loss": 0.0494,
      "step": 491
    },
    {
      "epoch": 2.3653846153846154,
      "grad_norm": 0.10792475277766793,
      "learning_rate": 1.1367036083100735e-05,
      "loss": 0.0416,
      "step": 492
    },
    {
      "epoch": 2.3701923076923075,
      "grad_norm": 0.05176621816133901,
      "learning_rate": 1.1336155492062439e-05,
      "loss": 0.0374,
      "step": 493
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.1830820283452648,
      "learning_rate": 1.130526192220052e-05,
      "loss": 0.0426,
      "step": 494
    },
    {
      "epoch": 2.3798076923076925,
      "grad_norm": 0.05939495816662279,
      "learning_rate": 1.1274355673601446e-05,
      "loss": 0.0377,
      "step": 495
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 0.3078689725508056,
      "learning_rate": 1.1243437046474854e-05,
      "loss": 0.0585,
      "step": 496
    },
    {
      "epoch": 2.389423076923077,
      "grad_norm": 0.05265975245343238,
      "learning_rate": 1.1212506341150615e-05,
      "loss": 0.0398,
      "step": 497
    },
    {
      "epoch": 2.394230769230769,
      "grad_norm": 0.05078907914565476,
      "learning_rate": 1.118156385807593e-05,
      "loss": 0.0306,
      "step": 498
    },
    {
      "epoch": 2.3990384615384617,
      "grad_norm": 0.0761346619715515,
      "learning_rate": 1.1150609897812387e-05,
      "loss": 0.0492,
      "step": 499
    },
    {
      "epoch": 2.4038461538461537,
      "grad_norm": 0.05117864250534968,
      "learning_rate": 1.1119644761033079e-05,
      "loss": 0.0365,
      "step": 500
    },
    {
      "epoch": 2.4086538461538463,
      "grad_norm": 0.10470822992789809,
      "learning_rate": 1.1088668748519646e-05,
      "loss": 0.0418,
      "step": 501
    },
    {
      "epoch": 2.4134615384615383,
      "grad_norm": 0.05049412687256636,
      "learning_rate": 1.105768216115938e-05,
      "loss": 0.0397,
      "step": 502
    },
    {
      "epoch": 2.418269230769231,
      "grad_norm": 0.1202403881367944,
      "learning_rate": 1.1026685299942286e-05,
      "loss": 0.0394,
      "step": 503
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 0.05480571624600338,
      "learning_rate": 1.0995678465958168e-05,
      "loss": 0.043,
      "step": 504
    },
    {
      "epoch": 2.4278846153846154,
      "grad_norm": 0.18682568398729676,
      "learning_rate": 1.0964661960393703e-05,
      "loss": 0.0501,
      "step": 505
    },
    {
      "epoch": 2.4326923076923075,
      "grad_norm": 0.060778331905410196,
      "learning_rate": 1.0933636084529507e-05,
      "loss": 0.0348,
      "step": 506
    },
    {
      "epoch": 2.4375,
      "grad_norm": 0.6316448595768777,
      "learning_rate": 1.0902601139737225e-05,
      "loss": 0.0786,
      "step": 507
    },
    {
      "epoch": 2.4423076923076925,
      "grad_norm": 0.05196017903289482,
      "learning_rate": 1.0871557427476585e-05,
      "loss": 0.0411,
      "step": 508
    },
    {
      "epoch": 2.4471153846153846,
      "grad_norm": 0.059240283913686,
      "learning_rate": 1.0840505249292477e-05,
      "loss": 0.0307,
      "step": 509
    },
    {
      "epoch": 2.451923076923077,
      "grad_norm": 0.05567013957381528,
      "learning_rate": 1.0809444906812034e-05,
      "loss": 0.0414,
      "step": 510
    },
    {
      "epoch": 2.456730769230769,
      "grad_norm": 0.13174412309075698,
      "learning_rate": 1.0778376701741688e-05,
      "loss": 0.0479,
      "step": 511
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 0.5467092813541966,
      "learning_rate": 1.0747300935864245e-05,
      "loss": 0.0787,
      "step": 512
    },
    {
      "epoch": 2.4663461538461537,
      "grad_norm": 0.060029266286785024,
      "learning_rate": 1.0716217911035952e-05,
      "loss": 0.0354,
      "step": 513
    },
    {
      "epoch": 2.4711538461538463,
      "grad_norm": 0.05106659980274923,
      "learning_rate": 1.0685127929183567e-05,
      "loss": 0.0284,
      "step": 514
    },
    {
      "epoch": 2.4759615384615383,
      "grad_norm": 0.053310037956680024,
      "learning_rate": 1.0654031292301432e-05,
      "loss": 0.0451,
      "step": 515
    },
    {
      "epoch": 2.480769230769231,
      "grad_norm": 0.05933062369552584,
      "learning_rate": 1.0622928302448523e-05,
      "loss": 0.0406,
      "step": 516
    },
    {
      "epoch": 2.485576923076923,
      "grad_norm": 0.05429850834654343,
      "learning_rate": 1.0591819261745528e-05,
      "loss": 0.0371,
      "step": 517
    },
    {
      "epoch": 2.4903846153846154,
      "grad_norm": 0.06471551286286231,
      "learning_rate": 1.0560704472371919e-05,
      "loss": 0.0365,
      "step": 518
    },
    {
      "epoch": 2.4951923076923075,
      "grad_norm": 0.048700369262165075,
      "learning_rate": 1.0529584236562995e-05,
      "loss": 0.027,
      "step": 519
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.0514502727491178,
      "learning_rate": 1.0498458856606972e-05,
      "loss": 0.029,
      "step": 520
    },
    {
      "epoch": 2.5048076923076925,
      "grad_norm": 0.06091824133511771,
      "learning_rate": 1.0467328634842024e-05,
      "loss": 0.0273,
      "step": 521
    },
    {
      "epoch": 2.5096153846153846,
      "grad_norm": 0.07463494475720503,
      "learning_rate": 1.0436193873653362e-05,
      "loss": 0.0361,
      "step": 522
    },
    {
      "epoch": 2.5144230769230766,
      "grad_norm": 0.06526451935821412,
      "learning_rate": 1.0405054875470287e-05,
      "loss": 0.0363,
      "step": 523
    },
    {
      "epoch": 2.519230769230769,
      "grad_norm": 0.07186538655212425,
      "learning_rate": 1.037391194276326e-05,
      "loss": 0.0413,
      "step": 524
    },
    {
      "epoch": 2.5240384615384617,
      "grad_norm": 0.07498143993876791,
      "learning_rate": 1.0342765378040953e-05,
      "loss": 0.0442,
      "step": 525
    },
    {
      "epoch": 2.5288461538461537,
      "grad_norm": 0.1504263505153848,
      "learning_rate": 1.0311615483847333e-05,
      "loss": 0.0454,
      "step": 526
    },
    {
      "epoch": 2.5336538461538463,
      "grad_norm": 0.06790265502838426,
      "learning_rate": 1.028046256275869e-05,
      "loss": 0.0402,
      "step": 527
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 0.08753553872740345,
      "learning_rate": 1.0249306917380731e-05,
      "loss": 0.0444,
      "step": 528
    },
    {
      "epoch": 2.543269230769231,
      "grad_norm": 0.10977643256677208,
      "learning_rate": 1.0218148850345613e-05,
      "loss": 0.0419,
      "step": 529
    },
    {
      "epoch": 2.5480769230769234,
      "grad_norm": 0.056478281298770194,
      "learning_rate": 1.0186988664309023e-05,
      "loss": 0.0333,
      "step": 530
    },
    {
      "epoch": 2.5528846153846154,
      "grad_norm": 0.13861759308243096,
      "learning_rate": 1.0155826661947232e-05,
      "loss": 0.0454,
      "step": 531
    },
    {
      "epoch": 2.5576923076923075,
      "grad_norm": 0.08288370818152845,
      "learning_rate": 1.0124663145954152e-05,
      "loss": 0.035,
      "step": 532
    },
    {
      "epoch": 2.5625,
      "grad_norm": 0.04984676350383764,
      "learning_rate": 1.0093498419038394e-05,
      "loss": 0.0366,
      "step": 533
    },
    {
      "epoch": 2.5673076923076925,
      "grad_norm": 0.0523442631904278,
      "learning_rate": 1.0062332783920337e-05,
      "loss": 0.0358,
      "step": 534
    },
    {
      "epoch": 2.5721153846153846,
      "grad_norm": 0.05093764185474241,
      "learning_rate": 1.0031166543329179e-05,
      "loss": 0.0338,
      "step": 535
    },
    {
      "epoch": 2.5769230769230766,
      "grad_norm": 0.07617381578433968,
      "learning_rate": 1e-05,
      "loss": 0.0463,
      "step": 536
    },
    {
      "epoch": 2.581730769230769,
      "grad_norm": 0.20982998385255489,
      "learning_rate": 9.968833456670824e-06,
      "loss": 0.0434,
      "step": 537
    },
    {
      "epoch": 2.5865384615384617,
      "grad_norm": 0.047404699159531945,
      "learning_rate": 9.937667216079665e-06,
      "loss": 0.032,
      "step": 538
    },
    {
      "epoch": 2.5913461538461537,
      "grad_norm": 0.11706527029756698,
      "learning_rate": 9.90650158096161e-06,
      "loss": 0.0405,
      "step": 539
    },
    {
      "epoch": 2.5961538461538463,
      "grad_norm": 0.0542903060011081,
      "learning_rate": 9.87533685404585e-06,
      "loss": 0.0313,
      "step": 540
    },
    {
      "epoch": 2.6009615384615383,
      "grad_norm": 0.08907544097199387,
      "learning_rate": 9.844173338052771e-06,
      "loss": 0.0482,
      "step": 541
    },
    {
      "epoch": 2.605769230769231,
      "grad_norm": 0.0998157061972321,
      "learning_rate": 9.81301133569098e-06,
      "loss": 0.0287,
      "step": 542
    },
    {
      "epoch": 2.6105769230769234,
      "grad_norm": 0.04890006141974439,
      "learning_rate": 9.78185114965439e-06,
      "loss": 0.0322,
      "step": 543
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 0.09618420659013831,
      "learning_rate": 9.750693082619274e-06,
      "loss": 0.0359,
      "step": 544
    },
    {
      "epoch": 2.6201923076923075,
      "grad_norm": 0.17195067784511725,
      "learning_rate": 9.719537437241311e-06,
      "loss": 0.0467,
      "step": 545
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.06751907999075941,
      "learning_rate": 9.68838451615267e-06,
      "loss": 0.0408,
      "step": 546
    },
    {
      "epoch": 2.6298076923076925,
      "grad_norm": 0.09323093770732031,
      "learning_rate": 9.65723462195905e-06,
      "loss": 0.0425,
      "step": 547
    },
    {
      "epoch": 2.6346153846153846,
      "grad_norm": 0.08845747541161238,
      "learning_rate": 9.626088057236745e-06,
      "loss": 0.0425,
      "step": 548
    },
    {
      "epoch": 2.6394230769230766,
      "grad_norm": 0.06351492826492794,
      "learning_rate": 9.594945124529718e-06,
      "loss": 0.0269,
      "step": 549
    },
    {
      "epoch": 2.644230769230769,
      "grad_norm": 0.041355677884764006,
      "learning_rate": 9.563806126346643e-06,
      "loss": 0.0268,
      "step": 550
    },
    {
      "epoch": 2.6490384615384617,
      "grad_norm": 0.05184768493283626,
      "learning_rate": 9.532671365157979e-06,
      "loss": 0.039,
      "step": 551
    },
    {
      "epoch": 2.6538461538461537,
      "grad_norm": 0.06385704162001284,
      "learning_rate": 9.501541143393028e-06,
      "loss": 0.0263,
      "step": 552
    },
    {
      "epoch": 2.6586538461538463,
      "grad_norm": 0.05146191076953899,
      "learning_rate": 9.470415763437003e-06,
      "loss": 0.0262,
      "step": 553
    },
    {
      "epoch": 2.6634615384615383,
      "grad_norm": 0.13734055195686642,
      "learning_rate": 9.439295527628083e-06,
      "loss": 0.0407,
      "step": 554
    },
    {
      "epoch": 2.668269230769231,
      "grad_norm": 0.06462268744975701,
      "learning_rate": 9.408180738254472e-06,
      "loss": 0.0367,
      "step": 555
    },
    {
      "epoch": 2.6730769230769234,
      "grad_norm": 0.06726587319878481,
      "learning_rate": 9.377071697551479e-06,
      "loss": 0.039,
      "step": 556
    },
    {
      "epoch": 2.6778846153846154,
      "grad_norm": 0.054033144956096665,
      "learning_rate": 9.34596870769857e-06,
      "loss": 0.0292,
      "step": 557
    },
    {
      "epoch": 2.6826923076923075,
      "grad_norm": 0.05665824179183569,
      "learning_rate": 9.314872070816435e-06,
      "loss": 0.0343,
      "step": 558
    },
    {
      "epoch": 2.6875,
      "grad_norm": 0.07320039346322695,
      "learning_rate": 9.28378208896405e-06,
      "loss": 0.0365,
      "step": 559
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 0.07376236579736474,
      "learning_rate": 9.252699064135759e-06,
      "loss": 0.0456,
      "step": 560
    },
    {
      "epoch": 2.6971153846153846,
      "grad_norm": 0.05025837931043774,
      "learning_rate": 9.221623298258315e-06,
      "loss": 0.0376,
      "step": 561
    },
    {
      "epoch": 2.7019230769230766,
      "grad_norm": 0.10617628490410105,
      "learning_rate": 9.190555093187968e-06,
      "loss": 0.0443,
      "step": 562
    },
    {
      "epoch": 2.706730769230769,
      "grad_norm": 0.36610179341920995,
      "learning_rate": 9.159494750707527e-06,
      "loss": 0.0577,
      "step": 563
    },
    {
      "epoch": 2.7115384615384617,
      "grad_norm": 0.11290041902172175,
      "learning_rate": 9.128442572523418e-06,
      "loss": 0.0426,
      "step": 564
    },
    {
      "epoch": 2.7163461538461537,
      "grad_norm": 0.0456954181773318,
      "learning_rate": 9.097398860262777e-06,
      "loss": 0.0259,
      "step": 565
    },
    {
      "epoch": 2.7211538461538463,
      "grad_norm": 0.10816083595452224,
      "learning_rate": 9.066363915470494e-06,
      "loss": 0.048,
      "step": 566
    },
    {
      "epoch": 2.7259615384615383,
      "grad_norm": 0.08367749983248716,
      "learning_rate": 9.0353380396063e-06,
      "loss": 0.0376,
      "step": 567
    },
    {
      "epoch": 2.730769230769231,
      "grad_norm": 0.0590589710003903,
      "learning_rate": 9.004321534041836e-06,
      "loss": 0.0342,
      "step": 568
    },
    {
      "epoch": 2.7355769230769234,
      "grad_norm": 0.060352322238875204,
      "learning_rate": 8.973314700057717e-06,
      "loss": 0.0456,
      "step": 569
    },
    {
      "epoch": 2.7403846153846154,
      "grad_norm": 0.21306107570514723,
      "learning_rate": 8.942317838840625e-06,
      "loss": 0.0538,
      "step": 570
    },
    {
      "epoch": 2.7451923076923075,
      "grad_norm": 0.06037909552573361,
      "learning_rate": 8.911331251480357e-06,
      "loss": 0.035,
      "step": 571
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.07160499284043235,
      "learning_rate": 8.880355238966923e-06,
      "loss": 0.038,
      "step": 572
    },
    {
      "epoch": 2.7548076923076925,
      "grad_norm": 0.0524047016786689,
      "learning_rate": 8.849390102187615e-06,
      "loss": 0.0405,
      "step": 573
    },
    {
      "epoch": 2.7596153846153846,
      "grad_norm": 0.07583213549698853,
      "learning_rate": 8.818436141924072e-06,
      "loss": 0.0433,
      "step": 574
    },
    {
      "epoch": 2.7644230769230766,
      "grad_norm": 0.0686754095663053,
      "learning_rate": 8.787493658849387e-06,
      "loss": 0.0379,
      "step": 575
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 0.07674892792868182,
      "learning_rate": 8.756562953525151e-06,
      "loss": 0.0411,
      "step": 576
    },
    {
      "epoch": 2.7740384615384617,
      "grad_norm": 0.08744786594007099,
      "learning_rate": 8.72564432639856e-06,
      "loss": 0.0382,
      "step": 577
    },
    {
      "epoch": 2.7788461538461537,
      "grad_norm": 0.12689916785393432,
      "learning_rate": 8.694738077799487e-06,
      "loss": 0.0432,
      "step": 578
    },
    {
      "epoch": 2.7836538461538463,
      "grad_norm": 0.47215513025914896,
      "learning_rate": 8.663844507937563e-06,
      "loss": 0.0708,
      "step": 579
    },
    {
      "epoch": 2.7884615384615383,
      "grad_norm": 0.04744684384186553,
      "learning_rate": 8.632963916899268e-06,
      "loss": 0.025,
      "step": 580
    },
    {
      "epoch": 2.793269230769231,
      "grad_norm": 0.054523021079865105,
      "learning_rate": 8.602096604645009e-06,
      "loss": 0.032,
      "step": 581
    },
    {
      "epoch": 2.7980769230769234,
      "grad_norm": 0.08089909175562715,
      "learning_rate": 8.571242871006202e-06,
      "loss": 0.0377,
      "step": 582
    },
    {
      "epoch": 2.8028846153846154,
      "grad_norm": 0.10426312821542465,
      "learning_rate": 8.540403015682382e-06,
      "loss": 0.0343,
      "step": 583
    },
    {
      "epoch": 2.8076923076923075,
      "grad_norm": 0.15835548001319968,
      "learning_rate": 8.509577338238255e-06,
      "loss": 0.0392,
      "step": 584
    },
    {
      "epoch": 2.8125,
      "grad_norm": 0.3016217085689419,
      "learning_rate": 8.478766138100834e-06,
      "loss": 0.0558,
      "step": 585
    },
    {
      "epoch": 2.8173076923076925,
      "grad_norm": 0.054130385368626986,
      "learning_rate": 8.447969714556484e-06,
      "loss": 0.0369,
      "step": 586
    },
    {
      "epoch": 2.8221153846153846,
      "grad_norm": 0.0629394938529723,
      "learning_rate": 8.417188366748051e-06,
      "loss": 0.0402,
      "step": 587
    },
    {
      "epoch": 2.8269230769230766,
      "grad_norm": 0.09540493460970031,
      "learning_rate": 8.386422393671934e-06,
      "loss": 0.0357,
      "step": 588
    },
    {
      "epoch": 2.831730769230769,
      "grad_norm": 0.08011753490657897,
      "learning_rate": 8.355672094175192e-06,
      "loss": 0.0352,
      "step": 589
    },
    {
      "epoch": 2.8365384615384617,
      "grad_norm": 0.07508285602941155,
      "learning_rate": 8.324937766952638e-06,
      "loss": 0.0388,
      "step": 590
    },
    {
      "epoch": 2.8413461538461537,
      "grad_norm": 0.13431605264087207,
      "learning_rate": 8.294219710543931e-06,
      "loss": 0.0485,
      "step": 591
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 0.10197909490521279,
      "learning_rate": 8.263518223330698e-06,
      "loss": 0.0396,
      "step": 592
    },
    {
      "epoch": 2.8509615384615383,
      "grad_norm": 0.06053779035965552,
      "learning_rate": 8.232833603533601e-06,
      "loss": 0.036,
      "step": 593
    },
    {
      "epoch": 2.855769230769231,
      "grad_norm": 0.09855017437748007,
      "learning_rate": 8.202166149209475e-06,
      "loss": 0.0391,
      "step": 594
    },
    {
      "epoch": 2.8605769230769234,
      "grad_norm": 0.0516648033589512,
      "learning_rate": 8.171516158248406e-06,
      "loss": 0.038,
      "step": 595
    },
    {
      "epoch": 2.8653846153846154,
      "grad_norm": 0.04904097817224913,
      "learning_rate": 8.140883928370855e-06,
      "loss": 0.0324,
      "step": 596
    },
    {
      "epoch": 2.8701923076923075,
      "grad_norm": 0.059319364418910454,
      "learning_rate": 8.11026975712476e-06,
      "loss": 0.0319,
      "step": 597
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.055192744015683824,
      "learning_rate": 8.079673941882639e-06,
      "loss": 0.0383,
      "step": 598
    },
    {
      "epoch": 2.8798076923076925,
      "grad_norm": 0.05303607738028502,
      "learning_rate": 8.04909677983872e-06,
      "loss": 0.0297,
      "step": 599
    },
    {
      "epoch": 2.8846153846153846,
      "grad_norm": 0.0489483645253042,
      "learning_rate": 8.018538568006027e-06,
      "loss": 0.0296,
      "step": 600
    },
    {
      "epoch": 2.8894230769230766,
      "grad_norm": 0.04249371972963132,
      "learning_rate": 7.987999603213518e-06,
      "loss": 0.0267,
      "step": 601
    },
    {
      "epoch": 2.894230769230769,
      "grad_norm": 0.0676602948680593,
      "learning_rate": 7.957480182103198e-06,
      "loss": 0.0455,
      "step": 602
    },
    {
      "epoch": 2.8990384615384617,
      "grad_norm": 0.049254397958863314,
      "learning_rate": 7.926980601127225e-06,
      "loss": 0.0322,
      "step": 603
    },
    {
      "epoch": 2.9038461538461537,
      "grad_norm": 0.07802766387278819,
      "learning_rate": 7.896501156545044e-06,
      "loss": 0.041,
      "step": 604
    },
    {
      "epoch": 2.9086538461538463,
      "grad_norm": 0.09666148528050351,
      "learning_rate": 7.866042144420502e-06,
      "loss": 0.0394,
      "step": 605
    },
    {
      "epoch": 2.9134615384615383,
      "grad_norm": 0.07932894408692619,
      "learning_rate": 7.835603860618973e-06,
      "loss": 0.0315,
      "step": 606
    },
    {
      "epoch": 2.918269230769231,
      "grad_norm": 0.056774952645466434,
      "learning_rate": 7.805186600804489e-06,
      "loss": 0.0344,
      "step": 607
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 0.07199503651492402,
      "learning_rate": 7.774790660436857e-06,
      "loss": 0.034,
      "step": 608
    },
    {
      "epoch": 2.9278846153846154,
      "grad_norm": 0.052765019743295546,
      "learning_rate": 7.744416334768809e-06,
      "loss": 0.0336,
      "step": 609
    },
    {
      "epoch": 2.9326923076923075,
      "grad_norm": 0.2425111964250572,
      "learning_rate": 7.714063918843106e-06,
      "loss": 0.0368,
      "step": 610
    },
    {
      "epoch": 2.9375,
      "grad_norm": 0.0573583428369469,
      "learning_rate": 7.6837337074897e-06,
      "loss": 0.0315,
      "step": 611
    },
    {
      "epoch": 2.9423076923076925,
      "grad_norm": 0.0794686210046641,
      "learning_rate": 7.653425995322852e-06,
      "loss": 0.0392,
      "step": 612
    },
    {
      "epoch": 2.9471153846153846,
      "grad_norm": 0.05725424968082567,
      "learning_rate": 7.623141076738271e-06,
      "loss": 0.034,
      "step": 613
    },
    {
      "epoch": 2.9519230769230766,
      "grad_norm": 0.1483746110976607,
      "learning_rate": 7.592879245910273e-06,
      "loss": 0.0409,
      "step": 614
    },
    {
      "epoch": 2.956730769230769,
      "grad_norm": 0.06971876108506422,
      "learning_rate": 7.562640796788893e-06,
      "loss": 0.0389,
      "step": 615
    },
    {
      "epoch": 2.9615384615384617,
      "grad_norm": 0.0614189216530926,
      "learning_rate": 7.532426023097063e-06,
      "loss": 0.0314,
      "step": 616
    },
    {
      "epoch": 2.9663461538461537,
      "grad_norm": 0.06013803388217264,
      "learning_rate": 7.50223521832773e-06,
      "loss": 0.033,
      "step": 617
    },
    {
      "epoch": 2.9711538461538463,
      "grad_norm": 0.05964676991050379,
      "learning_rate": 7.472068675741024e-06,
      "loss": 0.0403,
      "step": 618
    },
    {
      "epoch": 2.9759615384615383,
      "grad_norm": 0.05569912740234639,
      "learning_rate": 7.4419266883614e-06,
      "loss": 0.0358,
      "step": 619
    },
    {
      "epoch": 2.980769230769231,
      "grad_norm": 0.054947971179017556,
      "learning_rate": 7.411809548974792e-06,
      "loss": 0.0358,
      "step": 620
    },
    {
      "epoch": 2.9855769230769234,
      "grad_norm": 0.051681078129317395,
      "learning_rate": 7.38171755012578e-06,
      "loss": 0.0274,
      "step": 621
    },
    {
      "epoch": 2.9903846153846154,
      "grad_norm": 0.05151112904123479,
      "learning_rate": 7.3516509841147276e-06,
      "loss": 0.0251,
      "step": 622
    },
    {
      "epoch": 2.9951923076923075,
      "grad_norm": 0.19992916958393026,
      "learning_rate": 7.321610142994971e-06,
      "loss": 0.0475,
      "step": 623
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.051314457046689944,
      "learning_rate": 7.291595318569951e-06,
      "loss": 0.0244,
      "step": 624
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.03534391149878502,
      "eval_runtime": 678.3069,
      "eval_samples_per_second": 0.613,
      "eval_steps_per_second": 0.613,
      "step": 624
    },
    {
      "epoch": 3.0048076923076925,
      "grad_norm": 0.0558396496581489,
      "learning_rate": 7.2616068023904e-06,
      "loss": 0.0321,
      "step": 625
    },
    {
      "epoch": 3.0096153846153846,
      "grad_norm": 0.05018919612460003,
      "learning_rate": 7.2316448857515076e-06,
      "loss": 0.0325,
      "step": 626
    },
    {
      "epoch": 3.014423076923077,
      "grad_norm": 0.07567219310323099,
      "learning_rate": 7.201709859690081e-06,
      "loss": 0.0351,
      "step": 627
    },
    {
      "epoch": 3.019230769230769,
      "grad_norm": 0.057712846981354725,
      "learning_rate": 7.171802014981726e-06,
      "loss": 0.0306,
      "step": 628
    },
    {
      "epoch": 3.0240384615384617,
      "grad_norm": 0.07986035001364755,
      "learning_rate": 7.141921642138025e-06,
      "loss": 0.0321,
      "step": 629
    },
    {
      "epoch": 3.0288461538461537,
      "grad_norm": 0.05172988667322188,
      "learning_rate": 7.112069031403704e-06,
      "loss": 0.0396,
      "step": 630
    },
    {
      "epoch": 3.0336538461538463,
      "grad_norm": 0.0642918605142809,
      "learning_rate": 7.082244472753823e-06,
      "loss": 0.0372,
      "step": 631
    },
    {
      "epoch": 3.0384615384615383,
      "grad_norm": 0.09701914815409507,
      "learning_rate": 7.052448255890958e-06,
      "loss": 0.0377,
      "step": 632
    },
    {
      "epoch": 3.043269230769231,
      "grad_norm": 0.06590568891841105,
      "learning_rate": 7.022680670242387e-06,
      "loss": 0.0395,
      "step": 633
    },
    {
      "epoch": 3.048076923076923,
      "grad_norm": 0.0530010283883266,
      "learning_rate": 6.992942004957271e-06,
      "loss": 0.027,
      "step": 634
    },
    {
      "epoch": 3.0528846153846154,
      "grad_norm": 0.05224147759051235,
      "learning_rate": 6.963232548903853e-06,
      "loss": 0.0315,
      "step": 635
    },
    {
      "epoch": 3.0576923076923075,
      "grad_norm": 0.062312447774011806,
      "learning_rate": 6.933552590666659e-06,
      "loss": 0.0302,
      "step": 636
    },
    {
      "epoch": 3.0625,
      "grad_norm": 0.08410810647196987,
      "learning_rate": 6.903902418543671e-06,
      "loss": 0.0367,
      "step": 637
    },
    {
      "epoch": 3.0673076923076925,
      "grad_norm": 0.04333405352978138,
      "learning_rate": 6.874282320543557e-06,
      "loss": 0.0281,
      "step": 638
    },
    {
      "epoch": 3.0721153846153846,
      "grad_norm": 0.05773667880964415,
      "learning_rate": 6.844692584382848e-06,
      "loss": 0.0326,
      "step": 639
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 0.046691838609415844,
      "learning_rate": 6.815133497483157e-06,
      "loss": 0.029,
      "step": 640
    },
    {
      "epoch": 3.081730769230769,
      "grad_norm": 0.09509194295275586,
      "learning_rate": 6.785605346968387e-06,
      "loss": 0.0364,
      "step": 641
    },
    {
      "epoch": 3.0865384615384617,
      "grad_norm": 0.16116070013074998,
      "learning_rate": 6.7561084196619306e-06,
      "loss": 0.0357,
      "step": 642
    },
    {
      "epoch": 3.0913461538461537,
      "grad_norm": 0.05416265630901188,
      "learning_rate": 6.7266430020839e-06,
      "loss": 0.0303,
      "step": 643
    },
    {
      "epoch": 3.0961538461538463,
      "grad_norm": 0.06572078528686934,
      "learning_rate": 6.697209380448333e-06,
      "loss": 0.0331,
      "step": 644
    },
    {
      "epoch": 3.1009615384615383,
      "grad_norm": 0.055533601381190414,
      "learning_rate": 6.66780784066041e-06,
      "loss": 0.0342,
      "step": 645
    },
    {
      "epoch": 3.105769230769231,
      "grad_norm": 0.11361913593739617,
      "learning_rate": 6.638438668313695e-06,
      "loss": 0.0359,
      "step": 646
    },
    {
      "epoch": 3.110576923076923,
      "grad_norm": 0.06687260932031226,
      "learning_rate": 6.609102148687333e-06,
      "loss": 0.0282,
      "step": 647
    },
    {
      "epoch": 3.1153846153846154,
      "grad_norm": 0.09569130704314735,
      "learning_rate": 6.579798566743314e-06,
      "loss": 0.0381,
      "step": 648
    },
    {
      "epoch": 3.1201923076923075,
      "grad_norm": 0.09218757343861024,
      "learning_rate": 6.550528207123667e-06,
      "loss": 0.0316,
      "step": 649
    },
    {
      "epoch": 3.125,
      "grad_norm": 0.07363990238463801,
      "learning_rate": 6.521291354147727e-06,
      "loss": 0.037,
      "step": 650
    },
    {
      "epoch": 3.1298076923076925,
      "grad_norm": 0.05830264561990202,
      "learning_rate": 6.492088291809355e-06,
      "loss": 0.0359,
      "step": 651
    },
    {
      "epoch": 3.1346153846153846,
      "grad_norm": 0.08127277161762296,
      "learning_rate": 6.462919303774186e-06,
      "loss": 0.036,
      "step": 652
    },
    {
      "epoch": 3.139423076923077,
      "grad_norm": 0.08558512573017549,
      "learning_rate": 6.43378467337687e-06,
      "loss": 0.044,
      "step": 653
    },
    {
      "epoch": 3.144230769230769,
      "grad_norm": 0.06866156042826069,
      "learning_rate": 6.404684683618325e-06,
      "loss": 0.0359,
      "step": 654
    },
    {
      "epoch": 3.1490384615384617,
      "grad_norm": 0.35217307623121397,
      "learning_rate": 6.375619617162985e-06,
      "loss": 0.0414,
      "step": 655
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 0.05040332477438692,
      "learning_rate": 6.34658975633605e-06,
      "loss": 0.0361,
      "step": 656
    },
    {
      "epoch": 3.1586538461538463,
      "grad_norm": 0.049859783339357505,
      "learning_rate": 6.317595383120756e-06,
      "loss": 0.0348,
      "step": 657
    },
    {
      "epoch": 3.1634615384615383,
      "grad_norm": 0.05593342936714023,
      "learning_rate": 6.288636779155621e-06,
      "loss": 0.0308,
      "step": 658
    },
    {
      "epoch": 3.168269230769231,
      "grad_norm": 0.05974687709029862,
      "learning_rate": 6.2597142257317185e-06,
      "loss": 0.0295,
      "step": 659
    },
    {
      "epoch": 3.173076923076923,
      "grad_norm": 0.32555580136544565,
      "learning_rate": 6.230828003789949e-06,
      "loss": 0.0459,
      "step": 660
    },
    {
      "epoch": 3.1778846153846154,
      "grad_norm": 0.053611264050136206,
      "learning_rate": 6.201978393918291e-06,
      "loss": 0.0294,
      "step": 661
    },
    {
      "epoch": 3.1826923076923075,
      "grad_norm": 0.06571173858650159,
      "learning_rate": 6.173165676349103e-06,
      "loss": 0.0287,
      "step": 662
    },
    {
      "epoch": 3.1875,
      "grad_norm": 0.1330057399762646,
      "learning_rate": 6.144390130956384e-06,
      "loss": 0.0437,
      "step": 663
    },
    {
      "epoch": 3.1923076923076925,
      "grad_norm": 0.0643815672688259,
      "learning_rate": 6.115652037253054e-06,
      "loss": 0.0436,
      "step": 664
    },
    {
      "epoch": 3.1971153846153846,
      "grad_norm": 0.041714931382997346,
      "learning_rate": 6.086951674388252e-06,
      "loss": 0.0272,
      "step": 665
    },
    {
      "epoch": 3.201923076923077,
      "grad_norm": 0.10547890160670044,
      "learning_rate": 6.058289321144608e-06,
      "loss": 0.0425,
      "step": 666
    },
    {
      "epoch": 3.206730769230769,
      "grad_norm": 0.05132401124734449,
      "learning_rate": 6.02966525593555e-06,
      "loss": 0.0386,
      "step": 667
    },
    {
      "epoch": 3.2115384615384617,
      "grad_norm": 0.05167160025694013,
      "learning_rate": 6.001079756802592e-06,
      "loss": 0.0353,
      "step": 668
    },
    {
      "epoch": 3.2163461538461537,
      "grad_norm": 0.057911747818796176,
      "learning_rate": 5.97253310141263e-06,
      "loss": 0.0282,
      "step": 669
    },
    {
      "epoch": 3.2211538461538463,
      "grad_norm": 0.07429822610417894,
      "learning_rate": 5.944025567055251e-06,
      "loss": 0.03,
      "step": 670
    },
    {
      "epoch": 3.2259615384615383,
      "grad_norm": 0.10024734146222425,
      "learning_rate": 5.91555743064004e-06,
      "loss": 0.0298,
      "step": 671
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 0.07339953416721513,
      "learning_rate": 5.887128968693887e-06,
      "loss": 0.0339,
      "step": 672
    },
    {
      "epoch": 3.235576923076923,
      "grad_norm": 0.06641613589455411,
      "learning_rate": 5.858740457358298e-06,
      "loss": 0.0414,
      "step": 673
    },
    {
      "epoch": 3.2403846153846154,
      "grad_norm": 0.07244046110377167,
      "learning_rate": 5.830392172386723e-06,
      "loss": 0.0361,
      "step": 674
    },
    {
      "epoch": 3.2451923076923075,
      "grad_norm": 0.056761219719604857,
      "learning_rate": 5.802084389141862e-06,
      "loss": 0.0297,
      "step": 675
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.058260076041624954,
      "learning_rate": 5.773817382593008e-06,
      "loss": 0.0325,
      "step": 676
    },
    {
      "epoch": 3.2548076923076925,
      "grad_norm": 0.1604986109445373,
      "learning_rate": 5.745591427313365e-06,
      "loss": 0.0276,
      "step": 677
    },
    {
      "epoch": 3.2596153846153846,
      "grad_norm": 0.04709863901339315,
      "learning_rate": 5.717406797477371e-06,
      "loss": 0.0285,
      "step": 678
    },
    {
      "epoch": 3.264423076923077,
      "grad_norm": 0.0944499674031931,
      "learning_rate": 5.689263766858072e-06,
      "loss": 0.0301,
      "step": 679
    },
    {
      "epoch": 3.269230769230769,
      "grad_norm": 0.22995615076481438,
      "learning_rate": 5.66116260882442e-06,
      "loss": 0.0501,
      "step": 680
    },
    {
      "epoch": 3.2740384615384617,
      "grad_norm": 0.05493828220745707,
      "learning_rate": 5.633103596338631e-06,
      "loss": 0.0302,
      "step": 681
    },
    {
      "epoch": 3.2788461538461537,
      "grad_norm": 0.05911621094003557,
      "learning_rate": 5.6050870019535496e-06,
      "loss": 0.0348,
      "step": 682
    },
    {
      "epoch": 3.2836538461538463,
      "grad_norm": 0.04990912410603396,
      "learning_rate": 5.5771130978099896e-06,
      "loss": 0.0338,
      "step": 683
    },
    {
      "epoch": 3.2884615384615383,
      "grad_norm": 0.07947294692699122,
      "learning_rate": 5.549182155634076e-06,
      "loss": 0.0299,
      "step": 684
    },
    {
      "epoch": 3.293269230769231,
      "grad_norm": 0.08571212858270479,
      "learning_rate": 5.521294446734637e-06,
      "loss": 0.0331,
      "step": 685
    },
    {
      "epoch": 3.298076923076923,
      "grad_norm": 0.09555170625259642,
      "learning_rate": 5.493450242000546e-06,
      "loss": 0.0339,
      "step": 686
    },
    {
      "epoch": 3.3028846153846154,
      "grad_norm": 0.09928495543154953,
      "learning_rate": 5.465649811898098e-06,
      "loss": 0.0251,
      "step": 687
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 0.04162789525223076,
      "learning_rate": 5.43789342646837e-06,
      "loss": 0.0271,
      "step": 688
    },
    {
      "epoch": 3.3125,
      "grad_norm": 0.045641625343782216,
      "learning_rate": 5.410181355324622e-06,
      "loss": 0.0219,
      "step": 689
    },
    {
      "epoch": 3.3173076923076925,
      "grad_norm": 0.06707309136549945,
      "learning_rate": 5.382513867649663e-06,
      "loss": 0.0304,
      "step": 690
    },
    {
      "epoch": 3.3221153846153846,
      "grad_norm": 0.05433291974573126,
      "learning_rate": 5.354891232193225e-06,
      "loss": 0.0313,
      "step": 691
    },
    {
      "epoch": 3.326923076923077,
      "grad_norm": 0.14226264827291266,
      "learning_rate": 5.32731371726938e-06,
      "loss": 0.04,
      "step": 692
    },
    {
      "epoch": 3.331730769230769,
      "grad_norm": 0.06675532011929648,
      "learning_rate": 5.299781590753916e-06,
      "loss": 0.0334,
      "step": 693
    },
    {
      "epoch": 3.3365384615384617,
      "grad_norm": 0.061156654170012203,
      "learning_rate": 5.2722951200817315e-06,
      "loss": 0.0419,
      "step": 694
    },
    {
      "epoch": 3.3413461538461537,
      "grad_norm": 0.07645667031169676,
      "learning_rate": 5.244854572244249e-06,
      "loss": 0.0371,
      "step": 695
    },
    {
      "epoch": 3.3461538461538463,
      "grad_norm": 0.12561261967477294,
      "learning_rate": 5.217460213786822e-06,
      "loss": 0.0414,
      "step": 696
    },
    {
      "epoch": 3.3509615384615383,
      "grad_norm": 0.05263549355685997,
      "learning_rate": 5.190112310806126e-06,
      "loss": 0.0278,
      "step": 697
    },
    {
      "epoch": 3.355769230769231,
      "grad_norm": 0.07312632794271588,
      "learning_rate": 5.1628111289476025e-06,
      "loss": 0.0304,
      "step": 698
    },
    {
      "epoch": 3.360576923076923,
      "grad_norm": 0.06673570955086518,
      "learning_rate": 5.135556933402862e-06,
      "loss": 0.0292,
      "step": 699
    },
    {
      "epoch": 3.3653846153846154,
      "grad_norm": 0.06513293395396459,
      "learning_rate": 5.108349988907111e-06,
      "loss": 0.0303,
      "step": 700
    },
    {
      "epoch": 3.3701923076923075,
      "grad_norm": 0.05230886449616253,
      "learning_rate": 5.081190559736569e-06,
      "loss": 0.0296,
      "step": 701
    },
    {
      "epoch": 3.375,
      "grad_norm": 0.05186425040631229,
      "learning_rate": 5.054078909705926e-06,
      "loss": 0.0334,
      "step": 702
    },
    {
      "epoch": 3.3798076923076925,
      "grad_norm": 0.04719027550514939,
      "learning_rate": 5.027015302165768e-06,
      "loss": 0.0238,
      "step": 703
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 0.1149701438221525,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.0354,
      "step": 704
    },
    {
      "epoch": 3.389423076923077,
      "grad_norm": 0.09393420053934837,
      "learning_rate": 4.973033265623333e-06,
      "loss": 0.0346,
      "step": 705
    },
    {
      "epoch": 3.394230769230769,
      "grad_norm": 0.05037880383322261,
      "learning_rate": 4.946115360978696e-06,
      "loss": 0.0231,
      "step": 706
    },
    {
      "epoch": 3.3990384615384617,
      "grad_norm": 0.0493757891590856,
      "learning_rate": 4.919246547534709e-06,
      "loss": 0.0274,
      "step": 707
    },
    {
      "epoch": 3.4038461538461537,
      "grad_norm": 0.0456814084484306,
      "learning_rate": 4.892427086283147e-06,
      "loss": 0.0223,
      "step": 708
    },
    {
      "epoch": 3.4086538461538463,
      "grad_norm": 0.06182236691164266,
      "learning_rate": 4.865657237736397e-06,
      "loss": 0.0345,
      "step": 709
    },
    {
      "epoch": 3.4134615384615383,
      "grad_norm": 0.06720503295405543,
      "learning_rate": 4.838937261924933e-06,
      "loss": 0.036,
      "step": 710
    },
    {
      "epoch": 3.418269230769231,
      "grad_norm": 0.054028040268877545,
      "learning_rate": 4.812267418394784e-06,
      "loss": 0.0342,
      "step": 711
    },
    {
      "epoch": 3.423076923076923,
      "grad_norm": 0.11004021325990243,
      "learning_rate": 4.78564796620502e-06,
      "loss": 0.0419,
      "step": 712
    },
    {
      "epoch": 3.4278846153846154,
      "grad_norm": 0.05032986584673471,
      "learning_rate": 4.759079163925223e-06,
      "loss": 0.0295,
      "step": 713
    },
    {
      "epoch": 3.4326923076923075,
      "grad_norm": 0.11974045429080703,
      "learning_rate": 4.732561269632992e-06,
      "loss": 0.0329,
      "step": 714
    },
    {
      "epoch": 3.4375,
      "grad_norm": 0.08905893065125346,
      "learning_rate": 4.706094540911429e-06,
      "loss": 0.0277,
      "step": 715
    },
    {
      "epoch": 3.4423076923076925,
      "grad_norm": 0.06985510167187259,
      "learning_rate": 4.679679234846636e-06,
      "loss": 0.0354,
      "step": 716
    },
    {
      "epoch": 3.4471153846153846,
      "grad_norm": 0.04346205560668189,
      "learning_rate": 4.6533156080252076e-06,
      "loss": 0.0261,
      "step": 717
    },
    {
      "epoch": 3.451923076923077,
      "grad_norm": 0.1291607928989774,
      "learning_rate": 4.627003916531761e-06,
      "loss": 0.0339,
      "step": 718
    },
    {
      "epoch": 3.456730769230769,
      "grad_norm": 0.05004795093330168,
      "learning_rate": 4.600744415946438e-06,
      "loss": 0.0309,
      "step": 719
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 0.0576787401124099,
      "learning_rate": 4.5745373613424075e-06,
      "loss": 0.0339,
      "step": 720
    },
    {
      "epoch": 3.4663461538461537,
      "grad_norm": 0.08134815132715928,
      "learning_rate": 4.548383007283412e-06,
      "loss": 0.0378,
      "step": 721
    },
    {
      "epoch": 3.4711538461538463,
      "grad_norm": 0.07221365652566729,
      "learning_rate": 4.522281607821288e-06,
      "loss": 0.034,
      "step": 722
    },
    {
      "epoch": 3.4759615384615383,
      "grad_norm": 0.09146974418826424,
      "learning_rate": 4.496233416493481e-06,
      "loss": 0.0324,
      "step": 723
    },
    {
      "epoch": 3.480769230769231,
      "grad_norm": 0.06297853133672977,
      "learning_rate": 4.470238686320606e-06,
      "loss": 0.0282,
      "step": 724
    },
    {
      "epoch": 3.485576923076923,
      "grad_norm": 0.055590346955556036,
      "learning_rate": 4.444297669803981e-06,
      "loss": 0.0276,
      "step": 725
    },
    {
      "epoch": 3.4903846153846154,
      "grad_norm": 0.04909080131951138,
      "learning_rate": 4.418410618923163e-06,
      "loss": 0.0265,
      "step": 726
    },
    {
      "epoch": 3.4951923076923075,
      "grad_norm": 0.04647909543829399,
      "learning_rate": 4.392577785133521e-06,
      "loss": 0.0269,
      "step": 727
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.06130412681483204,
      "learning_rate": 4.3667994193637794e-06,
      "loss": 0.027,
      "step": 728
    },
    {
      "epoch": 3.5048076923076925,
      "grad_norm": 0.07366022583837682,
      "learning_rate": 4.3410757720135886e-06,
      "loss": 0.0349,
      "step": 729
    },
    {
      "epoch": 3.5096153846153846,
      "grad_norm": 0.05717461270807967,
      "learning_rate": 4.315407092951078e-06,
      "loss": 0.0405,
      "step": 730
    },
    {
      "epoch": 3.5144230769230766,
      "grad_norm": 0.09138446151407764,
      "learning_rate": 4.289793631510449e-06,
      "loss": 0.0331,
      "step": 731
    },
    {
      "epoch": 3.519230769230769,
      "grad_norm": 0.04852584293336356,
      "learning_rate": 4.264235636489542e-06,
      "loss": 0.0315,
      "step": 732
    },
    {
      "epoch": 3.5240384615384617,
      "grad_norm": 0.23672799911788228,
      "learning_rate": 4.238733356147414e-06,
      "loss": 0.0367,
      "step": 733
    },
    {
      "epoch": 3.5288461538461537,
      "grad_norm": 0.06038147003644638,
      "learning_rate": 4.213287038201943e-06,
      "loss": 0.0269,
      "step": 734
    },
    {
      "epoch": 3.5336538461538463,
      "grad_norm": 0.09226724154218877,
      "learning_rate": 4.187896929827414e-06,
      "loss": 0.0321,
      "step": 735
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 0.045908190653970636,
      "learning_rate": 4.162563277652104e-06,
      "loss": 0.0229,
      "step": 736
    },
    {
      "epoch": 3.543269230769231,
      "grad_norm": 0.09099979358886393,
      "learning_rate": 4.137286327755913e-06,
      "loss": 0.0324,
      "step": 737
    },
    {
      "epoch": 3.5480769230769234,
      "grad_norm": 0.05858919608600158,
      "learning_rate": 4.112066325667954e-06,
      "loss": 0.027,
      "step": 738
    },
    {
      "epoch": 3.5528846153846154,
      "grad_norm": 0.043880286593645526,
      "learning_rate": 4.086903516364179e-06,
      "loss": 0.026,
      "step": 739
    },
    {
      "epoch": 3.5576923076923075,
      "grad_norm": 0.05191678714909221,
      "learning_rate": 4.061798144264986e-06,
      "loss": 0.0314,
      "step": 740
    },
    {
      "epoch": 3.5625,
      "grad_norm": 0.14063247031167855,
      "learning_rate": 4.03675045323286e-06,
      "loss": 0.029,
      "step": 741
    },
    {
      "epoch": 3.5673076923076925,
      "grad_norm": 0.07203575300873699,
      "learning_rate": 4.0117606865699975e-06,
      "loss": 0.0318,
      "step": 742
    },
    {
      "epoch": 3.5721153846153846,
      "grad_norm": 0.059588709317263636,
      "learning_rate": 3.986829087015941e-06,
      "loss": 0.022,
      "step": 743
    },
    {
      "epoch": 3.5769230769230766,
      "grad_norm": 0.06557946110313137,
      "learning_rate": 3.961955896745224e-06,
      "loss": 0.0317,
      "step": 744
    },
    {
      "epoch": 3.581730769230769,
      "grad_norm": 0.102030334302982,
      "learning_rate": 3.937141357365023e-06,
      "loss": 0.0492,
      "step": 745
    },
    {
      "epoch": 3.5865384615384617,
      "grad_norm": 0.08120004179232553,
      "learning_rate": 3.912385709912794e-06,
      "loss": 0.0271,
      "step": 746
    },
    {
      "epoch": 3.5913461538461537,
      "grad_norm": 0.060249214478860995,
      "learning_rate": 3.887689194853951e-06,
      "loss": 0.0288,
      "step": 747
    },
    {
      "epoch": 3.5961538461538463,
      "grad_norm": 0.0613423527873444,
      "learning_rate": 3.8630520520795275e-06,
      "loss": 0.0325,
      "step": 748
    },
    {
      "epoch": 3.6009615384615383,
      "grad_norm": 0.056170299950328295,
      "learning_rate": 3.838474520903825e-06,
      "loss": 0.0334,
      "step": 749
    },
    {
      "epoch": 3.605769230769231,
      "grad_norm": 0.166489820603583,
      "learning_rate": 3.8139568400621184e-06,
      "loss": 0.0354,
      "step": 750
    },
    {
      "epoch": 3.6105769230769234,
      "grad_norm": 0.05378484856125202,
      "learning_rate": 3.7894992477083226e-06,
      "loss": 0.0276,
      "step": 751
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 0.0983539770608584,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 0.0383,
      "step": 752
    },
    {
      "epoch": 3.6201923076923075,
      "grad_norm": 0.07106435962265159,
      "learning_rate": 3.7407652781594094e-06,
      "loss": 0.0297,
      "step": 753
    },
    {
      "epoch": 3.625,
      "grad_norm": 0.11741800690784657,
      "learning_rate": 3.7164893743445274e-06,
      "loss": 0.0295,
      "step": 754
    },
    {
      "epoch": 3.6298076923076925,
      "grad_norm": 0.05020420600587778,
      "learning_rate": 3.692274505773419e-06,
      "loss": 0.0288,
      "step": 755
    },
    {
      "epoch": 3.6346153846153846,
      "grad_norm": 0.10467200397428557,
      "learning_rate": 3.6681209076586035e-06,
      "loss": 0.0408,
      "step": 756
    },
    {
      "epoch": 3.6394230769230766,
      "grad_norm": 0.06998322080100834,
      "learning_rate": 3.644028814617454e-06,
      "loss": 0.033,
      "step": 757
    },
    {
      "epoch": 3.644230769230769,
      "grad_norm": 0.046521465319213216,
      "learning_rate": 3.619998460669916e-06,
      "loss": 0.0234,
      "step": 758
    },
    {
      "epoch": 3.6490384615384617,
      "grad_norm": 0.06309688269351324,
      "learning_rate": 3.5960300792362124e-06,
      "loss": 0.0308,
      "step": 759
    },
    {
      "epoch": 3.6538461538461537,
      "grad_norm": 0.07953731609834738,
      "learning_rate": 3.5721239031346067e-06,
      "loss": 0.0321,
      "step": 760
    },
    {
      "epoch": 3.6586538461538463,
      "grad_norm": 0.051043791682097696,
      "learning_rate": 3.5482801645791266e-06,
      "loss": 0.0218,
      "step": 761
    },
    {
      "epoch": 3.6634615384615383,
      "grad_norm": 0.10413694970627424,
      "learning_rate": 3.5244990951772972e-06,
      "loss": 0.0328,
      "step": 762
    },
    {
      "epoch": 3.668269230769231,
      "grad_norm": 0.04258105685679391,
      "learning_rate": 3.5007809259279146e-06,
      "loss": 0.0269,
      "step": 763
    },
    {
      "epoch": 3.6730769230769234,
      "grad_norm": 0.05093323854741505,
      "learning_rate": 3.4771258872187917e-06,
      "loss": 0.0234,
      "step": 764
    },
    {
      "epoch": 3.6778846153846154,
      "grad_norm": 0.07297196548457857,
      "learning_rate": 3.453534208824507e-06,
      "loss": 0.0313,
      "step": 765
    },
    {
      "epoch": 3.6826923076923075,
      "grad_norm": 0.08279635789178397,
      "learning_rate": 3.4300061199041967e-06,
      "loss": 0.0221,
      "step": 766
    },
    {
      "epoch": 3.6875,
      "grad_norm": 0.05425110315837412,
      "learning_rate": 3.4065418489993118e-06,
      "loss": 0.0296,
      "step": 767
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 0.051222574834104695,
      "learning_rate": 3.3831416240314085e-06,
      "loss": 0.0265,
      "step": 768
    },
    {
      "epoch": 3.6971153846153846,
      "grad_norm": 0.04735295201063496,
      "learning_rate": 3.3598056722999185e-06,
      "loss": 0.0312,
      "step": 769
    },
    {
      "epoch": 3.7019230769230766,
      "grad_norm": 0.05734040998401968,
      "learning_rate": 3.3365342204799613e-06,
      "loss": 0.0307,
      "step": 770
    },
    {
      "epoch": 3.706730769230769,
      "grad_norm": 0.09490367775069944,
      "learning_rate": 3.3133274946201333e-06,
      "loss": 0.0307,
      "step": 771
    },
    {
      "epoch": 3.7115384615384617,
      "grad_norm": 0.40617172926167705,
      "learning_rate": 3.290185720140301e-06,
      "loss": 0.0515,
      "step": 772
    },
    {
      "epoch": 3.7163461538461537,
      "grad_norm": 0.05640299310431724,
      "learning_rate": 3.267109121829428e-06,
      "loss": 0.0268,
      "step": 773
    },
    {
      "epoch": 3.7211538461538463,
      "grad_norm": 0.056949116957695604,
      "learning_rate": 3.2440979238433977e-06,
      "loss": 0.0331,
      "step": 774
    },
    {
      "epoch": 3.7259615384615383,
      "grad_norm": 0.07950213821160639,
      "learning_rate": 3.221152349702802e-06,
      "loss": 0.0267,
      "step": 775
    },
    {
      "epoch": 3.730769230769231,
      "grad_norm": 0.07313377310238871,
      "learning_rate": 3.1982726222908046e-06,
      "loss": 0.0309,
      "step": 776
    },
    {
      "epoch": 3.7355769230769234,
      "grad_norm": 0.10308318096719848,
      "learning_rate": 3.1754589638509647e-06,
      "loss": 0.0326,
      "step": 777
    },
    {
      "epoch": 3.7403846153846154,
      "grad_norm": 0.09486086833260159,
      "learning_rate": 3.152711595985065e-06,
      "loss": 0.0306,
      "step": 778
    },
    {
      "epoch": 3.7451923076923075,
      "grad_norm": 0.06285887512306194,
      "learning_rate": 3.1300307396509833e-06,
      "loss": 0.0271,
      "step": 779
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.04414241431703045,
      "learning_rate": 3.10741661516053e-06,
      "loss": 0.0213,
      "step": 780
    },
    {
      "epoch": 3.7548076923076925,
      "grad_norm": 0.12087721669229565,
      "learning_rate": 3.0848694421773075e-06,
      "loss": 0.0349,
      "step": 781
    },
    {
      "epoch": 3.7596153846153846,
      "grad_norm": 0.055280060530026674,
      "learning_rate": 3.0623894397145837e-06,
      "loss": 0.0265,
      "step": 782
    },
    {
      "epoch": 3.7644230769230766,
      "grad_norm": 0.12913002612211152,
      "learning_rate": 3.0399768261331664e-06,
      "loss": 0.0373,
      "step": 783
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 0.0577486077182293,
      "learning_rate": 3.017631819139273e-06,
      "loss": 0.0356,
      "step": 784
    },
    {
      "epoch": 3.7740384615384617,
      "grad_norm": 0.05176864866434295,
      "learning_rate": 2.995354635782417e-06,
      "loss": 0.0277,
      "step": 785
    },
    {
      "epoch": 3.7788461538461537,
      "grad_norm": 0.08707112747290696,
      "learning_rate": 2.9731454924533086e-06,
      "loss": 0.0307,
      "step": 786
    },
    {
      "epoch": 3.7836538461538463,
      "grad_norm": 0.06298572340487642,
      "learning_rate": 2.95100460488175e-06,
      "loss": 0.0345,
      "step": 787
    },
    {
      "epoch": 3.7884615384615383,
      "grad_norm": 0.11080398488086161,
      "learning_rate": 2.9289321881345257e-06,
      "loss": 0.0337,
      "step": 788
    },
    {
      "epoch": 3.793269230769231,
      "grad_norm": 0.053356536069151805,
      "learning_rate": 2.906928456613336e-06,
      "loss": 0.0265,
      "step": 789
    },
    {
      "epoch": 3.7980769230769234,
      "grad_norm": 0.07254064051324988,
      "learning_rate": 2.884993624052701e-06,
      "loss": 0.0299,
      "step": 790
    },
    {
      "epoch": 3.8028846153846154,
      "grad_norm": 0.1434750958032174,
      "learning_rate": 2.8631279035178796e-06,
      "loss": 0.0429,
      "step": 791
    },
    {
      "epoch": 3.8076923076923075,
      "grad_norm": 0.0968255550393328,
      "learning_rate": 2.8413315074028157e-06,
      "loss": 0.0306,
      "step": 792
    },
    {
      "epoch": 3.8125,
      "grad_norm": 0.052531743341119604,
      "learning_rate": 2.819604647428067e-06,
      "loss": 0.0261,
      "step": 793
    },
    {
      "epoch": 3.8173076923076925,
      "grad_norm": 0.08334124495357187,
      "learning_rate": 2.7979475346387363e-06,
      "loss": 0.0298,
      "step": 794
    },
    {
      "epoch": 3.8221153846153846,
      "grad_norm": 0.07042433543360259,
      "learning_rate": 2.776360379402445e-06,
      "loss": 0.0287,
      "step": 795
    },
    {
      "epoch": 3.8269230769230766,
      "grad_norm": 0.06120252225618132,
      "learning_rate": 2.7548433914072736e-06,
      "loss": 0.0323,
      "step": 796
    },
    {
      "epoch": 3.831730769230769,
      "grad_norm": 0.050101525027078216,
      "learning_rate": 2.7333967796597317e-06,
      "loss": 0.0282,
      "step": 797
    },
    {
      "epoch": 3.8365384615384617,
      "grad_norm": 0.08489458241493845,
      "learning_rate": 2.712020752482717e-06,
      "loss": 0.036,
      "step": 798
    },
    {
      "epoch": 3.8413461538461537,
      "grad_norm": 0.04054422655718979,
      "learning_rate": 2.690715517513508e-06,
      "loss": 0.023,
      "step": 799
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 0.09730865356780485,
      "learning_rate": 2.669481281701739e-06,
      "loss": 0.0386,
      "step": 800
    },
    {
      "epoch": 3.8509615384615383,
      "grad_norm": 0.10083822635745084,
      "learning_rate": 2.6483182513073835e-06,
      "loss": 0.034,
      "step": 801
    },
    {
      "epoch": 3.855769230769231,
      "grad_norm": 0.09176294870742983,
      "learning_rate": 2.6272266318987606e-06,
      "loss": 0.0394,
      "step": 802
    },
    {
      "epoch": 3.8605769230769234,
      "grad_norm": 0.05406629976461731,
      "learning_rate": 2.6062066283505404e-06,
      "loss": 0.0301,
      "step": 803
    },
    {
      "epoch": 3.8653846153846154,
      "grad_norm": 0.06945330473451089,
      "learning_rate": 2.5852584448417327e-06,
      "loss": 0.0277,
      "step": 804
    },
    {
      "epoch": 3.8701923076923075,
      "grad_norm": 0.07501488739997,
      "learning_rate": 2.564382284853738e-06,
      "loss": 0.0274,
      "step": 805
    },
    {
      "epoch": 3.875,
      "grad_norm": 0.06239479536424768,
      "learning_rate": 2.5435783511683444e-06,
      "loss": 0.0284,
      "step": 806
    },
    {
      "epoch": 3.8798076923076925,
      "grad_norm": 0.0816880784756752,
      "learning_rate": 2.5228468458657585e-06,
      "loss": 0.0369,
      "step": 807
    },
    {
      "epoch": 3.8846153846153846,
      "grad_norm": 0.09669863927326834,
      "learning_rate": 2.502187970322657e-06,
      "loss": 0.0304,
      "step": 808
    },
    {
      "epoch": 3.8894230769230766,
      "grad_norm": 0.06007172727441471,
      "learning_rate": 2.4816019252102274e-06,
      "loss": 0.0271,
      "step": 809
    },
    {
      "epoch": 3.894230769230769,
      "grad_norm": 0.06085883111827961,
      "learning_rate": 2.461088910492202e-06,
      "loss": 0.0299,
      "step": 810
    },
    {
      "epoch": 3.8990384615384617,
      "grad_norm": 0.10084722312993903,
      "learning_rate": 2.440649125422937e-06,
      "loss": 0.0325,
      "step": 811
    },
    {
      "epoch": 3.9038461538461537,
      "grad_norm": 0.17722249333881146,
      "learning_rate": 2.420282768545469e-06,
      "loss": 0.0368,
      "step": 812
    },
    {
      "epoch": 3.9086538461538463,
      "grad_norm": 0.08900484454099358,
      "learning_rate": 2.3999900376895844e-06,
      "loss": 0.0309,
      "step": 813
    },
    {
      "epoch": 3.9134615384615383,
      "grad_norm": 0.07517993257700153,
      "learning_rate": 2.3797711299698924e-06,
      "loss": 0.0283,
      "step": 814
    },
    {
      "epoch": 3.918269230769231,
      "grad_norm": 0.043280883572014636,
      "learning_rate": 2.3596262417839256e-06,
      "loss": 0.0282,
      "step": 815
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 0.15930550249303463,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.0298,
      "step": 816
    },
    {
      "epoch": 3.9278846153846154,
      "grad_norm": 0.07733220377323659,
      "learning_rate": 2.319559306006417e-06,
      "loss": 0.0263,
      "step": 817
    },
    {
      "epoch": 3.9326923076923075,
      "grad_norm": 0.062864699473044,
      "learning_rate": 2.2996376476073724e-06,
      "loss": 0.027,
      "step": 818
    },
    {
      "epoch": 3.9375,
      "grad_norm": 0.14876568896415318,
      "learning_rate": 2.2797907871232673e-06,
      "loss": 0.0314,
      "step": 819
    },
    {
      "epoch": 3.9423076923076925,
      "grad_norm": 0.04450853794166596,
      "learning_rate": 2.2600189173377263e-06,
      "loss": 0.0205,
      "step": 820
    },
    {
      "epoch": 3.9471153846153846,
      "grad_norm": 0.4033692556676445,
      "learning_rate": 2.240322230305951e-06,
      "loss": 0.0417,
      "step": 821
    },
    {
      "epoch": 3.9519230769230766,
      "grad_norm": 0.07478893590639861,
      "learning_rate": 2.2207009173528528e-06,
      "loss": 0.0357,
      "step": 822
    },
    {
      "epoch": 3.956730769230769,
      "grad_norm": 0.09502322971523484,
      "learning_rate": 2.201155169071184e-06,
      "loss": 0.03,
      "step": 823
    },
    {
      "epoch": 3.9615384615384617,
      "grad_norm": 0.04448878091064718,
      "learning_rate": 2.1816851753197023e-06,
      "loss": 0.0216,
      "step": 824
    },
    {
      "epoch": 3.9663461538461537,
      "grad_norm": 0.07589036590400781,
      "learning_rate": 2.1622911252213195e-06,
      "loss": 0.0302,
      "step": 825
    },
    {
      "epoch": 3.9711538461538463,
      "grad_norm": 0.05427309952785381,
      "learning_rate": 2.1429732071612653e-06,
      "loss": 0.0246,
      "step": 826
    },
    {
      "epoch": 3.9759615384615383,
      "grad_norm": 0.05310781311516653,
      "learning_rate": 2.1237316087852465e-06,
      "loss": 0.0269,
      "step": 827
    },
    {
      "epoch": 3.980769230769231,
      "grad_norm": 0.08783997760186335,
      "learning_rate": 2.104566516997647e-06,
      "loss": 0.0326,
      "step": 828
    },
    {
      "epoch": 3.9855769230769234,
      "grad_norm": 0.515831675920274,
      "learning_rate": 2.0854781179596937e-06,
      "loss": 0.0552,
      "step": 829
    },
    {
      "epoch": 3.9903846153846154,
      "grad_norm": 0.23100949318452618,
      "learning_rate": 2.0664665970876496e-06,
      "loss": 0.0325,
      "step": 830
    },
    {
      "epoch": 3.9951923076923075,
      "grad_norm": 0.07456234944932776,
      "learning_rate": 2.0475321390510262e-06,
      "loss": 0.0305,
      "step": 831
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.0714027702047576,
      "learning_rate": 2.0286749277707783e-06,
      "loss": 0.031,
      "step": 832
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.02969340793788433,
      "eval_runtime": 671.2432,
      "eval_samples_per_second": 0.62,
      "eval_steps_per_second": 0.62,
      "step": 832
    },
    {
      "epoch": 4.0048076923076925,
      "grad_norm": 0.3418657319063206,
      "learning_rate": 2.009895146417512e-06,
      "loss": 0.0429,
      "step": 833
    },
    {
      "epoch": 4.009615384615385,
      "grad_norm": 0.09658384283251628,
      "learning_rate": 1.9911929774097216e-06,
      "loss": 0.0306,
      "step": 834
    },
    {
      "epoch": 4.014423076923077,
      "grad_norm": 0.04933723040813798,
      "learning_rate": 1.9725686024120093e-06,
      "loss": 0.0286,
      "step": 835
    },
    {
      "epoch": 4.019230769230769,
      "grad_norm": 0.18701710277943945,
      "learning_rate": 1.9540222023333165e-06,
      "loss": 0.0315,
      "step": 836
    },
    {
      "epoch": 4.024038461538462,
      "grad_norm": 0.06315311188334842,
      "learning_rate": 1.9355539573251737e-06,
      "loss": 0.032,
      "step": 837
    },
    {
      "epoch": 4.028846153846154,
      "grad_norm": 0.046786286261358966,
      "learning_rate": 1.9171640467799478e-06,
      "loss": 0.029,
      "step": 838
    },
    {
      "epoch": 4.033653846153846,
      "grad_norm": 0.052566793033024146,
      "learning_rate": 1.8988526493290948e-06,
      "loss": 0.0268,
      "step": 839
    },
    {
      "epoch": 4.038461538461538,
      "grad_norm": 0.0989123152199509,
      "learning_rate": 1.880619942841435e-06,
      "loss": 0.0292,
      "step": 840
    },
    {
      "epoch": 4.043269230769231,
      "grad_norm": 0.053968308106176366,
      "learning_rate": 1.8624661044214154e-06,
      "loss": 0.0221,
      "step": 841
    },
    {
      "epoch": 4.048076923076923,
      "grad_norm": 0.052631282812709436,
      "learning_rate": 1.8443913104073984e-06,
      "loss": 0.0299,
      "step": 842
    },
    {
      "epoch": 4.052884615384615,
      "grad_norm": 0.07281533730021943,
      "learning_rate": 1.826395736369937e-06,
      "loss": 0.0399,
      "step": 843
    },
    {
      "epoch": 4.0576923076923075,
      "grad_norm": 0.056606123129885604,
      "learning_rate": 1.808479557110081e-06,
      "loss": 0.0309,
      "step": 844
    },
    {
      "epoch": 4.0625,
      "grad_norm": 0.06555411518513708,
      "learning_rate": 1.7906429466576768e-06,
      "loss": 0.0415,
      "step": 845
    },
    {
      "epoch": 4.0673076923076925,
      "grad_norm": 0.06693627299634787,
      "learning_rate": 1.7728860782696666e-06,
      "loss": 0.025,
      "step": 846
    },
    {
      "epoch": 4.072115384615385,
      "grad_norm": 0.08114882243457942,
      "learning_rate": 1.7552091244284197e-06,
      "loss": 0.029,
      "step": 847
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 0.05417043179115923,
      "learning_rate": 1.7376122568400533e-06,
      "loss": 0.0304,
      "step": 848
    },
    {
      "epoch": 4.081730769230769,
      "grad_norm": 0.05618635507575586,
      "learning_rate": 1.7200956464327512e-06,
      "loss": 0.0297,
      "step": 849
    },
    {
      "epoch": 4.086538461538462,
      "grad_norm": 0.04434348867902943,
      "learning_rate": 1.7026594633551252e-06,
      "loss": 0.0274,
      "step": 850
    },
    {
      "epoch": 4.091346153846154,
      "grad_norm": 0.06192152437323278,
      "learning_rate": 1.6853038769745466e-06,
      "loss": 0.0246,
      "step": 851
    },
    {
      "epoch": 4.096153846153846,
      "grad_norm": 0.05834566357770496,
      "learning_rate": 1.6680290558755119e-06,
      "loss": 0.0233,
      "step": 852
    },
    {
      "epoch": 4.100961538461538,
      "grad_norm": 0.053478107437899706,
      "learning_rate": 1.6508351678579882e-06,
      "loss": 0.0222,
      "step": 853
    },
    {
      "epoch": 4.105769230769231,
      "grad_norm": 0.06845126738864249,
      "learning_rate": 1.6337223799358025e-06,
      "loss": 0.0315,
      "step": 854
    },
    {
      "epoch": 4.110576923076923,
      "grad_norm": 0.041241365206339135,
      "learning_rate": 1.6166908583350138e-06,
      "loss": 0.025,
      "step": 855
    },
    {
      "epoch": 4.115384615384615,
      "grad_norm": 0.06792278356845592,
      "learning_rate": 1.599740768492286e-06,
      "loss": 0.0295,
      "step": 856
    },
    {
      "epoch": 4.1201923076923075,
      "grad_norm": 0.11501404853410911,
      "learning_rate": 1.582872275053301e-06,
      "loss": 0.0324,
      "step": 857
    },
    {
      "epoch": 4.125,
      "grad_norm": 0.07288982548749953,
      "learning_rate": 1.566085541871145e-06,
      "loss": 0.0348,
      "step": 858
    },
    {
      "epoch": 4.1298076923076925,
      "grad_norm": 0.04137009367138896,
      "learning_rate": 1.5493807320047183e-06,
      "loss": 0.0249,
      "step": 859
    },
    {
      "epoch": 4.134615384615385,
      "grad_norm": 0.08716765593913729,
      "learning_rate": 1.5327580077171589e-06,
      "loss": 0.0366,
      "step": 860
    },
    {
      "epoch": 4.139423076923077,
      "grad_norm": 0.09911972418736693,
      "learning_rate": 1.5162175304742633e-06,
      "loss": 0.0292,
      "step": 861
    },
    {
      "epoch": 4.144230769230769,
      "grad_norm": 0.055932934545250945,
      "learning_rate": 1.499759460942909e-06,
      "loss": 0.0314,
      "step": 862
    },
    {
      "epoch": 4.149038461538462,
      "grad_norm": 0.1071345417800235,
      "learning_rate": 1.4833839589895072e-06,
      "loss": 0.0335,
      "step": 863
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 0.07340312558885746,
      "learning_rate": 1.467091183678444e-06,
      "loss": 0.0298,
      "step": 864
    },
    {
      "epoch": 4.158653846153846,
      "grad_norm": 0.06465059776421045,
      "learning_rate": 1.4508812932705364e-06,
      "loss": 0.0385,
      "step": 865
    },
    {
      "epoch": 4.163461538461538,
      "grad_norm": 0.04809022016066031,
      "learning_rate": 1.4347544452214869e-06,
      "loss": 0.0289,
      "step": 866
    },
    {
      "epoch": 4.168269230769231,
      "grad_norm": 0.07379171473506088,
      "learning_rate": 1.4187107961803704e-06,
      "loss": 0.0243,
      "step": 867
    },
    {
      "epoch": 4.173076923076923,
      "grad_norm": 0.08264002234257346,
      "learning_rate": 1.4027505019880972e-06,
      "loss": 0.0283,
      "step": 868
    },
    {
      "epoch": 4.177884615384615,
      "grad_norm": 0.09399857654405601,
      "learning_rate": 1.3868737176759105e-06,
      "loss": 0.0256,
      "step": 869
    },
    {
      "epoch": 4.1826923076923075,
      "grad_norm": 0.12728970822886407,
      "learning_rate": 1.3710805974638697e-06,
      "loss": 0.0318,
      "step": 870
    },
    {
      "epoch": 4.1875,
      "grad_norm": 0.059678853577289366,
      "learning_rate": 1.3553712947593655e-06,
      "loss": 0.0239,
      "step": 871
    },
    {
      "epoch": 4.1923076923076925,
      "grad_norm": 0.045097328752355004,
      "learning_rate": 1.339745962155613e-06,
      "loss": 0.0264,
      "step": 872
    },
    {
      "epoch": 4.197115384615385,
      "grad_norm": 0.07401713850009832,
      "learning_rate": 1.324204751430186e-06,
      "loss": 0.0345,
      "step": 873
    },
    {
      "epoch": 4.201923076923077,
      "grad_norm": 0.09263425618815709,
      "learning_rate": 1.3087478135435361e-06,
      "loss": 0.0298,
      "step": 874
    },
    {
      "epoch": 4.206730769230769,
      "grad_norm": 0.057366732841140564,
      "learning_rate": 1.293375298637518e-06,
      "loss": 0.0254,
      "step": 875
    },
    {
      "epoch": 4.211538461538462,
      "grad_norm": 0.07148746410886629,
      "learning_rate": 1.278087356033947e-06,
      "loss": 0.03,
      "step": 876
    },
    {
      "epoch": 4.216346153846154,
      "grad_norm": 0.05373539136404676,
      "learning_rate": 1.2628841342331389e-06,
      "loss": 0.027,
      "step": 877
    },
    {
      "epoch": 4.221153846153846,
      "grad_norm": 0.05588540123740186,
      "learning_rate": 1.2477657809124632e-06,
      "loss": 0.0303,
      "step": 878
    },
    {
      "epoch": 4.225961538461538,
      "grad_norm": 0.1567555507364754,
      "learning_rate": 1.2327324429249232e-06,
      "loss": 0.038,
      "step": 879
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 0.3989580665406895,
      "learning_rate": 1.2177842662977136e-06,
      "loss": 0.0449,
      "step": 880
    },
    {
      "epoch": 4.235576923076923,
      "grad_norm": 0.06964650843914619,
      "learning_rate": 1.2029213962308172e-06,
      "loss": 0.0294,
      "step": 881
    },
    {
      "epoch": 4.240384615384615,
      "grad_norm": 0.05506192077611758,
      "learning_rate": 1.188143977095576e-06,
      "loss": 0.0253,
      "step": 882
    },
    {
      "epoch": 4.2451923076923075,
      "grad_norm": 0.0661083565680751,
      "learning_rate": 1.1734521524333087e-06,
      "loss": 0.0241,
      "step": 883
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.04883835144776711,
      "learning_rate": 1.1588460649539036e-06,
      "loss": 0.0222,
      "step": 884
    },
    {
      "epoch": 4.2548076923076925,
      "grad_norm": 0.05644250770597906,
      "learning_rate": 1.1443258565344329e-06,
      "loss": 0.0254,
      "step": 885
    },
    {
      "epoch": 4.259615384615385,
      "grad_norm": 0.05877247595717598,
      "learning_rate": 1.129891668217783e-06,
      "loss": 0.031,
      "step": 886
    },
    {
      "epoch": 4.264423076923077,
      "grad_norm": 0.1993096691268002,
      "learning_rate": 1.1155436402112785e-06,
      "loss": 0.0296,
      "step": 887
    },
    {
      "epoch": 4.269230769230769,
      "grad_norm": 0.07020641648605166,
      "learning_rate": 1.1012819118853147e-06,
      "loss": 0.0279,
      "step": 888
    },
    {
      "epoch": 4.274038461538462,
      "grad_norm": 0.05873945244836915,
      "learning_rate": 1.0871066217720173e-06,
      "loss": 0.0294,
      "step": 889
    },
    {
      "epoch": 4.278846153846154,
      "grad_norm": 0.04993933892542076,
      "learning_rate": 1.073017907563887e-06,
      "loss": 0.0245,
      "step": 890
    },
    {
      "epoch": 4.283653846153846,
      "grad_norm": 0.10570073479628271,
      "learning_rate": 1.0590159061124606e-06,
      "loss": 0.0324,
      "step": 891
    },
    {
      "epoch": 4.288461538461538,
      "grad_norm": 0.06106710484545418,
      "learning_rate": 1.0451007534269908e-06,
      "loss": 0.0276,
      "step": 892
    },
    {
      "epoch": 4.293269230769231,
      "grad_norm": 0.08234392900855696,
      "learning_rate": 1.0312725846731174e-06,
      "loss": 0.025,
      "step": 893
    },
    {
      "epoch": 4.298076923076923,
      "grad_norm": 0.08934236083917761,
      "learning_rate": 1.0175315341715598e-06,
      "loss": 0.0303,
      "step": 894
    },
    {
      "epoch": 4.302884615384615,
      "grad_norm": 0.05895682698630586,
      "learning_rate": 1.003877735396801e-06,
      "loss": 0.0377,
      "step": 895
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 0.06314696870040204,
      "learning_rate": 9.903113209758098e-07,
      "loss": 0.0303,
      "step": 896
    },
    {
      "epoch": 4.3125,
      "grad_norm": 0.07753622811221304,
      "learning_rate": 9.768324226867353e-07,
      "loss": 0.0367,
      "step": 897
    },
    {
      "epoch": 4.3173076923076925,
      "grad_norm": 0.051460376318685226,
      "learning_rate": 9.634411714576353e-07,
      "loss": 0.0272,
      "step": 898
    },
    {
      "epoch": 4.322115384615385,
      "grad_norm": 0.09443747822780676,
      "learning_rate": 9.501376973651999e-07,
      "loss": 0.0268,
      "step": 899
    },
    {
      "epoch": 4.326923076923077,
      "grad_norm": 0.050556269034994286,
      "learning_rate": 9.369221296335007e-07,
      "loss": 0.0251,
      "step": 900
    },
    {
      "epoch": 4.331730769230769,
      "grad_norm": 0.07345359965671241,
      "learning_rate": 9.237945966327133e-07,
      "loss": 0.0336,
      "step": 901
    },
    {
      "epoch": 4.336538461538462,
      "grad_norm": 0.07291425742001394,
      "learning_rate": 9.107552258778907e-07,
      "loss": 0.0345,
      "step": 902
    },
    {
      "epoch": 4.341346153846154,
      "grad_norm": 0.04910221210992506,
      "learning_rate": 8.978041440277163e-07,
      "loss": 0.024,
      "step": 903
    },
    {
      "epoch": 4.346153846153846,
      "grad_norm": 0.045822496784374674,
      "learning_rate": 8.849414768832687e-07,
      "loss": 0.0216,
      "step": 904
    },
    {
      "epoch": 4.350961538461538,
      "grad_norm": 0.062378142605213516,
      "learning_rate": 8.721673493868111e-07,
      "loss": 0.0257,
      "step": 905
    },
    {
      "epoch": 4.355769230769231,
      "grad_norm": 0.11773772632625806,
      "learning_rate": 8.5948188562057e-07,
      "loss": 0.0287,
      "step": 906
    },
    {
      "epoch": 4.360576923076923,
      "grad_norm": 0.17724107886638832,
      "learning_rate": 8.468852088055291e-07,
      "loss": 0.0352,
      "step": 907
    },
    {
      "epoch": 4.365384615384615,
      "grad_norm": 0.06104640344866547,
      "learning_rate": 8.343774413002382e-07,
      "loss": 0.0293,
      "step": 908
    },
    {
      "epoch": 4.3701923076923075,
      "grad_norm": 0.068203694461132,
      "learning_rate": 8.219587045996203e-07,
      "loss": 0.0212,
      "step": 909
    },
    {
      "epoch": 4.375,
      "grad_norm": 0.06011565791563733,
      "learning_rate": 8.096291193337935e-07,
      "loss": 0.0298,
      "step": 910
    },
    {
      "epoch": 4.3798076923076925,
      "grad_norm": 0.050813377455517034,
      "learning_rate": 7.973888052668943e-07,
      "loss": 0.0289,
      "step": 911
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 0.07279886329353344,
      "learning_rate": 7.852378812959227e-07,
      "loss": 0.0279,
      "step": 912
    },
    {
      "epoch": 4.389423076923077,
      "grad_norm": 0.06702838445750309,
      "learning_rate": 7.731764654495832e-07,
      "loss": 0.0284,
      "step": 913
    },
    {
      "epoch": 4.394230769230769,
      "grad_norm": 0.05300957239730283,
      "learning_rate": 7.612046748871327e-07,
      "loss": 0.0255,
      "step": 914
    },
    {
      "epoch": 4.399038461538462,
      "grad_norm": 0.12752094421294227,
      "learning_rate": 7.493226258972519e-07,
      "loss": 0.026,
      "step": 915
    },
    {
      "epoch": 4.403846153846154,
      "grad_norm": 0.050264778159425996,
      "learning_rate": 7.375304338969135e-07,
      "loss": 0.0273,
      "step": 916
    },
    {
      "epoch": 4.408653846153846,
      "grad_norm": 0.053123047634663356,
      "learning_rate": 7.258282134302519e-07,
      "loss": 0.0271,
      "step": 917
    },
    {
      "epoch": 4.413461538461538,
      "grad_norm": 0.04702653152679454,
      "learning_rate": 7.142160781674645e-07,
      "loss": 0.0251,
      "step": 918
    },
    {
      "epoch": 4.418269230769231,
      "grad_norm": 0.053832930576202816,
      "learning_rate": 7.026941409036991e-07,
      "loss": 0.0251,
      "step": 919
    },
    {
      "epoch": 4.423076923076923,
      "grad_norm": 0.074824544652792,
      "learning_rate": 6.912625135579587e-07,
      "loss": 0.0256,
      "step": 920
    },
    {
      "epoch": 4.427884615384615,
      "grad_norm": 0.06970862369383037,
      "learning_rate": 6.799213071720156e-07,
      "loss": 0.032,
      "step": 921
    },
    {
      "epoch": 4.4326923076923075,
      "grad_norm": 0.08222029535392622,
      "learning_rate": 6.68670631909335e-07,
      "loss": 0.0274,
      "step": 922
    },
    {
      "epoch": 4.4375,
      "grad_norm": 0.23765391785546702,
      "learning_rate": 6.57510597054003e-07,
      "loss": 0.0398,
      "step": 923
    },
    {
      "epoch": 4.4423076923076925,
      "grad_norm": 0.04679123939674237,
      "learning_rate": 6.464413110096601e-07,
      "loss": 0.0261,
      "step": 924
    },
    {
      "epoch": 4.447115384615385,
      "grad_norm": 0.057986416616097144,
      "learning_rate": 6.354628812984576e-07,
      "loss": 0.0311,
      "step": 925
    },
    {
      "epoch": 4.451923076923077,
      "grad_norm": 0.048534286342444395,
      "learning_rate": 6.245754145600091e-07,
      "loss": 0.0291,
      "step": 926
    },
    {
      "epoch": 4.456730769230769,
      "grad_norm": 0.17969550513019583,
      "learning_rate": 6.137790165503499e-07,
      "loss": 0.0261,
      "step": 927
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 0.0728884079371602,
      "learning_rate": 6.030737921409169e-07,
      "loss": 0.0276,
      "step": 928
    },
    {
      "epoch": 4.466346153846154,
      "grad_norm": 0.08947286515391538,
      "learning_rate": 5.924598453175278e-07,
      "loss": 0.0303,
      "step": 929
    },
    {
      "epoch": 4.471153846153846,
      "grad_norm": 0.11509243516197683,
      "learning_rate": 5.819372791793654e-07,
      "loss": 0.0242,
      "step": 930
    },
    {
      "epoch": 4.475961538461538,
      "grad_norm": 0.050300163837627226,
      "learning_rate": 5.715061959379875e-07,
      "loss": 0.0236,
      "step": 931
    },
    {
      "epoch": 4.480769230769231,
      "grad_norm": 0.0576646822398393,
      "learning_rate": 5.611666969163243e-07,
      "loss": 0.0237,
      "step": 932
    },
    {
      "epoch": 4.485576923076923,
      "grad_norm": 0.1057558746964276,
      "learning_rate": 5.509188825476964e-07,
      "loss": 0.022,
      "step": 933
    },
    {
      "epoch": 4.490384615384615,
      "grad_norm": 0.09906110970347887,
      "learning_rate": 5.407628523748398e-07,
      "loss": 0.0277,
      "step": 934
    },
    {
      "epoch": 4.4951923076923075,
      "grad_norm": 0.058208871851744406,
      "learning_rate": 5.306987050489442e-07,
      "loss": 0.0334,
      "step": 935
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.06690986462557695,
      "learning_rate": 5.207265383286831e-07,
      "loss": 0.0247,
      "step": 936
    },
    {
      "epoch": 4.5048076923076925,
      "grad_norm": 0.377051517906678,
      "learning_rate": 5.108464490792753e-07,
      "loss": 0.0507,
      "step": 937
    },
    {
      "epoch": 4.509615384615385,
      "grad_norm": 0.06205858125318092,
      "learning_rate": 5.010585332715401e-07,
      "loss": 0.0218,
      "step": 938
    },
    {
      "epoch": 4.514423076923077,
      "grad_norm": 0.04778929629779314,
      "learning_rate": 4.913628859809638e-07,
      "loss": 0.0281,
      "step": 939
    },
    {
      "epoch": 4.519230769230769,
      "grad_norm": 0.05059395969685337,
      "learning_rate": 4.817596013867765e-07,
      "loss": 0.0252,
      "step": 940
    },
    {
      "epoch": 4.524038461538462,
      "grad_norm": 0.09630637131811831,
      "learning_rate": 4.7224877277103673e-07,
      "loss": 0.0329,
      "step": 941
    },
    {
      "epoch": 4.528846153846154,
      "grad_norm": 0.05131451807540809,
      "learning_rate": 4.628304925177318e-07,
      "loss": 0.0275,
      "step": 942
    },
    {
      "epoch": 4.533653846153846,
      "grad_norm": 0.0668584362600953,
      "learning_rate": 4.535048521118668e-07,
      "loss": 0.0268,
      "step": 943
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 0.10251572454070715,
      "learning_rate": 4.4427194213859216e-07,
      "loss": 0.0309,
      "step": 944
    },
    {
      "epoch": 4.543269230769231,
      "grad_norm": 0.056220383393265745,
      "learning_rate": 4.351318522823134e-07,
      "loss": 0.0203,
      "step": 945
    },
    {
      "epoch": 4.548076923076923,
      "grad_norm": 0.04963435226739599,
      "learning_rate": 4.2608467132581934e-07,
      "loss": 0.0266,
      "step": 946
    },
    {
      "epoch": 4.552884615384615,
      "grad_norm": 0.06240900006531489,
      "learning_rate": 4.171304871494264e-07,
      "loss": 0.0236,
      "step": 947
    },
    {
      "epoch": 4.5576923076923075,
      "grad_norm": 0.0836659565799828,
      "learning_rate": 4.082693867301224e-07,
      "loss": 0.0284,
      "step": 948
    },
    {
      "epoch": 4.5625,
      "grad_norm": 0.047597974391768647,
      "learning_rate": 3.99501456140714e-07,
      "loss": 0.0271,
      "step": 949
    },
    {
      "epoch": 4.5673076923076925,
      "grad_norm": 0.05303562221398212,
      "learning_rate": 3.908267805490051e-07,
      "loss": 0.0286,
      "step": 950
    },
    {
      "epoch": 4.572115384615385,
      "grad_norm": 0.05744414442536019,
      "learning_rate": 3.8224544421695766e-07,
      "loss": 0.0376,
      "step": 951
    },
    {
      "epoch": 4.576923076923077,
      "grad_norm": 0.07232655302301497,
      "learning_rate": 3.7375753049987974e-07,
      "loss": 0.0401,
      "step": 952
    },
    {
      "epoch": 4.581730769230769,
      "grad_norm": 0.08372592404701645,
      "learning_rate": 3.6536312184560996e-07,
      "loss": 0.038,
      "step": 953
    },
    {
      "epoch": 4.586538461538462,
      "grad_norm": 0.16540616781372067,
      "learning_rate": 3.570622997937234e-07,
      "loss": 0.0428,
      "step": 954
    },
    {
      "epoch": 4.591346153846154,
      "grad_norm": 0.14717139259594558,
      "learning_rate": 3.4885514497473574e-07,
      "loss": 0.0359,
      "step": 955
    },
    {
      "epoch": 4.596153846153846,
      "grad_norm": 0.057687802790113006,
      "learning_rate": 3.4074173710931804e-07,
      "loss": 0.0242,
      "step": 956
    },
    {
      "epoch": 4.600961538461538,
      "grad_norm": 0.04469434836530978,
      "learning_rate": 3.327221550075266e-07,
      "loss": 0.02,
      "step": 957
    },
    {
      "epoch": 4.605769230769231,
      "grad_norm": 0.05275380086785854,
      "learning_rate": 3.247964765680389e-07,
      "loss": 0.0236,
      "step": 958
    },
    {
      "epoch": 4.610576923076923,
      "grad_norm": 0.07425802929057401,
      "learning_rate": 3.1696477877738664e-07,
      "loss": 0.0248,
      "step": 959
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 0.08006616304531214,
      "learning_rate": 3.0922713770922155e-07,
      "loss": 0.0277,
      "step": 960
    },
    {
      "epoch": 4.6201923076923075,
      "grad_norm": 0.04202189042110731,
      "learning_rate": 3.0158362852356627e-07,
      "loss": 0.0228,
      "step": 961
    },
    {
      "epoch": 4.625,
      "grad_norm": 0.07257193436046669,
      "learning_rate": 2.940343254660905e-07,
      "loss": 0.0279,
      "step": 962
    },
    {
      "epoch": 4.6298076923076925,
      "grad_norm": 0.18849117361331164,
      "learning_rate": 2.865793018673857e-07,
      "loss": 0.0309,
      "step": 963
    },
    {
      "epoch": 4.634615384615385,
      "grad_norm": 0.06095598442535464,
      "learning_rate": 2.7921863014225504e-07,
      "loss": 0.0251,
      "step": 964
    },
    {
      "epoch": 4.639423076923077,
      "grad_norm": 0.10221589658168558,
      "learning_rate": 2.7195238178900685e-07,
      "loss": 0.0275,
      "step": 965
    },
    {
      "epoch": 4.644230769230769,
      "grad_norm": 0.046385042451594666,
      "learning_rate": 2.6478062738876654e-07,
      "loss": 0.0249,
      "step": 966
    },
    {
      "epoch": 4.649038461538462,
      "grad_norm": 0.08380224500002717,
      "learning_rate": 2.577034366047848e-07,
      "loss": 0.0362,
      "step": 967
    },
    {
      "epoch": 4.653846153846154,
      "grad_norm": 0.11671633705920467,
      "learning_rate": 2.507208781817638e-07,
      "loss": 0.0352,
      "step": 968
    },
    {
      "epoch": 4.658653846153846,
      "grad_norm": 0.09376844508720386,
      "learning_rate": 2.4383301994518773e-07,
      "loss": 0.0279,
      "step": 969
    },
    {
      "epoch": 4.663461538461538,
      "grad_norm": 0.5066692211480123,
      "learning_rate": 2.370399288006664e-07,
      "loss": 0.048,
      "step": 970
    },
    {
      "epoch": 4.668269230769231,
      "grad_norm": 0.04669681045218132,
      "learning_rate": 2.3034167073328283e-07,
      "loss": 0.0257,
      "step": 971
    },
    {
      "epoch": 4.673076923076923,
      "grad_norm": 0.0438301763601614,
      "learning_rate": 2.2373831080695463e-07,
      "loss": 0.0242,
      "step": 972
    },
    {
      "epoch": 4.677884615384615,
      "grad_norm": 0.05785413300818901,
      "learning_rate": 2.1722991316380005e-07,
      "loss": 0.0294,
      "step": 973
    },
    {
      "epoch": 4.6826923076923075,
      "grad_norm": 0.047720225883125925,
      "learning_rate": 2.1081654102351634e-07,
      "loss": 0.0285,
      "step": 974
    },
    {
      "epoch": 4.6875,
      "grad_norm": 0.07553742021787213,
      "learning_rate": 2.0449825668276246e-07,
      "loss": 0.0248,
      "step": 975
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 0.07693960763737699,
      "learning_rate": 1.9827512151456175e-07,
      "loss": 0.027,
      "step": 976
    },
    {
      "epoch": 4.697115384615385,
      "grad_norm": 0.09380491748560991,
      "learning_rate": 1.921471959676957e-07,
      "loss": 0.0278,
      "step": 977
    },
    {
      "epoch": 4.701923076923077,
      "grad_norm": 0.047197644478585904,
      "learning_rate": 1.8611453956612346e-07,
      "loss": 0.0248,
      "step": 978
    },
    {
      "epoch": 4.706730769230769,
      "grad_norm": 0.05412166234290511,
      "learning_rate": 1.8017721090840324e-07,
      "loss": 0.0243,
      "step": 979
    },
    {
      "epoch": 4.711538461538462,
      "grad_norm": 0.10547778866245477,
      "learning_rate": 1.7433526766711727e-07,
      "loss": 0.0324,
      "step": 980
    },
    {
      "epoch": 4.716346153846154,
      "grad_norm": 0.09099239534760335,
      "learning_rate": 1.6858876658832235e-07,
      "loss": 0.0285,
      "step": 981
    },
    {
      "epoch": 4.721153846153846,
      "grad_norm": 0.08162050934932856,
      "learning_rate": 1.629377634909868e-07,
      "loss": 0.0295,
      "step": 982
    },
    {
      "epoch": 4.725961538461538,
      "grad_norm": 0.0634831323096344,
      "learning_rate": 1.5738231326645758e-07,
      "loss": 0.0264,
      "step": 983
    },
    {
      "epoch": 4.730769230769231,
      "grad_norm": 0.0509792051894,
      "learning_rate": 1.519224698779198e-07,
      "loss": 0.0247,
      "step": 984
    },
    {
      "epoch": 4.735576923076923,
      "grad_norm": 0.09446720339524335,
      "learning_rate": 1.465582863598791e-07,
      "loss": 0.0316,
      "step": 985
    },
    {
      "epoch": 4.740384615384615,
      "grad_norm": 0.0712695423192039,
      "learning_rate": 1.4128981481764115e-07,
      "loss": 0.0252,
      "step": 986
    },
    {
      "epoch": 4.7451923076923075,
      "grad_norm": 0.04570082153216246,
      "learning_rate": 1.3611710642681076e-07,
      "loss": 0.0287,
      "step": 987
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.09859280150420978,
      "learning_rate": 1.3104021143278911e-07,
      "loss": 0.0282,
      "step": 988
    },
    {
      "epoch": 4.7548076923076925,
      "grad_norm": 0.05273151607456718,
      "learning_rate": 1.2605917915028743e-07,
      "loss": 0.023,
      "step": 989
    },
    {
      "epoch": 4.759615384615385,
      "grad_norm": 0.05840372906233733,
      "learning_rate": 1.2117405796285286e-07,
      "loss": 0.0293,
      "step": 990
    },
    {
      "epoch": 4.764423076923077,
      "grad_norm": 0.06393200065539793,
      "learning_rate": 1.1638489532239339e-07,
      "loss": 0.0258,
      "step": 991
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 0.05295928973625504,
      "learning_rate": 1.1169173774871478e-07,
      "loss": 0.0241,
      "step": 992
    },
    {
      "epoch": 4.774038461538462,
      "grad_norm": 0.08735252683661104,
      "learning_rate": 1.0709463082907545e-07,
      "loss": 0.0277,
      "step": 993
    },
    {
      "epoch": 4.778846153846154,
      "grad_norm": 0.23892976754883355,
      "learning_rate": 1.0259361921774014e-07,
      "loss": 0.0409,
      "step": 994
    },
    {
      "epoch": 4.783653846153846,
      "grad_norm": 0.06298806493714756,
      "learning_rate": 9.818874663554356e-08,
      "loss": 0.0311,
      "step": 995
    },
    {
      "epoch": 4.788461538461538,
      "grad_norm": 0.04594482842415682,
      "learning_rate": 9.388005586947191e-08,
      "loss": 0.0206,
      "step": 996
    },
    {
      "epoch": 4.793269230769231,
      "grad_norm": 0.04840500280254559,
      "learning_rate": 8.966758877224202e-08,
      "loss": 0.0265,
      "step": 997
    },
    {
      "epoch": 4.798076923076923,
      "grad_norm": 0.05088818586037738,
      "learning_rate": 8.555138626189619e-08,
      "loss": 0.0253,
      "step": 998
    },
    {
      "epoch": 4.802884615384615,
      "grad_norm": 0.08399566629598647,
      "learning_rate": 8.153148832140467e-08,
      "loss": 0.0355,
      "step": 999
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 0.14579957772829252,
      "learning_rate": 7.760793399827937e-08,
      "loss": 0.0294,
      "step": 1000
    },
    {
      "epoch": 4.8125,
      "grad_norm": 0.12810559485850295,
      "learning_rate": 7.378076140419188e-08,
      "loss": 0.0328,
      "step": 1001
    },
    {
      "epoch": 4.8173076923076925,
      "grad_norm": 0.0491935492764327,
      "learning_rate": 7.00500077146038e-08,
      "loss": 0.0234,
      "step": 1002
    },
    {
      "epoch": 4.822115384615385,
      "grad_norm": 0.06621902617216599,
      "learning_rate": 6.641570916840923e-08,
      "loss": 0.0269,
      "step": 1003
    },
    {
      "epoch": 4.826923076923077,
      "grad_norm": 0.05877425741394798,
      "learning_rate": 6.287790106757396e-08,
      "loss": 0.0376,
      "step": 1004
    },
    {
      "epoch": 4.831730769230769,
      "grad_norm": 0.07311636578364461,
      "learning_rate": 5.943661777680354e-08,
      "loss": 0.0309,
      "step": 1005
    },
    {
      "epoch": 4.836538461538462,
      "grad_norm": 0.13144810757818062,
      "learning_rate": 5.609189272320237e-08,
      "loss": 0.0389,
      "step": 1006
    },
    {
      "epoch": 4.841346153846154,
      "grad_norm": 0.09620384813713904,
      "learning_rate": 5.284375839594958e-08,
      "loss": 0.0272,
      "step": 1007
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 0.1026511470855064,
      "learning_rate": 4.9692246345985905e-08,
      "loss": 0.0299,
      "step": 1008
    },
    {
      "epoch": 4.850961538461538,
      "grad_norm": 0.06347692643613856,
      "learning_rate": 4.663738718570621e-08,
      "loss": 0.0304,
      "step": 1009
    },
    {
      "epoch": 4.855769230769231,
      "grad_norm": 0.0891545347241656,
      "learning_rate": 4.367921058866187e-08,
      "loss": 0.0295,
      "step": 1010
    },
    {
      "epoch": 4.860576923076923,
      "grad_norm": 0.11808700899614864,
      "learning_rate": 4.0817745289272184e-08,
      "loss": 0.0311,
      "step": 1011
    },
    {
      "epoch": 4.865384615384615,
      "grad_norm": 0.07333636454400218,
      "learning_rate": 3.805301908254455e-08,
      "loss": 0.0271,
      "step": 1012
    },
    {
      "epoch": 4.8701923076923075,
      "grad_norm": 0.056787680996523135,
      "learning_rate": 3.538505882380916e-08,
      "loss": 0.0295,
      "step": 1013
    },
    {
      "epoch": 4.875,
      "grad_norm": 0.06725871557661173,
      "learning_rate": 3.281389042844918e-08,
      "loss": 0.0271,
      "step": 1014
    },
    {
      "epoch": 4.8798076923076925,
      "grad_norm": 0.10571479737978914,
      "learning_rate": 3.033953887166097e-08,
      "loss": 0.0285,
      "step": 1015
    },
    {
      "epoch": 4.884615384615385,
      "grad_norm": 0.0562208188989804,
      "learning_rate": 2.796202818819871e-08,
      "loss": 0.0368,
      "step": 1016
    },
    {
      "epoch": 4.889423076923077,
      "grad_norm": 0.05162294806985461,
      "learning_rate": 2.5681381472151268e-08,
      "loss": 0.0279,
      "step": 1017
    },
    {
      "epoch": 4.894230769230769,
      "grad_norm": 0.05482458316994797,
      "learning_rate": 2.349762087671126e-08,
      "loss": 0.0244,
      "step": 1018
    },
    {
      "epoch": 4.899038461538462,
      "grad_norm": 0.05657969131106677,
      "learning_rate": 2.1410767613965212e-08,
      "loss": 0.0247,
      "step": 1019
    },
    {
      "epoch": 4.903846153846154,
      "grad_norm": 0.055626237312767154,
      "learning_rate": 1.9420841954681525e-08,
      "loss": 0.0301,
      "step": 1020
    },
    {
      "epoch": 4.908653846153846,
      "grad_norm": 0.055735255106991385,
      "learning_rate": 1.7527863228118393e-08,
      "loss": 0.0224,
      "step": 1021
    },
    {
      "epoch": 4.913461538461538,
      "grad_norm": 0.05730950176906906,
      "learning_rate": 1.5731849821833955e-08,
      "loss": 0.0239,
      "step": 1022
    },
    {
      "epoch": 4.918269230769231,
      "grad_norm": 0.08412949054884762,
      "learning_rate": 1.4032819181509783e-08,
      "loss": 0.0371,
      "step": 1023
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 0.24764403397678603,
      "learning_rate": 1.2430787810776556e-08,
      "loss": 0.036,
      "step": 1024
    },
    {
      "epoch": 4.927884615384615,
      "grad_norm": 0.05428211230114028,
      "learning_rate": 1.0925771271058649e-08,
      "loss": 0.0296,
      "step": 1025
    },
    {
      "epoch": 4.9326923076923075,
      "grad_norm": 0.05012889797971388,
      "learning_rate": 9.517784181422018e-09,
      "loss": 0.0264,
      "step": 1026
    },
    {
      "epoch": 4.9375,
      "grad_norm": 0.08878769765078133,
      "learning_rate": 8.20684021843099e-09,
      "loss": 0.0322,
      "step": 1027
    },
    {
      "epoch": 4.9423076923076925,
      "grad_norm": 0.07801842299059507,
      "learning_rate": 6.992952116013918e-09,
      "loss": 0.0207,
      "step": 1028
    },
    {
      "epoch": 4.947115384615385,
      "grad_norm": 0.06536217848500146,
      "learning_rate": 5.876131665345508e-09,
      "loss": 0.0283,
      "step": 1029
    },
    {
      "epoch": 4.951923076923077,
      "grad_norm": 0.06482612566802776,
      "learning_rate": 4.856389714723575e-09,
      "loss": 0.0312,
      "step": 1030
    },
    {
      "epoch": 4.956730769230769,
      "grad_norm": 0.0672346447473067,
      "learning_rate": 3.933736169471347e-09,
      "loss": 0.0258,
      "step": 1031
    },
    {
      "epoch": 4.961538461538462,
      "grad_norm": 0.05551101931668255,
      "learning_rate": 3.1081799918375454e-09,
      "loss": 0.021,
      "step": 1032
    },
    {
      "epoch": 4.966346153846154,
      "grad_norm": 0.07451250831261923,
      "learning_rate": 2.379729200908676e-09,
      "loss": 0.0318,
      "step": 1033
    },
    {
      "epoch": 4.971153846153846,
      "grad_norm": 0.04223348375042281,
      "learning_rate": 1.7483908725357546e-09,
      "loss": 0.0246,
      "step": 1034
    },
    {
      "epoch": 4.975961538461538,
      "grad_norm": 0.049272216151055565,
      "learning_rate": 1.214171139258813e-09,
      "loss": 0.0279,
      "step": 1035
    },
    {
      "epoch": 4.980769230769231,
      "grad_norm": 0.05522233874340624,
      "learning_rate": 7.770751902513862e-10,
      "loss": 0.0241,
      "step": 1036
    },
    {
      "epoch": 4.985576923076923,
      "grad_norm": 0.04339089646914917,
      "learning_rate": 4.3710727127277417e-10,
      "loss": 0.0197,
      "step": 1037
    },
    {
      "epoch": 4.990384615384615,
      "grad_norm": 0.09332478906979683,
      "learning_rate": 1.9427068461808086e-10,
      "loss": 0.0402,
      "step": 1038
    },
    {
      "epoch": 4.9951923076923075,
      "grad_norm": 0.07058644463793315,
      "learning_rate": 4.856778909601012e-11,
      "loss": 0.0303,
      "step": 1039
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.04441486615435933,
      "learning_rate": 0.0,
      "loss": 0.0203,
      "step": 1040
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.02863776870071888,
      "eval_runtime": 651.994,
      "eval_samples_per_second": 0.638,
      "eval_steps_per_second": 0.638,
      "step": 1040
    },
    {
      "epoch": 5.0,
      "step": 1040,
      "total_flos": 77518246772736.0,
      "train_loss": 0.4836399020662961,
      "train_runtime": 7822.1499,
      "train_samples_per_second": 0.266,
      "train_steps_per_second": 0.133
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1040,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 77518246772736.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
